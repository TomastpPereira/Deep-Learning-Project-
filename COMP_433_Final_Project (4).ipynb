{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChD96p4XkuKW"
      },
      "source": [
        "# COMP 433 Final Project Fall 2022\n",
        "\n",
        "### Written by: Tomas Pereira (40128504), Aryamann Mehra (40127106), Andrei Skachkou (40134189)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru-BlJSCgvbT",
        "outputId": "e7492cbd-bb23-424a-bb95-83512395e3e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-HPpionnvw"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JojquPUg0um"
      },
      "source": [
        "### Method 1: Data Augmentation\n",
        "\n",
        "##### Methods to try\n",
        "\n",
        "- Image rotations and flips. Proven success of horizontal flips on CIFAR-10\n",
        "- Color augmentation. Isolate the RGB channel and zero out 2/3. Can also scale the brightness\n",
        "- Patchshuffle Regulariztion, proven effectiveness for CIFAR-10\n",
        "- Image Mixing/Sample Pairing, proven use on CIFAR-10 especially in limited data applications\n",
        "- Random Erasing\n",
        "\n",
        "\n",
        "- Potential\n",
        "  - random cropping\n",
        "  - Noise Injection\n",
        "\n",
        "\n",
        "  -  \"Combining augmentations such as cropping, flipping, color shifts, and random erasing can result in massively inflated dataset sizes. However, this is not guaranteed to be advantageous. In domains with very limited data, this could result in further overfitting.\"\n",
        "\n",
        "\n",
        "Research: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0#Sec3\n",
        "\n",
        "\n",
        "\n",
        "Additional Links: \n",
        "https://towardsdatascience.com/a-comprehensive-guide-to-image-augmentation-using-pytorch-fb162f2444be\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxeOufAugyuw",
        "outputId": "c2b87b4c-028f-42fd-dc13-6b4e78de53a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "2BDJWvF8gy1G",
        "outputId": "5bfae163-5c7d-42de-aafe-bbd93e1586c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "700\n",
            "\n",
            "\n",
            "Baseline Datapoint\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Each datapoint is a tuple with a (3,32,32) tensor and an int as label \n",
        "\n",
        "prng = RandomState(2)\n",
        "random_permute = prng.permutation(np.arange(0, 1000))\n",
        "classes =  prng.permutation(np.arange(0,10))\n",
        "indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "train_data = Subset(cifar_data, indx_train)\n",
        "val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "print(train_data.indices.shape[0])\n",
        "print(val_data.indices.shape[0])\n",
        "\n",
        "print('\\n\\nBaseline Datapoint')\n",
        "datapoint = train_data[82][0]\n",
        "datapoint = datapoint.numpy()\n",
        "datapoint = np.transpose(datapoint, (1,2,0))\n",
        "plt.imshow(datapoint);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMaS8fEy2teF"
      },
      "source": [
        "#### Horizontal Flipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "dp09kpT-AyJf",
        "outputId": "a3a0831a-daf1-40d1-d59d-3f67ba4dacad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedbb10cdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASPklEQVR4nO3de7BV5XnH8e8TldAIBvUckQB6vNAI8QJkh+IlaiAKGiZoTI12YmjGBO1ARqaaKSVJpU07VRNvUxkVKxUz1ktEK96qBG2JxiFuQBA8GpRiBREO3kmGGOTpH3sxOdD9vmeffVn7HN7fZ4Zhn/XstdfDgh9r7/Xu9S5zd0Rk7/eJZjcgIvlQ2EUSobCLJEJhF0mEwi6SCIVdJBH71rKymU0EbgT2Af7V3a+KPb+lpcXb2tpq2aSIRKxfv56tW7dauVrVYTezfYA5wBnABuB5M1vo7i+F1mlra6NYLFa7SRHpQqFQCNZqeRs/BnjV3de5+0fAPcDkGl5PRBqolrAPBt7o9POGbJmI9EANP0FnZlPNrGhmxY6OjkZvTkQCagn7RmBop5+HZMt24+5z3b3g7oXW1tYaNicitagl7M8Dw8zsCDPrA1wALKxPWyJSb1WfjXf3HWY2HXiC0tDbPHdfU7fOJDfr1r4brB057MAcO5FGqmmc3d0fAx6rUy8i0kD6Bp1IIhR2kUQo7CKJUNhFEqGwiySiprPx0ns8eut/B2uTLj09WNOEpHsPHdlFEqGwiyRCYRdJhMIukgiFXSQRe+/Z+B2R2t77p+aDDz8quzx2xn3m+d9pUDfSk+jILpIIhV0kEQq7SCIUdpFEKOwiiVDYRRLRqwehVj31aLA2bvykYO1vL/nLYO3yW/6tlpZyce2cO4O1K6ZP6fbr/fO9c2ppR3oJHdlFEqGwiyRCYRdJhMIukgiFXSQRCrtIImoaejOz9cCHwMfADncP3wm+AX4+59Zg7e3Iej+59Y5g7eFH7w/W/uuN/w1UwrdIWv5UeO63v/hGeJjsla2vB2v1dvvsWcHaoSedEaxNmjCxrn1MuWRasHbHLTfVdVspqsc4+5fcfWsdXkdEGkhv40USUWvYHXjSzJaZ2dR6NCQijVHr2/hT3H2jmR0CLDKzl919SecnZP8JTAU47LDDatyciFSrpiO7u2/Mft8CPAiMKfOcue5ecPdCa2trLZsTkRpUHXYz29/M+u96DJwJrK5XYyJSX1bt7X3M7EhKR3MofRz4d3f/p9g6hULBi8ViVdsr580l4WGtwaedXrft7O0mjBoRrD2x4qUcO6nOV8ZdEKw9svjuHDtpvkKhQLFYtHK1qj+zu/s64ISquxKRXGnoTSQRCrtIIhR2kUQo7CKJUNhFEtGrJ5z8zKmnNbuFpjgqUusbWL4mss7wIwYGay+vXResvb5te+RV8/PoU/cEa2bla/+5+LngOhPGja25p55IR3aRRCjsIolQ2EUSobCLJEJhF0lErz4bH3NCy+HB2soc53drhNcitfOGDy5faN8YXOeXDzwdrPXuPRU2cfyJwdrmD94P1g7pf0Aj2smFjuwiiVDYRRKhsIskQmEXSYTCLpIIhV0kEXvt0FtvH16r1oLAEFt4IBJaGtNKrzXwgE8Ha9XO2dgT6MgukgiFXSQRCrtIIhR2kUQo7CKJUNhFEtHl0JuZzQMmAVvc/dhs2UHAvUAbsB44393fbVybUqvYQGRsJrkZw8K1G9ZW203vVRh9crBWXP5sjp10XyVH9juAiXssmwksdvdhwOLsZxHpwboMe3a/9Xf2WDwZmJ89ng+cU+e+RKTOqv3MPtDdN2WP3wLCcxGLSI9Q8wk6L31/MPgdQjObamZFMyt2dHTUujkRqVK1Yd9sZoMAst+3hJ7o7nPdveDuhdbW1io3JyK1qjbsC4Ep2eMpwEP1aUdEGqWSobe7gdOBFjPbAFwJXAXcZ2YXUxrVOb+RTVZjwhdOCtaeeP5XOXaSr88Hli+LrLM5UvtZZHhtfORfz+IdkRftxZatCP/b+d70GcHav9x0QyPa6ZYuw+7uFwZK4+vci4g0kL5BJ5IIhV0kEQq7SCIUdpFEKOwiibA8J9ArFApeLBZz2dbvIrX9zXLpoSc5r1+4NmhouLYjMoR2zBfPCtZmzHu8gq6ap0+kFtkd0fvsfSIyuPWx/6GrluqiUChQLBbL/gPXkV0kEQq7SCIUdpFEKOwiiVDYRRKhsIskYq+919unmt1ABT4XqS186Jpg7eGHFgVrs+aVr70V+Zv+9re+Gelk/2Dlx9fOD9Z6uo8itbYhg4O11zaUv5cewE7C45RfPvXsYO0XSx6LdFM/OrKLJEJhF0mEwi6SCIVdJBEKu0gi9tqz8b3Bmkjt4QXhC0m+PfPScG3Gd8ouP+DQw8Ib6zsgWBo74svB2tKtsRtH9V6LI2fcD46s93bsNX/Z/AuDdGQXSYTCLpIIhV0kEQq7SCIUdpFEKOwiiajk9k/zgEnAFnc/Nls2G/gusOu2rLPcPZ9v89fBpyJ/7N9FLmbI04w7nw7WBrSGh8qmXHxJ+cLW8LZGHv+FYG3ljm3hFRM0oO+ng7W3t79f1Wu+Gbi78WfqfCPUSo7sdwATyyy/3t1HZr96TdBFUtVl2N19CfBODr2ISAPV8pl9upmtMrN5ZnZg3ToSkYaoNuw3A0cBI4FNwLWhJ5rZVDMrmlmxI/DZREQar6qwu/tmd//Y3XcCtwFjIs+d6+4Fdy+01vmEg4hUrqqwm9mgTj+eC6yuTzsi0iiVDL3dDZwOtJjZBuBK4HQzGwk4sB4IjPf0TKNGBd+I8OyKX9V1W7G58GK3GeobqT3+2MPBWnHFqrLLb3oqduMiaaZ9++fzjrfLsLv7hWUW396AXkSkgfQNOpFEKOwiiVDYRRKhsIskQmEXSUSSE06uXxueULDeYlMyHhu+eI1tkYvvVreHi2vaNcTWSB1VXtkWc0hsnLWOdGQXSYTCLpIIhV0kEQq7SCIUdpFEKOwiiUhy6K2lNXzHro3bXq/rtnZGagveq+umJAe9efpNHdlFEqGwiyRCYRdJhMIukgiFXSQRSZ6Nn3fHnGDt86edmGMn0hMNjMRiR+T2YG9HXvOqK6+poaP60JFdJBEKu0giFHaRRCjsIolQ2EUSobCLJKKS2z8NBe4EBlK63dNcd7/RzA4C7gXaKN0C6nx3f7dxrdbP6FPHNrsFabLD+4Ynfjtm6JHB2rZ3fxusPbs1fBHV38z+fmWNNVAlR/YdwOXuPgIYC0wzsxHATGCxuw8DFmc/i0gP1WXY3X2Tuy/PHn8ItAODgcnA/Oxp84FzGtWkiNSuW5/ZzawNGAUsBQa6+6as9Balt/ki0kNVHHYz6wcsAGa4+weda+7ulD7Pl1tvqpkVzazY0dFRU7MiUr2Kwm5m+1EK+l3u/kC2eLOZDcrqg4At5dZ197nuXnD3QmtrPvehFpH/r8uwm5lRuh97u7tf16m0EJiSPZ4CPFT/9kSkXiq56u1k4CLgRTN7IVs2C7gKuM/MLgZeB85vTIv5OvnE44O1Z59blWMn1ZkcGFFqidxi6JHIXHiba2un6UKzDR49bHBwnW2bwtevHXpgeL0ZX51YaVtN0WXY3f0ZwALl8fVtR0QaRd+gE0mEwi6SCIVdJBEKu0giFHaRRCQ54WTMj354ebA28StTgrVqhAdxiExrCAMitaGB4r6Rv+mRkaG3JyLb6in+rCV8O6/t239fdvmr7a8F1+lHeJxy1PDRwdr1NzR/UskYHdlFEqGwiyRCYRdJhMIukgiFXSQRCrtIIjT0tocJZ38rWPtsS/mht1e2hl/vqMi2Do3UIhepsT1SeyPQy7bIOi9EanmKHXnOHfanwdr6/3kzWNu6o/yfPDYzauG48ISTf/6NSeEV+x8QedXm05FdJBEKu0giFHaRRCjsIolQ2EUSobPx3fC9mdPKLp9+xZzgOuHLLeK12EUyR0dq+wZO8W+PXOzyduxUfZ3Fji5/Pe6kYK1jU3jIY0PgjDuE59CLnTcfflL4YpdTp10aWbNn05FdJBEKu0giFHaRRCjsIolQ2EUSobCLJKLLoTczGwrcSemWzA7MdfcbzWw28F1g161ZZ7n7Y41qNC9LnnwmWFv05Lrc+thYZY0NdW4k4oRI7UeXfL3s8rcoPyccwHNLlgdr7216P1gbEumjJbD8i187I7jOj28JD6X25uNjJePsO4DL3X25mfUHlpnZoqx2vbv/tHHtiUi9VHKvt03Apuzxh2bWTvw7HyLSA3XrPYmZtQGjgKXZoulmtsrM5pnZgXXuTUTqqOKwm1k/YAEww90/AG6mNDfDSEpH/msD6001s6KZFTs6Oso9RURyUFHYzWw/SkG/y90fAHD3ze7+sbvvBG4DxpRb193nunvB3Qutra316ltEuqnLsJuZAbcD7e5+Xaflgzo97Vxgdf3bE5F6qeRs/MnARcCLZrZrurJZwIVmNpLScNx64JKGdNgAOyP3Vmpv/02w9vSSVQ3opveK3aLqJ/P/o+zypdtja4XFrlK75rLLgrVRo44vu3zMlG9GXrFPZU31MpWcjX8GsDKlXj+mLpKS3vsNARHpFoVdJBEKu0giFHaRRCjsIolIcsLJT0T+1G1HHB6sDRtavrZsbfQ6tF7tc5HaWV/7UrD24hu/Lbv8vL4HB9cZPmJEsDbyuPJDaADnTQvfskv+SEd2kUQo7CKJUNhFEqGwiyRCYRdJhMIukogkh95iJnx1fLA26tDyw0a3Xf0PwXUeX/hgsBbb+S39wrVXI/dtWxlY/tMzw8Naxwz/bLB23Inh4bXDJp8TbqTv0HBNmkJHdpFEKOwiiVDYRRKhsIskQmEXSYTCLpIIDb11wyFjRpZd/oMFDwTX+X7HpmDt5faXgrUhreGrwx5eEB7OKz6/ouzyy++9PbgOfTXFdwp0ZBdJhMIukgiFXSQRCrtIIhR2kUR0eTbezPoCS4BPZs+/392vNLMjgHuAg4FlwEXu/lEjm+2N+rQOCtaOj9Ripvyw/KgAwJSqXlFSUMmR/ffAOHc/gdLtmSea2VjgauB6dz8aeBe4uHFtikitugy7l2zLftwv++XAOOD+bPl8IHK9o4g0W6X3Z98nu4PrFmAR8BrwnrvvuiXnBmBwY1oUkXqoKOzu/rG7jwSGAGOAYyrdgJlNNbOimRU7OjqqbFNEatWts/Hu/h7wNHAiMMDMdp3gGwKUvVOCu89194K7F1pb9bVMkWbpMuxm1mpmA7LHfwKcAbRTCv3Xs6dNAR5qVJMiUrtKLoQZBMw3s30o/edwn7s/YmYvAfeY2T8CK4DIlRYi0mxdht3dVwGjyixfR+nzu4j0AvoGnUgiFHaRRCjsIolQ2EUSobCLJMLcPb+NmXUAr2c/tgBbc9t4mPrYnfrYXW/r43B3L/vttVzDvtuGzYruXmjKxtWH+kiwD72NF0mEwi6SiGaGfW4Tt92Z+tid+tjdXtNH0z6zi0i+9DZeJBFNCbuZTTSzV8zsVTOb2Ywesj7Wm9mLZvaCmRVz3O48M9tiZqs7LTvIzBaZ2drs9wOb1MdsM9uY7ZMXzOzsHPoYamZPm9lLZrbGzC7Llue6TyJ95LpPzKyvmf3azFZmffx9tvwIM1ua5eZeM+vTrRd291x/AftQmtbqSKAPsBIYkXcfWS/rgZYmbPdUYDSwutOya4CZ2eOZwNVN6mM2cEXO+2MQMDp73B/4DTAi730S6SPXfQIY0C97vB+wFBgL3AdckC2/Bfir7rxuM47sY4BX3X2dl6aevgeY3IQ+msbdlwDv7LF4MqWJOyGnCTwDfeTO3Te5+/Ls8YeUJkcZTM77JNJHrryk7pO8NiPsg4E3Ov3czMkqHXjSzJaZ2dQm9bDLQHffdcvXt4CBTexlupmtyt7mN/zjRGdm1kZp/oSlNHGf7NEH5LxPGjHJa+on6E5x99HAWcA0Mzu12Q1B6X92Sv8RNcPNwFGU7hGwCbg2rw2bWT9gATDD3T/oXMtzn5TpI/d94jVM8hrSjLBvBIZ2+jk4WWWjufvG7PctwIM0d+adzWY2CCD7fUszmnD3zdk/tJ3AbeS0T8xsP0oBu8vdd93wPvd9Uq6PZu2TbNvdnuQ1pBlhfx4Ylp1Z7ANcACzMuwkz29/M+u96DJwJrI6v1VAL+ePdm5o2geeucGXOJYd9YmZGaQ7Ddne/rlMp130S6iPvfdKwSV7zOsO4x9nGsymd6XwN+EGTejiS0kjASmBNnn0Ad1N6O/gHSp+9LqZ0z7zFwFrgF8BBTerjZ8CLwCpKYRuUQx+nUHqLvgp4Ift1dt77JNJHrvsEOJ7SJK6rKP3H8ned/s3+GngV+Dnwye68rr5BJ5KI1E/QiSRDYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEvF/uUWASJ7JLpgAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Creates a new datapoint by flipping the input image and assigning it the same label.  \n",
        "def flipHorizontal(data):\n",
        "  label = data[1]\n",
        "  flip = torchvision.transforms.RandomHorizontalFlip(p=1.0)\n",
        "  img = flip(data[0])\n",
        "  datapoint = (img, label)\n",
        "  return datapoint\n",
        "\n",
        "\n",
        "datapoint = train_data[82]\n",
        "newPoint = flipHorizontal(datapoint)\n",
        "\n",
        "img = newPoint[0].numpy()\n",
        "img = np.transpose(img, (1,2,0))\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmBrQaQ-2xj1"
      },
      "source": [
        "#### Image Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "_NlbolYvCRUt",
        "outputId": "8d654657-df01-4534-f0f5-374165a2326c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedbb029e50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Creates a new datapoint by rotating the input by a number of degrees indicated by angle (float), and assigns it the same class label. \n",
        "def rotatePoint(data, angle):\n",
        "  rotated = torchvision.transforms.functional.rotate(data[0],angle = angle)\n",
        "  newPoint = (rotated, data[1])\n",
        "  return newPoint\n",
        "\n",
        "datapoint = train_data[82]\n",
        "newPoint = rotatePoint(datapoint, 180.0)\n",
        "img = newPoint[0].numpy()\n",
        "img = np.transpose(img, (1,2,0))\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu3_LPM12zyY"
      },
      "source": [
        "#### Color Isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "-A_mKJC-EWsq",
        "outputId": "700508e8-6e9a-4156-bfe5-9e634b429297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedbaaac3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Creates 3 new datapoints by isolating each RGB channel of the input image. The same original label is assigned to each\n",
        "def isolateRGB(data):\n",
        "  datapointR = np.copy(data[0])\n",
        "  datapointG = np.copy(data[0])\n",
        "  datapointB = np.copy(data[0])\n",
        "  datapointR[1] = datapointR[2] = datapointG[0] = datapointG[2] = datapointB[0] = datapointB[1] = 0\n",
        "  newPoints = [(datapointR, data[1]), (datapointG, data[1]), (datapointB, data[1])]\n",
        "  return newPoints\n",
        "\n",
        "datapoint = train_data[82]\n",
        "newPoints = isolateRGB(datapoint)\n",
        "\n",
        "imgR = newPoints[0][0]\n",
        "imgG = newPoints[1][0]\n",
        "imgB = newPoints[2][0]\n",
        "\n",
        "imgR = np.transpose(imgR, (1,2,0))\n",
        "imgG = np.transpose(imgG, (1,2,0))\n",
        "imgB = np.transpose(imgB, (1,2,0))\n",
        "\n",
        "f, axarr = plt.subplots(1,3)\n",
        "axarr[0].imshow(imgR)\n",
        "axarr[1].imshow(imgG)\n",
        "axarr[2].imshow(imgB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7THgWkX222L_"
      },
      "source": [
        "#### PatchShuffle Regularization\n",
        "\n",
        "https://arxiv.org/pdf/1707.07103.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "4yeeOsQ7Lvet",
        "outputId": "a43b6911-0a76-4761-dbb3-5793fbbddb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa647db9cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import torch.nn.functional as nnf\n",
        "\n",
        "# PatchShuffle implementation from: https://stackoverflow.com/questions/66962837/shuffle-patches-in-image-batch\n",
        "class ShufflePatches(object):\n",
        "  def __init__(self, patch_size):\n",
        "    self.ps = patch_size\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # divide the batch of images into non-overlapping patches\n",
        "    u = nnf.unfold(x, kernel_size=self.ps, stride=self.ps, padding=0)\n",
        "    # permute the patches of each image in the batch\n",
        "    pu = torch.cat([b_[:, torch.randperm(b_.shape[-1])][None,...] for b_ in u], dim=0)\n",
        "    # fold the permuted patches back together\n",
        "    f = nnf.fold(pu, x.shape[-2:], kernel_size=self.ps, stride=self.ps, padding=0)\n",
        "    return f\n",
        "\n",
        "# Creates new datapoint by randomly shuffling sections of the input image with size determined by the kernel. \n",
        "def PatchShuffle(data, kernel=(16,16)):\n",
        "  shuffle = ShufflePatches(kernel)\n",
        "  img = np.reshape(data[0], (-1,3,32,32))\n",
        "  newImg = shuffle(img)\n",
        "  newImg = np.reshape(newImg, (3,32,32))\n",
        "  newPoint = (newImg, data[1])\n",
        "  return newPoint\n",
        "\n",
        "dataPoint = train_data[82]\n",
        "newPoint = PatchShuffle(dataPoint)\n",
        "img = newPoint[0]\n",
        "img = np.transpose(img, (1,2,0))\n",
        "plt.imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4awdOALM4pXh"
      },
      "source": [
        "#### Sample Pairing/Image Mixing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AtFbwpJ7S-w"
      },
      "outputs": [],
      "source": [
        "# Creates a new image by averaging the pixel value of 2 images at each color channel. The label attached should be the same as im1. \n",
        "# Point1 and Point2 are tuples of a (3,32,32) tensor and an int representing the images label\n",
        "\n",
        "def averageImages(point1, point2):\n",
        "  generatedImg = np.empty([3,32,32])\n",
        "  for i in range(3):\n",
        "    layer = (point1[0][i] + point2[0][i])/2\n",
        "    generatedImg[i] = layer\n",
        "  generatedPoint = (generatedImg, point1[1])\n",
        "  return generatedPoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "tWHmwb816aAN",
        "outputId": "cb471428-32b7-4b2f-d2d5-25d8328e2e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "datapoint = train_data[82]\n",
        "datapoint2 = train_data[81]\n",
        "\n",
        "generatedPoint = averageImages(datapoint, datapoint2)\n",
        "\n",
        "generatedImg = generatedPoint[0]\n",
        "generatedImg = np.transpose(generatedImg, (1,2,0))\n",
        "plt.imshow(generatedImg)\n",
        "print(\"Label:\",generatedPoint[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXepORfLhFjO"
      },
      "source": [
        "#### Testing Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Definition"
      ],
      "metadata": {
        "id": "64MnilBaWSXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y4P1Fb_GjuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*5*5, 10)\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*5*5)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pF39NC8GRCy"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        target = target.type(torch.LongTensor)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDnocWCtV915"
      },
      "source": [
        "###### First Augmentation Set Attempt - TESTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmXluuALhI_8",
        "outputId": "42a15479-c5e1-4e46-82e5-9e3b65dc6cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n"
          ]
        }
      ],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(360600, 3, 32, 32)\n",
        "allSamplesL = torch.empty(360600)\n",
        "\n",
        "# The set currently becomes 6N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation  - 2N\n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamples.append(flip)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation - 5N \n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamples.append(rotate90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamples.append(rotate180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamples.append(rotate270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "    # Third Augmentation - 6N\n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamples.append(patch)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "print(len(allSamples))  \n",
        "\n",
        "# Image Mixing Augmentation \n",
        "# Makes the set (6N)^2+6N\n",
        "for i in range(len(allSamples)):\n",
        "  point1 = allSamples[i]\n",
        "  for j in range(len(allSamples)):\n",
        "    point2 = allSamples[j]\n",
        "    avgImg = averageImages(point1, point2)\n",
        "    img, label = avgImg[0], avgImg[1]\n",
        "    img = torch.from_numpy(img)\n",
        "    allSamplesT[tensorIndex] = img\n",
        "    allSamplesL[tensorIndex] = label\n",
        "    tensorIndex += 1\n",
        "\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0\n",
        "shuffledSamplesT = 0\n",
        "shuffledSamplesL = 0\n",
        "\n",
        "\n",
        "# Makes the set 4((6N)^2 + 6N)\n",
        "# for sample in allSamples:\n",
        "#     # Fourth Augmentation \n",
        "#     RGB = isolateRGB(sample)\n",
        "#     allSamples.append(RGB[0])\n",
        "#     allSamples.append(RGB[1])\n",
        "#     allSamples.append(RGB[2])\n",
        "\n",
        "\n",
        "# Running into crash after RAM maxed out\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdCHtA6VWcQ"
      },
      "source": [
        "Performance 1: On set of size (6N)^2+6N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_F8aODbGWxx",
        "outputId": "c9958820-ab39-49cc-be80-e4d0d84ac037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 360600 Num Samples For Val 700\n",
            "Train Epoch: 0 [67608/360600 (100%)]\tLoss: 0.334017\n",
            "Train Epoch: 5 [67608/360600 (100%)]\tLoss: 0.281043\n",
            "Train Epoch: 10 [67608/360600 (100%)]\tLoss: 0.434169\n",
            "Train Epoch: 15 [67608/360600 (100%)]\tLoss: 0.460172\n",
            "Train Epoch: 20 [67608/360600 (100%)]\tLoss: 0.169757\n",
            "Train Epoch: 25 [67608/360600 (100%)]\tLoss: 0.417007\n",
            "Train Epoch: 30 [67608/360600 (100%)]\tLoss: 0.215559\n",
            "Train Epoch: 35 [67608/360600 (100%)]\tLoss: 0.293782\n",
            "Train Epoch: 40 [67608/360600 (100%)]\tLoss: 0.324275\n",
            "Train Epoch: 45 [67608/360600 (100%)]\tLoss: 0.443098\n",
            "Train Epoch: 50 [67608/360600 (100%)]\tLoss: 0.351807\n",
            "Train Epoch: 55 [67608/360600 (100%)]\tLoss: 0.271874\n",
            "Train Epoch: 60 [67608/360600 (100%)]\tLoss: 0.400660\n",
            "Train Epoch: 65 [67608/360600 (100%)]\tLoss: 0.432385\n",
            "Train Epoch: 70 [67608/360600 (100%)]\tLoss: 0.307998\n",
            "Train Epoch: 75 [67608/360600 (100%)]\tLoss: 0.363038\n",
            "Train Epoch: 80 [67608/360600 (100%)]\tLoss: 0.374827\n",
            "Train Epoch: 85 [67608/360600 (100%)]\tLoss: 0.515594\n",
            "Train Epoch: 90 [67608/360600 (100%)]\tLoss: 0.425725\n",
            "Train Epoch: 95 [67608/360600 (100%)]\tLoss: 0.389141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9283, Accuracy: 417/700 (59.57%)\n",
            "\n",
            "Num Samples For Training 360600 Num Samples For Val 700\n",
            "Train Epoch: 0 [67608/360600 (100%)]\tLoss: 0.477207\n",
            "Train Epoch: 5 [67608/360600 (100%)]\tLoss: 0.247694\n",
            "Train Epoch: 10 [67608/360600 (100%)]\tLoss: 0.329753\n",
            "Train Epoch: 15 [67608/360600 (100%)]\tLoss: 0.294983\n",
            "Train Epoch: 20 [67608/360600 (100%)]\tLoss: 0.367429\n",
            "Train Epoch: 25 [67608/360600 (100%)]\tLoss: 0.594104\n",
            "Train Epoch: 30 [67608/360600 (100%)]\tLoss: 0.415422\n",
            "Train Epoch: 35 [67608/360600 (100%)]\tLoss: 0.420015\n",
            "Train Epoch: 40 [67608/360600 (100%)]\tLoss: 0.499807\n",
            "Train Epoch: 45 [67608/360600 (100%)]\tLoss: 0.198612\n",
            "Train Epoch: 50 [67608/360600 (100%)]\tLoss: 0.421486\n",
            "Train Epoch: 55 [67608/360600 (100%)]\tLoss: 0.324170\n",
            "Train Epoch: 60 [67608/360600 (100%)]\tLoss: 0.286813\n",
            "Train Epoch: 65 [67608/360600 (100%)]\tLoss: 0.485551\n",
            "Train Epoch: 70 [67608/360600 (100%)]\tLoss: 0.451019\n",
            "Train Epoch: 75 [67608/360600 (100%)]\tLoss: 0.349651\n",
            "Train Epoch: 80 [67608/360600 (100%)]\tLoss: 0.347308\n",
            "Train Epoch: 85 [67608/360600 (100%)]\tLoss: 0.294683\n",
            "Train Epoch: 90 [67608/360600 (100%)]\tLoss: 0.401246\n",
            "Train Epoch: 95 [67608/360600 (100%)]\tLoss: 0.292433\n",
            "\n",
            "Test set: Average loss: 0.9781, Accuracy: 410/700 (58.57%)\n",
            "\n",
            "Num Samples For Training 360600 Num Samples For Val 700\n",
            "Train Epoch: 0 [67608/360600 (100%)]\tLoss: 0.377260\n",
            "Train Epoch: 5 [67608/360600 (100%)]\tLoss: 0.364752\n",
            "Train Epoch: 10 [67608/360600 (100%)]\tLoss: 0.403236\n",
            "Train Epoch: 15 [67608/360600 (100%)]\tLoss: 0.410834\n",
            "Train Epoch: 20 [67608/360600 (100%)]\tLoss: 0.426119\n",
            "Train Epoch: 25 [67608/360600 (100%)]\tLoss: 0.575723\n",
            "Train Epoch: 30 [67608/360600 (100%)]\tLoss: 0.392886\n",
            "Train Epoch: 35 [67608/360600 (100%)]\tLoss: 0.384024\n",
            "Train Epoch: 40 [67608/360600 (100%)]\tLoss: 0.373813\n",
            "Train Epoch: 45 [67608/360600 (100%)]\tLoss: 0.387146\n",
            "Train Epoch: 50 [67608/360600 (100%)]\tLoss: 0.365023\n",
            "Train Epoch: 55 [67608/360600 (100%)]\tLoss: 0.422956\n",
            "Train Epoch: 60 [67608/360600 (100%)]\tLoss: 0.293024\n",
            "Train Epoch: 65 [67608/360600 (100%)]\tLoss: 0.314922\n",
            "Train Epoch: 70 [67608/360600 (100%)]\tLoss: 0.326895\n",
            "Train Epoch: 75 [67608/360600 (100%)]\tLoss: 0.399151\n",
            "Train Epoch: 80 [67608/360600 (100%)]\tLoss: 0.441141\n",
            "Train Epoch: 85 [67608/360600 (100%)]\tLoss: 0.306334\n",
            "Train Epoch: 90 [67608/360600 (100%)]\tLoss: 0.235475\n",
            "Train Epoch: 95 [67608/360600 (100%)]\tLoss: 0.432030\n",
            "\n",
            "Test set: Average loss: 0.9197, Accuracy: 421/700 (60.14%)\n",
            "\n",
            "Num Samples For Training 360600 Num Samples For Val 700\n",
            "Train Epoch: 0 [67608/360600 (100%)]\tLoss: 0.566815\n",
            "Train Epoch: 5 [67608/360600 (100%)]\tLoss: 0.276741\n",
            "Train Epoch: 10 [67608/360600 (100%)]\tLoss: 0.402607\n",
            "Train Epoch: 15 [67608/360600 (100%)]\tLoss: 0.406738\n",
            "Train Epoch: 20 [67608/360600 (100%)]\tLoss: 0.358852\n",
            "Train Epoch: 25 [67608/360600 (100%)]\tLoss: 0.297844\n",
            "Train Epoch: 30 [67608/360600 (100%)]\tLoss: 0.450459\n",
            "Train Epoch: 35 [67608/360600 (100%)]\tLoss: 0.390453\n",
            "Train Epoch: 40 [67608/360600 (100%)]\tLoss: 0.281147\n",
            "Train Epoch: 45 [67608/360600 (100%)]\tLoss: 0.251636\n",
            "Train Epoch: 50 [67608/360600 (100%)]\tLoss: 0.344717\n",
            "Train Epoch: 55 [67608/360600 (100%)]\tLoss: 0.317929\n",
            "Train Epoch: 60 [67608/360600 (100%)]\tLoss: 0.404129\n",
            "Train Epoch: 65 [67608/360600 (100%)]\tLoss: 0.453168\n",
            "Train Epoch: 70 [67608/360600 (100%)]\tLoss: 0.363256\n",
            "Train Epoch: 75 [67608/360600 (100%)]\tLoss: 0.320210\n",
            "Train Epoch: 80 [67608/360600 (100%)]\tLoss: 0.538521\n",
            "Train Epoch: 85 [67608/360600 (100%)]\tLoss: 0.339065\n",
            "Train Epoch: 90 [67608/360600 (100%)]\tLoss: 0.367291\n",
            "Train Epoch: 95 [67608/360600 (100%)]\tLoss: 0.335792\n",
            "\n",
            "Test set: Average loss: 0.9839, Accuracy: 418/700 (59.71%)\n",
            "\n",
            "Num Samples For Training 360600 Num Samples For Val 700\n",
            "Train Epoch: 0 [67608/360600 (100%)]\tLoss: 0.343796\n",
            "Train Epoch: 5 [67608/360600 (100%)]\tLoss: 0.363604\n",
            "Train Epoch: 10 [67608/360600 (100%)]\tLoss: 0.437321\n",
            "Train Epoch: 15 [67608/360600 (100%)]\tLoss: 0.247364\n",
            "Train Epoch: 20 [67608/360600 (100%)]\tLoss: 0.321865\n",
            "Train Epoch: 25 [67608/360600 (100%)]\tLoss: 0.537671\n",
            "Train Epoch: 30 [67608/360600 (100%)]\tLoss: 0.334130\n",
            "Train Epoch: 35 [67608/360600 (100%)]\tLoss: 0.476985\n",
            "Train Epoch: 40 [67608/360600 (100%)]\tLoss: 0.453445\n",
            "Train Epoch: 45 [67608/360600 (100%)]\tLoss: 0.189344\n",
            "Train Epoch: 50 [67608/360600 (100%)]\tLoss: 0.395528\n",
            "Train Epoch: 55 [67608/360600 (100%)]\tLoss: 0.374236\n",
            "Train Epoch: 60 [67608/360600 (100%)]\tLoss: 0.294493\n",
            "Train Epoch: 65 [67608/360600 (100%)]\tLoss: 0.456104\n",
            "Train Epoch: 70 [67608/360600 (100%)]\tLoss: 0.202188\n",
            "Train Epoch: 75 [67608/360600 (100%)]\tLoss: 0.275660\n",
            "Train Epoch: 80 [67608/360600 (100%)]\tLoss: 0.421614\n",
            "Train Epoch: 85 [67608/360600 (100%)]\tLoss: 0.459836\n",
            "Train Epoch: 90 [67608/360600 (100%)]\tLoss: 0.413260\n",
            "Train Epoch: 95 [67608/360600 (100%)]\tLoss: 0.341514\n",
            "\n",
            "Test set: Average loss: 0.9391, Accuracy: 420/700 (60.00%)\n",
            "\n",
            "Acc over 10 instances: 59.60 +- 0.55\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "# Reduced seed range here due to excessive compute\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT4pPi1g_8zf"
      },
      "source": [
        "Resulted in an average accuracy opf 57% and took 3 hours to complete. Very poor performance in comparison to the testbed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emAybYRET36O"
      },
      "source": [
        "###### Second Augmentation Set Attempt - TESTED\n",
        "\n",
        "Here I decide to mix images only if they have not been PatchShuffled. The intent is to reduce chaos of images and look for improved performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUsBrb_UVkEJ"
      },
      "source": [
        "Performance 2: On set of size 250600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APdfE9N2T_T-",
        "outputId": "287f205b-aef2-4866-bba4-c62b31e3b8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "250600\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(250600, 3, 32, 32)\n",
        "allSamplesL = torch.empty(250600)\n",
        "#shuffledSamplesT = torch.empty(400000, 3, 32, 32)\n",
        "#shuffledSamplesL = torch.empty(400000)\n",
        "\n",
        "# The set currently becomes 5N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation \n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamples.append(flip)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation\n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamples.append(rotate90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamples.append(rotate180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamples.append(rotate270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Third Augmentation \n",
        "    # The data created here is not passed to the image mixing step\n",
        "    # Final size is ???\n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "print(len(allSamples))  \n",
        "\n",
        "# Image Mixing Augmentation\n",
        "# Makes the set (5N)^2+6N\n",
        "for i in range(len(allSamples)):\n",
        "  point1 = allSamples[i]\n",
        "  for j in range(len(allSamples)):\n",
        "    point2 = allSamples[j]\n",
        "    avgImg = averageImages(point1, point2)\n",
        "    img, label = avgImg[0], avgImg[1]\n",
        "    img = torch.from_numpy(img)\n",
        "    allSamplesT[tensorIndex] = img\n",
        "    allSamplesL[tensorIndex] = label\n",
        "    tensorIndex += 1\n",
        "\n",
        "\n",
        "print(tensorIndex)\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0\n",
        "shuffledSamplesT = 0\n",
        "shuffledSamplesL = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZoEVIn7Vv2L",
        "outputId": "ee4d19b1-2e46-4970-f501-9e6b094c20c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 250600 Num Samples For Val 700\n",
            "Train Epoch: 0 [203528/250600 (100%)]\tLoss: 0.484987\n",
            "Train Epoch: 5 [203528/250600 (100%)]\tLoss: 0.432981\n",
            "Train Epoch: 10 [203528/250600 (100%)]\tLoss: 0.408245\n",
            "Train Epoch: 15 [203528/250600 (100%)]\tLoss: 0.435153\n",
            "Train Epoch: 20 [203528/250600 (100%)]\tLoss: 0.389108\n",
            "Train Epoch: 25 [203528/250600 (100%)]\tLoss: 0.354758\n",
            "Train Epoch: 30 [203528/250600 (100%)]\tLoss: 0.395042\n",
            "Train Epoch: 35 [203528/250600 (100%)]\tLoss: 0.420857\n",
            "Train Epoch: 40 [203528/250600 (100%)]\tLoss: 0.351634\n",
            "Train Epoch: 45 [203528/250600 (100%)]\tLoss: 0.366728\n",
            "Train Epoch: 50 [203528/250600 (100%)]\tLoss: 0.377671\n",
            "Train Epoch: 55 [203528/250600 (100%)]\tLoss: 0.456261\n",
            "Train Epoch: 60 [203528/250600 (100%)]\tLoss: 0.341150\n",
            "Train Epoch: 65 [203528/250600 (100%)]\tLoss: 0.325609\n",
            "Train Epoch: 70 [203528/250600 (100%)]\tLoss: 0.346958\n",
            "Train Epoch: 75 [203528/250600 (100%)]\tLoss: 0.421911\n",
            "Train Epoch: 80 [203528/250600 (100%)]\tLoss: 0.395348\n",
            "Train Epoch: 85 [203528/250600 (100%)]\tLoss: 0.291597\n",
            "Train Epoch: 90 [203528/250600 (100%)]\tLoss: 0.350120\n",
            "Train Epoch: 95 [203528/250600 (100%)]\tLoss: 0.383027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9879, Accuracy: 421/700 (60.14%)\n",
            "\n",
            "Num Samples For Training 250600 Num Samples For Val 700\n",
            "Train Epoch: 0 [203528/250600 (100%)]\tLoss: 0.481821\n",
            "Train Epoch: 5 [203528/250600 (100%)]\tLoss: 0.388102\n",
            "Train Epoch: 10 [203528/250600 (100%)]\tLoss: 0.456845\n",
            "Train Epoch: 15 [203528/250600 (100%)]\tLoss: 0.375706\n",
            "Train Epoch: 20 [203528/250600 (100%)]\tLoss: 0.476417\n",
            "Train Epoch: 25 [203528/250600 (100%)]\tLoss: 0.416962\n",
            "Train Epoch: 30 [203528/250600 (100%)]\tLoss: 0.296291\n",
            "Train Epoch: 35 [203528/250600 (100%)]\tLoss: 0.404478\n",
            "Train Epoch: 40 [203528/250600 (100%)]\tLoss: 0.374806\n",
            "Train Epoch: 45 [203528/250600 (100%)]\tLoss: 0.395454\n",
            "Train Epoch: 50 [203528/250600 (100%)]\tLoss: 0.430250\n",
            "Train Epoch: 55 [203528/250600 (100%)]\tLoss: 0.404180\n",
            "Train Epoch: 60 [203528/250600 (100%)]\tLoss: 0.441256\n",
            "Train Epoch: 65 [203528/250600 (100%)]\tLoss: 0.357114\n",
            "Train Epoch: 70 [203528/250600 (100%)]\tLoss: 0.363584\n",
            "Train Epoch: 75 [203528/250600 (100%)]\tLoss: 0.378627\n",
            "Train Epoch: 80 [203528/250600 (100%)]\tLoss: 0.335149\n",
            "Train Epoch: 85 [203528/250600 (100%)]\tLoss: 0.398922\n",
            "Train Epoch: 90 [203528/250600 (100%)]\tLoss: 0.410829\n",
            "Train Epoch: 95 [203528/250600 (100%)]\tLoss: 0.374787\n",
            "\n",
            "Test set: Average loss: 1.0006, Accuracy: 438/700 (62.57%)\n",
            "\n",
            "Num Samples For Training 250600 Num Samples For Val 700\n",
            "Train Epoch: 0 [203528/250600 (100%)]\tLoss: 0.379926\n",
            "Train Epoch: 5 [203528/250600 (100%)]\tLoss: 0.408843\n",
            "Train Epoch: 10 [203528/250600 (100%)]\tLoss: 0.386763\n",
            "Train Epoch: 15 [203528/250600 (100%)]\tLoss: 0.428806\n",
            "Train Epoch: 20 [203528/250600 (100%)]\tLoss: 0.449253\n",
            "Train Epoch: 25 [203528/250600 (100%)]\tLoss: 0.333628\n",
            "Train Epoch: 30 [203528/250600 (100%)]\tLoss: 0.296401\n",
            "Train Epoch: 35 [203528/250600 (100%)]\tLoss: 0.354447\n",
            "Train Epoch: 40 [203528/250600 (100%)]\tLoss: 0.354058\n",
            "Train Epoch: 45 [203528/250600 (100%)]\tLoss: 0.378827\n",
            "Train Epoch: 50 [203528/250600 (100%)]\tLoss: 0.377868\n",
            "Train Epoch: 55 [203528/250600 (100%)]\tLoss: 0.403811\n",
            "Train Epoch: 60 [203528/250600 (100%)]\tLoss: 0.369120\n",
            "Train Epoch: 65 [203528/250600 (100%)]\tLoss: 0.359298\n",
            "Train Epoch: 70 [203528/250600 (100%)]\tLoss: 0.377003\n",
            "Train Epoch: 75 [203528/250600 (100%)]\tLoss: 0.453401\n",
            "Train Epoch: 80 [203528/250600 (100%)]\tLoss: 0.431273\n",
            "Train Epoch: 85 [203528/250600 (100%)]\tLoss: 0.384412\n",
            "Train Epoch: 90 [203528/250600 (100%)]\tLoss: 0.376668\n",
            "Train Epoch: 95 [203528/250600 (100%)]\tLoss: 0.305473\n",
            "\n",
            "Test set: Average loss: 1.0185, Accuracy: 430/700 (61.43%)\n",
            "\n",
            "Num Samples For Training 250600 Num Samples For Val 700\n",
            "Train Epoch: 0 [203528/250600 (100%)]\tLoss: 0.371527\n",
            "Train Epoch: 5 [203528/250600 (100%)]\tLoss: 0.410793\n",
            "Train Epoch: 10 [203528/250600 (100%)]\tLoss: 0.409403\n",
            "Train Epoch: 15 [203528/250600 (100%)]\tLoss: 0.417370\n",
            "Train Epoch: 20 [203528/250600 (100%)]\tLoss: 0.361222\n",
            "Train Epoch: 25 [203528/250600 (100%)]\tLoss: 0.427032\n",
            "Train Epoch: 30 [203528/250600 (100%)]\tLoss: 0.366898\n",
            "Train Epoch: 35 [203528/250600 (100%)]\tLoss: 0.459757\n",
            "Train Epoch: 40 [203528/250600 (100%)]\tLoss: 0.333965\n",
            "Train Epoch: 45 [203528/250600 (100%)]\tLoss: 0.372989\n",
            "Train Epoch: 50 [203528/250600 (100%)]\tLoss: 0.380471\n",
            "Train Epoch: 55 [203528/250600 (100%)]\tLoss: 0.364361\n",
            "Train Epoch: 60 [203528/250600 (100%)]\tLoss: 0.388757\n",
            "Train Epoch: 65 [203528/250600 (100%)]\tLoss: 0.429129\n",
            "Train Epoch: 70 [203528/250600 (100%)]\tLoss: 0.382888\n",
            "Train Epoch: 75 [203528/250600 (100%)]\tLoss: 0.447678\n",
            "Train Epoch: 80 [203528/250600 (100%)]\tLoss: 0.404487\n",
            "Train Epoch: 85 [203528/250600 (100%)]\tLoss: 0.307697\n",
            "Train Epoch: 90 [203528/250600 (100%)]\tLoss: 0.407872\n",
            "Train Epoch: 95 [203528/250600 (100%)]\tLoss: 0.456497\n",
            "\n",
            "Test set: Average loss: 0.9868, Accuracy: 431/700 (61.57%)\n",
            "\n",
            "Num Samples For Training 250600 Num Samples For Val 700\n",
            "Train Epoch: 0 [203528/250600 (100%)]\tLoss: 0.498585\n",
            "Train Epoch: 5 [203528/250600 (100%)]\tLoss: 0.354760\n",
            "Train Epoch: 10 [203528/250600 (100%)]\tLoss: 0.387689\n",
            "Train Epoch: 15 [203528/250600 (100%)]\tLoss: 0.364761\n",
            "Train Epoch: 20 [203528/250600 (100%)]\tLoss: 0.403490\n",
            "Train Epoch: 25 [203528/250600 (100%)]\tLoss: 0.401662\n",
            "Train Epoch: 30 [203528/250600 (100%)]\tLoss: 0.435197\n",
            "Train Epoch: 35 [203528/250600 (100%)]\tLoss: 0.328983\n",
            "Train Epoch: 40 [203528/250600 (100%)]\tLoss: 0.395625\n",
            "Train Epoch: 45 [203528/250600 (100%)]\tLoss: 0.441978\n",
            "Train Epoch: 50 [203528/250600 (100%)]\tLoss: 0.398860\n",
            "Train Epoch: 55 [203528/250600 (100%)]\tLoss: 0.294966\n",
            "Train Epoch: 60 [203528/250600 (100%)]\tLoss: 0.489713\n",
            "Train Epoch: 65 [203528/250600 (100%)]\tLoss: 0.357557\n",
            "Train Epoch: 70 [203528/250600 (100%)]\tLoss: 0.365854\n",
            "Train Epoch: 75 [203528/250600 (100%)]\tLoss: 0.318490\n",
            "Train Epoch: 80 [203528/250600 (100%)]\tLoss: 0.323820\n",
            "Train Epoch: 85 [203528/250600 (100%)]\tLoss: 0.376560\n",
            "Train Epoch: 90 [203528/250600 (100%)]\tLoss: 0.381488\n",
            "Train Epoch: 95 [203528/250600 (100%)]\tLoss: 0.395781\n",
            "\n",
            "Test set: Average loss: 0.9415, Accuracy: 421/700 (60.14%)\n",
            "\n",
            "Acc over 10 instances: 61.17 +- 0.93\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPb2Gq4EOE54"
      },
      "source": [
        "###### Augmentation Third Attempt: TESTED\n",
        "\n",
        "In the third attempt I decide to look for performance change without Image Mixing. This is to test its effectiveness and to reduce test time on this attempt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRuXKkRQOSQ-",
        "outputId": "1502a57a-9328-4253-ee00-77557372d791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# For storing and counting num of samples\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(600, 3, 32, 32)\n",
        "allSamplesL = torch.empty(600)\n",
        "\n",
        "# The set becomes 6N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation \n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamples.append(flip)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation\n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamples.append(rotate90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamples.append(rotate180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamples.append(rotate270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "    # Third Augmentation \n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamples.append(patch)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "print(len(allSamples))\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b6-nOfl-UAw"
      },
      "source": [
        "Performance on Set of Size 600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtLI6ALu-RGC",
        "outputId": "c35745ec-381e-45ff-e16f-b46572b27e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.211313\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.836205\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.692692\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.685642\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.705069\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.630276\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.638211\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.655020\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.627447\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.612058\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.658295\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.545183\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.526525\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.525666\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.554711\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.440040\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.415106\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.266856\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.296138\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.230130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0856, Accuracy: 448/700 (64.00%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.188674\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.976589\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.737039\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.694004\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.692214\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.693035\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.692845\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.689665\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.692252\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.692041\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.689053\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.661365\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.620348\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.641612\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.516714\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.646567\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.526837\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.535274\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.486256\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.529739\n",
            "\n",
            "Test set: Average loss: 0.6273, Accuracy: 475/700 (67.86%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.204037\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.723957\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.652277\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.682525\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.591559\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.526727\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.544436\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.542466\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.521216\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.496734\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.451123\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.469342\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.374493\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.268643\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.243554\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.154299\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.139723\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.163628\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.084583\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.126743\n",
            "\n",
            "Test set: Average loss: 1.9759, Accuracy: 443/700 (63.29%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.256614\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.692694\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.707761\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.583812\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.594187\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.705476\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.562944\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.734781\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.496322\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.484623\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.358041\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.370477\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.419719\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.210395\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.224871\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.144158\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.213785\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.076224\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.045890\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.028248\n",
            "\n",
            "Test set: Average loss: 2.2649, Accuracy: 460/700 (65.71%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.243597\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 1.002218\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.701378\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.695162\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.692397\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.655065\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.647759\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.660975\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.617985\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.502954\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.517690\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.483773\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.543951\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.402244\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.393352\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.186089\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.261774\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.169906\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.143918\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.117840\n",
            "\n",
            "Test set: Average loss: 1.6324, Accuracy: 454/700 (64.86%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.210957\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.744669\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.702526\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.689572\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.684740\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.662441\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.647111\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.659957\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.618335\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.657862\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.594573\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.526804\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.641731\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.453382\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.405035\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.370827\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.400638\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.433934\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.252706\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.258231\n",
            "\n",
            "Test set: Average loss: 0.8915, Accuracy: 487/700 (69.57%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.235222\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.778395\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.674382\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.650620\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.657454\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.663530\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.586356\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.529756\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.506052\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.570243\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.480898\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.436008\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.433663\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.335621\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.194423\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.222423\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.177352\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.287922\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.180189\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.104005\n",
            "\n",
            "Test set: Average loss: 1.6448, Accuracy: 467/700 (66.71%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.223817\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 0.831417\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.697155\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.691757\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.670905\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.697474\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.619772\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.613453\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.605545\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.533796\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.573136\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.423758\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.381496\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.441471\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.472610\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.351690\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.375767\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.232772\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.257851\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.193614\n",
            "\n",
            "Test set: Average loss: 1.1562, Accuracy: 459/700 (65.57%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.254238\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 1.191515\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.706551\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.658446\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.617351\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.624646\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.566681\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.517056\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.469737\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.462665\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.433330\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.398152\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.419558\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.366336\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.348577\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.334966\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.278441\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.213951\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.154050\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.172548\n",
            "\n",
            "Test set: Average loss: 1.3199, Accuracy: 460/700 (65.71%)\n",
            "\n",
            "Num Samples For Training 600 Num Samples For Val 700\n",
            "Train Epoch: 0 [352/600 (80%)]\tLoss: 2.202486\n",
            "Train Epoch: 5 [352/600 (80%)]\tLoss: 1.044289\n",
            "Train Epoch: 10 [352/600 (80%)]\tLoss: 0.689985\n",
            "Train Epoch: 15 [352/600 (80%)]\tLoss: 0.691171\n",
            "Train Epoch: 20 [352/600 (80%)]\tLoss: 0.672099\n",
            "Train Epoch: 25 [352/600 (80%)]\tLoss: 0.672385\n",
            "Train Epoch: 30 [352/600 (80%)]\tLoss: 0.680890\n",
            "Train Epoch: 35 [352/600 (80%)]\tLoss: 0.624145\n",
            "Train Epoch: 40 [352/600 (80%)]\tLoss: 0.598982\n",
            "Train Epoch: 45 [352/600 (80%)]\tLoss: 0.618456\n",
            "Train Epoch: 50 [352/600 (80%)]\tLoss: 0.550251\n",
            "Train Epoch: 55 [352/600 (80%)]\tLoss: 0.568970\n",
            "Train Epoch: 60 [352/600 (80%)]\tLoss: 0.472148\n",
            "Train Epoch: 65 [352/600 (80%)]\tLoss: 0.566336\n",
            "Train Epoch: 70 [352/600 (80%)]\tLoss: 0.466434\n",
            "Train Epoch: 75 [352/600 (80%)]\tLoss: 0.479890\n",
            "Train Epoch: 80 [352/600 (80%)]\tLoss: 0.399874\n",
            "Train Epoch: 85 [352/600 (80%)]\tLoss: 0.367115\n",
            "Train Epoch: 90 [352/600 (80%)]\tLoss: 0.302874\n",
            "Train Epoch: 95 [352/600 (80%)]\tLoss: 0.280315\n",
            "\n",
            "Test set: Average loss: 0.9830, Accuracy: 445/700 (63.57%)\n",
            "\n",
            "Acc over 10 instances: 65.69 +- 1.86\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ADdNAm1qK0q"
      },
      "source": [
        "###### Augmentation Fourth Attempt: TESTED\n",
        "\n",
        "In this attempt only original samples are passed through the image mixing or PatchShuffle augmentations. Essentially trying all the augmentations but keeping them all independent. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohkuv0kRrU4l",
        "outputId": "d59d7498-61db-4ce9-adb5-15f385b11c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "10600\n"
          ]
        }
      ],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(10600, 3, 32, 32)\n",
        "allSamplesL = torch.empty(10600)\n",
        "\n",
        "# The set becomes N^2 + 6N\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation  - 2N\n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation - 5N \n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "    # Third Augmentation - 6N\n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "print(len(allSamples))  \n",
        "\n",
        "# Image Mixing Augmentation \n",
        "for i in range(len(allSamples)):\n",
        "  point1 = allSamples[i]\n",
        "  for j in range(len(allSamples)):\n",
        "    point2 = allSamples[j]\n",
        "    avgImg = averageImages(point1, point2)\n",
        "    img, label = avgImg[0], avgImg[1]\n",
        "    img = torch.from_numpy(img)\n",
        "    allSamplesT[tensorIndex] = img\n",
        "    allSamplesL[tensorIndex] = label\n",
        "    tensorIndex += 1\n",
        "\n",
        "print(tensorIndex)\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0\n",
        "\n",
        "# Makes the set 4((6N)^2 + 6N)\n",
        "# for sample in allSamples:\n",
        "#     # Fourth Augmentation \n",
        "#     RGB = isolateRGB(sample)\n",
        "#     allSamples.append(RGB[0])\n",
        "#     allSamples.append(RGB[1])\n",
        "#     allSamples.append(RGB[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6AMZrS0-2yT"
      },
      "source": [
        "Performance on set of size N^2 + 6N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFj_g-m4-6k5",
        "outputId": "86f158e5-3a5f-4413-c791-cc3279fb954b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.663527\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.463868\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.324130\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.349264\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.365799\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.381983\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.484144\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.385368\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.358300\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.395377\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.441888\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.300268\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.344399\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.331104\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.440514\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.380356\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.375819\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.339792\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.327838\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.298423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8351, Accuracy: 454/700 (64.86%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.692713\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.446631\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.421598\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.424688\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.365492\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.440624\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.421309\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.323700\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.390913\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.383265\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.366609\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.395695\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.365585\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.322023\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.377845\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.356799\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.364799\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.319904\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.383359\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.371430\n",
            "\n",
            "Test set: Average loss: 0.8737, Accuracy: 433/700 (61.86%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.702215\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.577065\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.437337\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.342676\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.351136\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.415808\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.410451\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.379819\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.355300\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.334729\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.366151\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.349316\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.386657\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.378946\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.394872\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.378618\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.317656\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.311910\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.367759\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.382660\n",
            "\n",
            "Test set: Average loss: 0.8720, Accuracy: 452/700 (64.57%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.694956\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.511203\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.450626\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.345194\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.407948\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.434950\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.391038\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.366641\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.370193\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.442628\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.326982\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.329728\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.411473\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.337157\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.445485\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.370642\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.349273\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.437770\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.308135\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.302236\n",
            "\n",
            "Test set: Average loss: 0.7879, Accuracy: 459/700 (65.57%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.682358\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.499519\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.485111\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.331608\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.350317\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.321428\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.421923\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.358714\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.375606\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.340042\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.436373\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.376058\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.404782\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.370059\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.387907\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.369371\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.373555\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.339974\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.358340\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.367761\n",
            "\n",
            "Test set: Average loss: 0.8800, Accuracy: 445/700 (63.57%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.710365\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.525308\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.411790\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.357830\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.390791\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.439664\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.369195\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.384786\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.360735\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.419380\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.385554\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.392243\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.363140\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.322287\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.402398\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.371636\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.414720\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.326275\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.305268\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.399688\n",
            "\n",
            "Test set: Average loss: 0.8273, Accuracy: 449/700 (64.14%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.693183\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.576217\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.508725\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.438930\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.344349\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.376182\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.416607\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.401447\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.433685\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.342666\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.429186\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.383264\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.379218\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.318808\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.393239\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.285653\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.388927\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.368781\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.284331\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.393422\n",
            "\n",
            "Test set: Average loss: 0.7986, Accuracy: 459/700 (65.57%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.698843\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.583481\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.447195\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.468661\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.388211\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.454242\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.381654\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.370095\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.379708\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.417949\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.323560\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.367933\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.360077\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.322277\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.390655\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.374339\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.382904\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.335729\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.443975\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.371316\n",
            "\n",
            "Test set: Average loss: 0.8330, Accuracy: 442/700 (63.14%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.694170\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.516867\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.418859\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.423937\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.425643\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.383733\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.399801\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.360857\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.428991\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.315630\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.445718\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.317262\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.322213\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.344523\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.369310\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.337343\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.346495\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.352657\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.325827\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.363960\n",
            "\n",
            "Test set: Average loss: 0.8620, Accuracy: 441/700 (63.00%)\n",
            "\n",
            "Num Samples For Training 10600 Num Samples For Val 700\n",
            "Train Epoch: 0 [8528/10600 (99%)]\tLoss: 0.658418\n",
            "Train Epoch: 5 [8528/10600 (99%)]\tLoss: 0.468140\n",
            "Train Epoch: 10 [8528/10600 (99%)]\tLoss: 0.433680\n",
            "Train Epoch: 15 [8528/10600 (99%)]\tLoss: 0.371706\n",
            "Train Epoch: 20 [8528/10600 (99%)]\tLoss: 0.432895\n",
            "Train Epoch: 25 [8528/10600 (99%)]\tLoss: 0.356960\n",
            "Train Epoch: 30 [8528/10600 (99%)]\tLoss: 0.355691\n",
            "Train Epoch: 35 [8528/10600 (99%)]\tLoss: 0.354146\n",
            "Train Epoch: 40 [8528/10600 (99%)]\tLoss: 0.375105\n",
            "Train Epoch: 45 [8528/10600 (99%)]\tLoss: 0.450824\n",
            "Train Epoch: 50 [8528/10600 (99%)]\tLoss: 0.419345\n",
            "Train Epoch: 55 [8528/10600 (99%)]\tLoss: 0.362974\n",
            "Train Epoch: 60 [8528/10600 (99%)]\tLoss: 0.350538\n",
            "Train Epoch: 65 [8528/10600 (99%)]\tLoss: 0.335917\n",
            "Train Epoch: 70 [8528/10600 (99%)]\tLoss: 0.445164\n",
            "Train Epoch: 75 [8528/10600 (99%)]\tLoss: 0.291549\n",
            "Train Epoch: 80 [8528/10600 (99%)]\tLoss: 0.354602\n",
            "Train Epoch: 85 [8528/10600 (99%)]\tLoss: 0.324908\n",
            "Train Epoch: 90 [8528/10600 (99%)]\tLoss: 0.326730\n",
            "Train Epoch: 95 [8528/10600 (99%)]\tLoss: 0.330136\n",
            "\n",
            "Test set: Average loss: 0.8490, Accuracy: 448/700 (64.00%)\n",
            "\n",
            "Acc over 10 instances: 64.03 +- 1.12\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XMW6l24sz9b"
      },
      "source": [
        "###### Augmentation Fifth Attempt  (WORKING ON)\n",
        "\n",
        "Here the same process as 5 is repeated but the RBG isolation technique is also added in on original samples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGpnF4MvtICt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3046966b-47e0-45c3-c970-b70ac5fc06a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-09778d68e269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misolateRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mallSamplesT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorIndex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mallSamplesL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorIndex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtensorIndex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.FloatTensor"
          ]
        }
      ],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(360600, 3, 32, 32)\n",
        "allSamplesL = torch.empty(360600)\n",
        "\n",
        "# The set becomes N^2 + 9N\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesT[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation  - 2N\n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation - 5N \n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "    # Third Augmentation - 6N\n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Fourth Augmentation - 9N\n",
        "    RGB = isolateRGB(sample)\n",
        "\n",
        "    allSamplesT[tensorIndex] = RGB[0][0]\n",
        "    allSamplesL[tensorIndex] = RGB[0][1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    allSamplesT[tensorIndex] = RGB[1][0]\n",
        "    allSamplesL[tensorIndex] = RGB[1][1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    allSamplesT[tensorIndex] = RGB[2][0]\n",
        "    allSamplesL[tensorIndex] = RGB[2][1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "print(len(allSamples))  \n",
        "\n",
        "# Image Mixing Augmentation \n",
        "# Makes the set (6N)^2+6N\n",
        "for i in range(len(allSamples)):\n",
        "  point1 = allSamples[i]\n",
        "  for j in range(len(allSamples)):\n",
        "    point2 = allSamples[j]\n",
        "    avgImg = averageImages(point1, point2)\n",
        "    img, label = avgImg[0], avgImg[1]\n",
        "    img = torch.from_numpy(img)\n",
        "    allSamplesT[tensorIndex] = img\n",
        "    allSamplesL[tensorIndex] = label\n",
        "    tensorIndex += 1\n",
        "\n",
        "print(tensorIndex)\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0\n",
        "\n",
        "# Makes the set 4((6N)^2 + 6N)\n",
        "# for sample in allSamples:\n",
        "#     # Fourth Augmentation \n",
        "#     RGB = isolateRGB(sample)\n",
        "#     allSamples.append(RGB[0])\n",
        "#     allSamples.append(RGB[1])\n",
        "#     allSamples.append(RGB[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99htfdl5iTmb"
      },
      "source": [
        "###### Testing Augmentations Independently: Image Mixing - TESTED\n",
        "\n",
        "In this attempt, I try only using Image Mixing. The idea is that the technique might be worsened by the other augmentations. It will also help to guide the question of whether data quantity matters more.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC6vBszVjNxN",
        "outputId": "9c058d79-08bf-4006-9ca6-2283ec8d451b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(10000, 3, 32, 32)\n",
        "allSamplesL = torch.empty(10000)\n",
        "\n",
        "\n",
        "# Result in a set of size N^2\n",
        "tensorIndex = 0\n",
        "# Image Mixing Augmentation \n",
        "for i in range(len(train_data)):\n",
        "  point1 = train_data[i]\n",
        "  for j in range(len(train_data)):\n",
        "    point2 = train_data[j]\n",
        "    avgImg = averageImages(point1, point2)\n",
        "    img, label = avgImg[0], avgImg[1]\n",
        "    img = torch.from_numpy(img)\n",
        "    allSamplesT[tensorIndex] = img\n",
        "    allSamplesL[tensorIndex] = label\n",
        "    tensorIndex += 1\n",
        "\n",
        "\n",
        "print(tensorIndex)\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLupnnLq-uF9"
      },
      "source": [
        "Performance on set of size 10000: TESTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrWLR-VU-xbw",
        "outputId": "858e5295-e03b-47b0-de6b-1c9b89e3be59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.695857\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.292536\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.373371\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.383896\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.367283\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.177316\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.253645\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.303409\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.423574\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.364522\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.330531\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.471528\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.348307\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.311509\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.455536\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.482772\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.581650\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.431359\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.280814\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.296640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9080, Accuracy: 398/700 (56.86%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.657208\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.421814\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.638311\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.459616\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.495959\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.347256\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.431690\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.333331\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.377229\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.429976\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.302949\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.240309\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.236269\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.232860\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.373429\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.307543\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.499006\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.289204\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.345041\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.401070\n",
            "\n",
            "Test set: Average loss: 0.8787, Accuracy: 418/700 (59.71%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.629909\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.636984\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.300092\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.576591\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.357249\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.481918\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.510952\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.406901\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.482218\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.468959\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.375284\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.508876\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.318942\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.295081\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.466554\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.345881\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.418535\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.406497\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.519555\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.578785\n",
            "\n",
            "Test set: Average loss: 0.9356, Accuracy: 401/700 (57.29%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.753578\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.531052\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.324713\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.434193\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.608831\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.461575\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.432515\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.309247\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.497015\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.386477\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.320735\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.243530\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.449289\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.395352\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.471369\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.274973\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.284559\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.297553\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.388989\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.494311\n",
            "\n",
            "Test set: Average loss: 0.8637, Accuracy: 430/700 (61.43%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.664888\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.305947\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.456216\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.252664\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.377736\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.366948\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.416480\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.289268\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.316830\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.410759\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.401339\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.472559\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.204032\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.419438\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.230506\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.268929\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.509616\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.439225\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.335134\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.487047\n",
            "\n",
            "Test set: Average loss: 0.8412, Accuracy: 412/700 (58.86%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.671139\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.472909\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.400832\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.535876\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.385333\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.544928\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.499087\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.384050\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.396450\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.221050\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.310377\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.646331\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.585852\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.451910\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.500266\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.324473\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.323709\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.223747\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.297159\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.360303\n",
            "\n",
            "Test set: Average loss: 0.9042, Accuracy: 409/700 (58.43%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.747159\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.494654\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.248091\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.491830\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.242443\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.366196\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.131051\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.345366\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.575185\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.298449\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.389637\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.462012\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.400106\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.401957\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.362535\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.504719\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.409091\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.238017\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.436033\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.377879\n",
            "\n",
            "Test set: Average loss: 0.8372, Accuracy: 404/700 (57.71%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.564084\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.375517\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.385533\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.346934\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.324688\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.387314\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.360704\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.291750\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.307181\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.447448\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.436970\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.151746\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.323165\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.534405\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.223795\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.255223\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.414320\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.358838\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.177681\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.334206\n",
            "\n",
            "Test set: Average loss: 0.8904, Accuracy: 409/700 (58.43%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.688520\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.354188\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.555027\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.472079\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.389075\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.299819\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.389613\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.080633\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.463679\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.282989\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.292735\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.449355\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.333750\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.505506\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.359726\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.271480\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.355728\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.452548\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.427796\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.326495\n",
            "\n",
            "Test set: Average loss: 0.8988, Accuracy: 438/700 (62.57%)\n",
            "\n",
            "Num Samples For Training 10000 Num Samples For Val 700\n",
            "Train Epoch: 0 [1248/10000 (99%)]\tLoss: 0.707501\n",
            "Train Epoch: 5 [1248/10000 (99%)]\tLoss: 0.466855\n",
            "Train Epoch: 10 [1248/10000 (99%)]\tLoss: 0.487299\n",
            "Train Epoch: 15 [1248/10000 (99%)]\tLoss: 0.329136\n",
            "Train Epoch: 20 [1248/10000 (99%)]\tLoss: 0.414053\n",
            "Train Epoch: 25 [1248/10000 (99%)]\tLoss: 0.416047\n",
            "Train Epoch: 30 [1248/10000 (99%)]\tLoss: 0.285771\n",
            "Train Epoch: 35 [1248/10000 (99%)]\tLoss: 0.378287\n",
            "Train Epoch: 40 [1248/10000 (99%)]\tLoss: 0.357741\n",
            "Train Epoch: 45 [1248/10000 (99%)]\tLoss: 0.313995\n",
            "Train Epoch: 50 [1248/10000 (99%)]\tLoss: 0.551448\n",
            "Train Epoch: 55 [1248/10000 (99%)]\tLoss: 0.437963\n",
            "Train Epoch: 60 [1248/10000 (99%)]\tLoss: 0.252937\n",
            "Train Epoch: 65 [1248/10000 (99%)]\tLoss: 0.433954\n",
            "Train Epoch: 70 [1248/10000 (99%)]\tLoss: 0.411945\n",
            "Train Epoch: 75 [1248/10000 (99%)]\tLoss: 0.301227\n",
            "Train Epoch: 80 [1248/10000 (99%)]\tLoss: 0.476290\n",
            "Train Epoch: 85 [1248/10000 (99%)]\tLoss: 0.237902\n",
            "Train Epoch: 90 [1248/10000 (99%)]\tLoss: 0.502266\n",
            "Train Epoch: 95 [1248/10000 (99%)]\tLoss: 0.481235\n",
            "\n",
            "Test set: Average loss: 0.9391, Accuracy: 411/700 (58.71%)\n",
            "\n",
            "Acc over 10 instances: 59.00 +- 1.71\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiGz6lzu_5AL"
      },
      "source": [
        "###### Testing Augmentations Independently: Horizontal Flip - TESTED\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzLVUZUf_5AR"
      },
      "outputs": [],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(200, 3, 32, 32)\n",
        "allSamplesL = torch.empty(200)\n",
        "\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # First Augmentation  - 2N\n",
        "    flip = flipHorizontal(sample)\n",
        "    allSamples.append(flip)\n",
        "    allSamplesT[tensorIndex] = flip[0]\n",
        "    allSamplesL[tensorIndex] = flip[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2EN22sN_5AS"
      },
      "source": [
        "Performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862da193-f591-4ae1-ad51-8932a1b2a756",
        "id": "iD4YlueW_5AS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.305393\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.228265\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 0.925695\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.702102\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.590716\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.566845\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.448774\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.518920\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.550923\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.503271\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.522410\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.470818\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.414937\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.401721\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.349051\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.248904\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.300288\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.320198\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.196312\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.092491\n",
            "\n",
            "Test set: Average loss: 0.9819, Accuracy: 499/700 (71.29%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.286442\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.693387\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 0.921586\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.678032\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.698677\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.664643\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.648016\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.608002\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.640335\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.610884\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.632663\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.526404\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.526307\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.461923\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.591066\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.515928\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.426602\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.374585\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.281154\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.223281\n",
            "\n",
            "Test set: Average loss: 1.0235, Accuracy: 443/700 (63.29%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.274806\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 0.905020\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 0.865738\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.720528\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.684058\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.662243\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.582582\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.485676\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.502259\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.520917\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.505252\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.372503\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.366519\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.334466\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.308138\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.457457\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.267964\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.202597\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.150157\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.738578\n",
            "\n",
            "Test set: Average loss: 1.0566, Accuracy: 458/700 (65.43%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.276877\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 0.978337\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 1.420773\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.917022\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.729413\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.722497\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.695756\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.699325\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.689908\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.686239\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.678890\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.684376\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.636075\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.604917\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.630063\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.615121\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.639866\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.489379\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.568431\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.525504\n",
            "\n",
            "Test set: Average loss: 0.6641, Accuracy: 457/700 (65.29%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.313821\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.212497\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 1.158750\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.735391\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.685486\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.634014\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.529209\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.589463\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.661807\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.538881\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.547983\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.545996\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.575010\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.629365\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.458144\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.497933\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.493415\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.424877\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.452371\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.356231\n",
            "\n",
            "Test set: Average loss: 0.7955, Accuracy: 457/700 (65.29%)\n",
            "\n",
            "Acc over 10 instances: 66.11 +- 2.71\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "# Reduced seed range here due to excessive compute\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzlPNAwyA3gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ5aOdlSA1wN"
      },
      "source": [
        "###### Testing Augmentations Independently: Rotation - TESTED\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcQmI7BbA1wd"
      },
      "outputs": [],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(400, 3, 32, 32)\n",
        "allSamplesL = torch.empty(400)\n",
        "\n",
        "# The set currently becomes 4N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Second Augmentation - 5N \n",
        "    rotate90 = rotatePoint(sample, 90)\n",
        "    allSamples.append(rotate90)\n",
        "    allSamplesT[tensorIndex] = rotate90[0]\n",
        "    allSamplesL[tensorIndex] = rotate90[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate180 = rotatePoint(sample, 180)\n",
        "    allSamples.append(rotate180)\n",
        "    allSamplesT[tensorIndex] = rotate180[0]\n",
        "    allSamplesL[tensorIndex] = rotate180[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    rotate270 = rotatePoint(sample, 270)\n",
        "    allSamples.append(rotate270)\n",
        "    allSamplesT[tensorIndex] = rotate270[0]\n",
        "    allSamplesL[tensorIndex] = rotate270[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I0qYc2rA1we"
      },
      "source": [
        "Performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3986767-e7a3-4d21-9409-d82c701cdde4",
        "id": "nlQTk6SsA1we"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.258873\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.797519\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.657878\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.608598\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.611185\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.587426\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.583778\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.576386\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.692542\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.667150\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.483062\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.456198\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.425293\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.345842\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.384416\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.525832\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.177597\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.378766\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.149796\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.069188\n",
            "\n",
            "Test set: Average loss: 1.4139, Accuracy: 455/700 (65.00%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.227476\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.966284\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.695467\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.690513\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.689568\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.683588\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.714765\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.657788\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.783180\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.784847\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.720234\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.620200\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.556724\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.552913\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.369884\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.554415\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.503926\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.412506\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.368022\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.418464\n",
            "\n",
            "Test set: Average loss: 0.9295, Accuracy: 438/700 (62.57%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.213468\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.771978\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.696513\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.609907\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.565187\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.637294\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.575875\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.609823\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.596718\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.660429\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.468916\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.431866\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.456845\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.415162\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.208720\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.444749\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.723978\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.337735\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.295491\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.127222\n",
            "\n",
            "Test set: Average loss: 0.9794, Accuracy: 479/700 (68.43%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.253880\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.920364\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.698762\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.651802\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.757697\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.562560\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.569866\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.545453\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.497447\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.788440\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.504226\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.977725\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.467704\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.256372\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.243176\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.163909\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.226539\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.114022\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.042539\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.055521\n",
            "\n",
            "Test set: Average loss: 1.5745, Accuracy: 465/700 (66.43%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.182598\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.947264\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.681979\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.707189\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.694861\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.690561\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.693644\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.654918\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.623590\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.647196\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.602881\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.506213\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.762022\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.763010\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.777302\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.613991\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.558555\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.443279\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.622325\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.531241\n",
            "\n",
            "Test set: Average loss: 0.7028, Accuracy: 465/700 (66.43%)\n",
            "\n",
            "Acc over 10 instances: 65.77 +- 1.94\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "# Reduced seed range here due to excessive compute\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sVBtZixBBiC"
      },
      "source": [
        "###### Testing Augmentations Independently: PatchShuffle - TESTED\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpl38XQEBBiD"
      },
      "outputs": [],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(200, 3, 32, 32)\n",
        "allSamplesL = torch.empty(200)\n",
        "\n",
        "# The set currently becomes 2N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamples.append(sample)\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "    \n",
        "    # Third Augmentation - 2N\n",
        "    patch = PatchShuffle(sample)\n",
        "    allSamples.append(patch)\n",
        "    allSamplesT[tensorIndex] = patch[0]\n",
        "    allSamplesL[tensorIndex] = patch[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyABd1nyBBiE"
      },
      "source": [
        "Performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93a611c-67c4-4d2d-a74c-7cd7b32887dc",
        "id": "K5qn2kuNBBiF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.271960\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.477528\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 3.708284\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.980311\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.702991\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.696860\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.701446\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.651991\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.669048\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.664618\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.655785\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.613575\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.610391\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.578833\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.540518\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.543510\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.512203\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.410833\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.476515\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.442851\n",
            "\n",
            "Test set: Average loss: 0.8759, Accuracy: 406/700 (58.00%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.293069\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.416731\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 1.485411\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.795688\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.811621\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.727974\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.685937\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.675299\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.655844\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.665248\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.642904\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.604199\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.634742\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.631318\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.602340\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.519838\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.541409\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.585357\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.573093\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.506485\n",
            "\n",
            "Test set: Average loss: 0.7462, Accuracy: 411/700 (58.71%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.301626\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 0.932865\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 0.796224\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.735760\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.685521\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.705925\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.677506\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.695622\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.691776\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.627330\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.604120\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.648100\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.607421\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.628702\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.618138\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.544764\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.592260\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.578061\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.514838\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.391120\n",
            "\n",
            "Test set: Average loss: 0.8233, Accuracy: 402/700 (57.43%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.321296\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.919319\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 0.718294\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.870101\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.785274\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.715506\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.701711\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.664231\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.695049\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.666691\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.669696\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.678819\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.661928\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.608032\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.582118\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.541054\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.607161\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.546466\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.514554\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.531817\n",
            "\n",
            "Test set: Average loss: 0.6929, Accuracy: 424/700 (60.57%)\n",
            "\n",
            "Num Samples For Training 200 Num Samples For Val 700\n",
            "Train Epoch: 0 [72/200 (50%)]\tLoss: 2.295424\n",
            "Train Epoch: 5 [72/200 (50%)]\tLoss: 1.731786\n",
            "Train Epoch: 10 [72/200 (50%)]\tLoss: 1.008464\n",
            "Train Epoch: 15 [72/200 (50%)]\tLoss: 0.673934\n",
            "Train Epoch: 20 [72/200 (50%)]\tLoss: 0.696931\n",
            "Train Epoch: 25 [72/200 (50%)]\tLoss: 0.670172\n",
            "Train Epoch: 30 [72/200 (50%)]\tLoss: 0.655453\n",
            "Train Epoch: 35 [72/200 (50%)]\tLoss: 0.613913\n",
            "Train Epoch: 40 [72/200 (50%)]\tLoss: 0.605107\n",
            "Train Epoch: 45 [72/200 (50%)]\tLoss: 0.621511\n",
            "Train Epoch: 50 [72/200 (50%)]\tLoss: 0.591810\n",
            "Train Epoch: 55 [72/200 (50%)]\tLoss: 0.557411\n",
            "Train Epoch: 60 [72/200 (50%)]\tLoss: 0.544446\n",
            "Train Epoch: 65 [72/200 (50%)]\tLoss: 0.518402\n",
            "Train Epoch: 70 [72/200 (50%)]\tLoss: 0.428053\n",
            "Train Epoch: 75 [72/200 (50%)]\tLoss: 0.451998\n",
            "Train Epoch: 80 [72/200 (50%)]\tLoss: 0.436834\n",
            "Train Epoch: 85 [72/200 (50%)]\tLoss: 0.330198\n",
            "Train Epoch: 90 [72/200 (50%)]\tLoss: 0.270033\n",
            "Train Epoch: 95 [72/200 (50%)]\tLoss: 0.229437\n",
            "\n",
            "Test set: Average loss: 1.1472, Accuracy: 423/700 (60.43%)\n",
            "\n",
            "Acc over 10 instances: 59.03 +- 1.27\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "# Reduced seed range here due to excessive compute\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBQYNE3zFaNt"
      },
      "source": [
        "###### Testing Augmentations Independently: Color Isolation - TESTED\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a865713-4850-4284-96e2-6a49bccadf12",
        "id": "RJGQPCJiFaN1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ],
      "source": [
        "# Augmenting Training Set\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# For storing the samples and generated samples in between steps\n",
        "allSamples = []\n",
        "\n",
        "# ------>  Size of tensor would need to change for different input sizes (here assumes 100)\n",
        "allSamplesT = torch.empty(400, 3, 32, 32)\n",
        "allSamplesL = torch.empty(400)\n",
        "\n",
        "# The set currently becomes 6N where N is 100\n",
        "tensorIndex = 0\n",
        "for sample in train_data:\n",
        "    # Appending the original samples - N\n",
        "    allSamplesT[tensorIndex] = sample[0]\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "\n",
        "    # Fourth Augmentation \n",
        "    RGB = isolateRGB(sample)\n",
        "    allSamples.append(RGB[0])\n",
        "    allSamples.append(RGB[1])\n",
        "    allSamples.append(RGB[2])\n",
        "\n",
        "for sample in allSamples:\n",
        "    allSamplesT[tensorIndex] = torch.from_numpy(sample[0])\n",
        "    allSamplesL[tensorIndex] = sample[1]\n",
        "    tensorIndex += 1\n",
        "   \n",
        "\n",
        "print(tensorIndex)\n",
        "allSamples = TensorDataset(allSamplesT, allSamplesL)\n",
        "\n",
        "# Free memory\n",
        "allSamplesT = 0\n",
        "allSamplesL = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUVf-6z7FaN2"
      },
      "source": [
        "Performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bea33e-0f51-4cbf-c359-c211da7ab5af",
        "id": "OvYI6SgnFaN2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.219952\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.774265\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.681632\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.739388\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.550577\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.656374\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.616016\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.513901\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.313403\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.513899\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.367290\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.252869\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.105109\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.403444\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.032661\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.005320\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.010663\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.026192\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.034583\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.330638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0438, Accuracy: 402/700 (57.43%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.249136\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.658873\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.625060\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.671216\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.837758\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.690276\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.749813\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.567002\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.614080\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.450116\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.474037\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.438748\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.208759\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.152993\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.061572\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.076897\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.073512\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.031268\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.003571\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.052772\n",
            "\n",
            "Test set: Average loss: 2.4458, Accuracy: 434/700 (62.00%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.256010\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.910248\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.604587\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.693245\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.688242\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.706742\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.665846\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.757989\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.614688\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.559099\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.483283\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.508175\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.471143\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.431616\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.220865\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.138044\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.133362\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.052561\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.019732\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.004325\n",
            "\n",
            "Test set: Average loss: 5.3175, Accuracy: 429/700 (61.29%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.238156\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.767832\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.701994\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.676148\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.708941\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.716170\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.709080\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.688167\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.660125\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.654135\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.570032\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.576811\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.538064\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.461944\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.206881\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.339541\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.330859\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.108205\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.182617\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.287395\n",
            "\n",
            "Test set: Average loss: 2.8664, Accuracy: 384/700 (54.86%)\n",
            "\n",
            "Num Samples For Training 400 Num Samples For Val 700\n",
            "Train Epoch: 0 [48/400 (75%)]\tLoss: 2.270506\n",
            "Train Epoch: 5 [48/400 (75%)]\tLoss: 0.705857\n",
            "Train Epoch: 10 [48/400 (75%)]\tLoss: 0.632894\n",
            "Train Epoch: 15 [48/400 (75%)]\tLoss: 0.606125\n",
            "Train Epoch: 20 [48/400 (75%)]\tLoss: 0.766032\n",
            "Train Epoch: 25 [48/400 (75%)]\tLoss: 0.592530\n",
            "Train Epoch: 30 [48/400 (75%)]\tLoss: 0.623588\n",
            "Train Epoch: 35 [48/400 (75%)]\tLoss: 0.412243\n",
            "Train Epoch: 40 [48/400 (75%)]\tLoss: 0.537608\n",
            "Train Epoch: 45 [48/400 (75%)]\tLoss: 0.618125\n",
            "Train Epoch: 50 [48/400 (75%)]\tLoss: 0.496717\n",
            "Train Epoch: 55 [48/400 (75%)]\tLoss: 0.451722\n",
            "Train Epoch: 60 [48/400 (75%)]\tLoss: 0.158202\n",
            "Train Epoch: 65 [48/400 (75%)]\tLoss: 0.215425\n",
            "Train Epoch: 70 [48/400 (75%)]\tLoss: 0.122317\n",
            "Train Epoch: 75 [48/400 (75%)]\tLoss: 0.045107\n",
            "Train Epoch: 80 [48/400 (75%)]\tLoss: 0.032031\n",
            "Train Epoch: 85 [48/400 (75%)]\tLoss: 0.007299\n",
            "Train Epoch: 90 [48/400 (75%)]\tLoss: 0.002918\n",
            "Train Epoch: 95 [48/400 (75%)]\tLoss: 0.003004\n",
            "\n",
            "Test set: Average loss: 6.7363, Accuracy: 421/700 (60.14%)\n",
            "\n",
            "Acc over 10 instances: 59.14 +- 2.65\n"
          ]
        }
      ],
      "source": [
        "accs = []\n",
        "\n",
        "# Reduced seed range here due to excessive compute\n",
        "for seed in range(5):\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(tensorIndex,val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(allSamples,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6swn7ns0tc0"
      },
      "source": [
        "### Method 2 Synthetic Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "bDP_O8X8zRy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU10TQVa00Fx"
      },
      "outputs": [],
      "source": [
        "# example of a dcgan on cifar10\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA SPLIT"
      ],
      "metadata": {
        "id": "9mKpLE6vzctU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Loading data\n",
        "  (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "  # Value normalization\n",
        "  X_train  = X_train/255\n",
        "  X_test  = X_test/255\n",
        "\n",
        "  #extract indexes of 2 classes (0 and 1 in his case)\n",
        "  classes_indx=[]\n",
        "  for i in range(len(y_train)):\n",
        "    if y_train[i][0]==0 or y_train[i][0]==1:\n",
        "      temp=[]\n",
        "      temp.append(i)\n",
        "      classes_indx.append(temp)\n",
        "\n",
        "  classes_indx_test=[]\n",
        "  for i in range(len(y_test)):\n",
        "    if y_test[i][0]==0 or y_test[i][0]==1:\n",
        "      temp=[]\n",
        "      temp.append(i)\n",
        "      classes_indx_test.append(temp)\n",
        "\n",
        "  #pick only 2 classes for train and test\n",
        "  X_train_2classes=[]\n",
        "  for i in classes_indx:\n",
        "    X_train_2classes.append(X_train[i[0]])\n",
        "\n",
        "  y_train_2classes=[]\n",
        "  for i in classes_indx:\n",
        "    y_train_2classes.append(y_train[i[0]])\n",
        "\n",
        "  X_test_2classes=[]\n",
        "  for i in classes_indx_test:\n",
        "    X_test_2classes.append(X_test[i[0]])\n",
        "\n",
        "  y_test_2classes=[]\n",
        "  for i in classes_indx_test:\n",
        "    y_test_2classes.append(y_test[i[0]])\n",
        "\n",
        "  #pick 100 samples\n",
        "  X_train_2classes_100 = X_train_2classes[:100]\n",
        "  y_train_2classes_100 = y_train_2classes[:100]\n",
        "\n",
        "  X_test_2classes_100 = X_train_2classes[101:801]\n",
        "  y_test_2classes_100 = y_train_2classes[101:801]\n",
        "\n",
        "  #lists to np arrays\n",
        "  X_train_2classes_100 = np.asarray(X_train_2classes_100)\n",
        "  y_train_2classes_100 = np.asarray(y_train_2classes_100)\n",
        "\n",
        "  X_test_2classes_700 = np.asarray(X_test_2classes_100)\n",
        "  y_test_2classes_700 = np.asarray(y_test_2classes_100)"
      ],
      "metadata": {
        "id": "q2l-kYUhlcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get 50 samples of target class. Receives integer from 0-9, return 50 samples of the appropriate CIFAR10 calss"
      ],
      "metadata": {
        "id": "n4keV6d0zl7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_train_dataset(target_class):\n",
        "  (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "  #extract indexes of 2 classes (0 and 1 in his case)\n",
        "  classes_indx=[]\n",
        "  for i in range(len(y_train)):\n",
        "    if y_train[i][0]==target_class:\n",
        "      temp=[]\n",
        "      temp.append(i)\n",
        "      classes_indx.append(temp)\n",
        "\n",
        "\n",
        "  #pick only 1 classes for train and test\n",
        "  X_train_target_class=[]\n",
        "  for i in classes_indx:\n",
        "    X_train_target_class.append(X_train[i[0]])\n",
        "\n",
        "  #pick 50 samples\n",
        "  X_train_target_class_50 = X_train_target_class[:50]\n",
        "  #lists to np arrays\n",
        "  X_train_target_class_50 = np.asarray(X_train_target_class_50)\n",
        "  return X_train_target_class_50"
      ],
      "metadata": {
        "id": "3_Tzg0n-zXbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATOR AND DISCRIMINATOR DEFINITION FOR SYNTHETIC DATA CREATION (**CLASS 1**) USING TRAINED MODEL. THE MODEL TRAINED ONLY ON 50 SAPLES/2CLASSES AS PER REQUIREMENTS"
      ],
      "metadata": {
        "id": "ZpczwGBp0bjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_train_dataset(target_class):\n",
        "  (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "  #extract indexes of 2 classes (0 and 1 in his case)\n",
        "  classes_indx=[]\n",
        "  for i in range(len(y_train)):\n",
        "    if y_train[i][0]==target_class:\n",
        "      temp=[]\n",
        "      temp.append(i)\n",
        "      classes_indx.append(temp)\n",
        "\n",
        "\n",
        "  #pick only 1 classes for train and test\n",
        "  X_train_target_class=[]\n",
        "  for i in classes_indx:\n",
        "    X_train_target_class.append(X_train[i[0]])\n",
        "\n",
        "  #pick 50 samples\n",
        "  X_train_target_class_50 = X_train_target_class[:50]\n",
        "  #lists to np arrays\n",
        "  X_train_target_class_50 = np.asarray(X_train_target_class_50)\n",
        "  return X_train_target_class_50"
      ],
      "metadata": {
        "id": "ChzvBRbqcsfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt0VE72z1tvU",
        "outputId": "1a44c0de-4d71-4c69-c6e2-b7d3a1c67b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5/5 [==============================] - 0s 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "\tmodel = Sequential()\n",
        " \n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\tn_nodes = 256 * 4 * 4\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((4, 4, 256)))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and discriminator model\n",
        "def define_gan(g_model, d_model):\n",
        "\td_model.trainable = False\n",
        "\tmodel = Sequential()\n",
        " \t\n",
        "\t# add generator\n",
        "\tmodel.add(g_model)\n",
        "\t# add the discriminator\n",
        "\tmodel.add(d_model)\n",
        "\t# compile the model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def load_real_samples():\n",
        "\t\n",
        "\t# load cifar10 dataset\n",
        "\t(trainX, _), (_, _) = load_data()\n",
        " \n",
        "\ttrainX = custom_train_dataset(0)\n",
        "\tX = trainX.astype('float32')\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X\n",
        "\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "\t# prepare real samples\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\t#  fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(filename)\n",
        "\n",
        "#  generator and discriminator training\n",
        "def train(g_model, device, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=2): \n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\td_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "latent_dim = 1000\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "\n",
        "# generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(g_model,device, d_model, gan_model, dataset, latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "SwsQmadZ5GCt",
        "outputId": "ceede874-b054-4eeb-9f0e-45f6f679a471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "def create_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "    \n",
        "\t\tpyplot.imshow(examples[i, :, :])\n",
        "  \n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = load_model('generator_model_200.h5')\n",
        "# generate images\n",
        "latent_points = generate_latent_points(1000, 1000)\n",
        "# generate images\n",
        "X = model.predict(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "pyplot.figure(figsize=(15, 15), dpi=80)\n",
        "create_plot(X, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATOR AND DISCRIMINATOR DEFINITION FOR SYNTHETIC DATA CREATION (**CLASS 2**) USING TRAINED MODEL. THE MODEL TRAINED ONLY ON 50 SAPLES/2CLASSES AS PER REQUIREMENTS"
      ],
      "metadata": {
        "id": "GcIKoHL56lzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b915f7e0-4564-4dc8-c1bd-bff5e09f6732",
        "id": "9upg4snW8zoU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\tn_nodes = 256 * 4 * 4\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((4, 4, 256)))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\treturn model\n",
        "\n",
        "def define_gan(g_model, d_model):\n",
        "\td_model.trainable = False\n",
        "\tmodel = Sequential()\n",
        " \t\n",
        "\t#  generator\n",
        "\tmodel.add(g_model)\n",
        "\t# discriminator\n",
        "\tmodel.add(d_model)\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def load_real_samples():\n",
        "\t(trainX, _), (_, _) = load_data()\n",
        " \n",
        "\ttrainX = custom_train_dataset(1)\n",
        "\t# convert from unsigned ints to floats\n",
        "\tX = trainX.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\t\n",
        "\treturn X\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=200):\n",
        "\t# prepare real samples\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\t# prepare fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(filename)\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, device, d_model, gan_model, dataset, latent_dim, n_epochs=150, n_batch=2): #!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\td_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\td_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\n",
        "\n",
        "\n",
        "# latent space\n",
        "latent_dim = 1000\n",
        "# discriminator\n",
        "d_model = define_discriminator()\n",
        "\n",
        "#  generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(g_model,device, d_model, gan_model, dataset, latent_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATE 1000 SAMPLES  OF CLASS 2"
      ],
      "metadata": {
        "id": "haEzHAk_2U-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# plot the generated images\n",
        "def create_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "    \n",
        "\t\tpyplot.imshow(examples[i, :, :])\n",
        "  \n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = load_model('generator_model_200.h5')\n",
        "# generate images\n",
        "latent_points = generate_latent_points(1000, 1000)\n",
        "# generate images\n",
        "X1 = model.predict(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X1 = (X + 1) / 2.0\n",
        "# plot the result\n",
        "pyplot.figure(figsize=(15, 15), dpi=80)\n",
        "create_plot(X1, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "BJrxs55Q6oM9",
        "outputId": "db21ac22-61d8-4924-f8a2-5b7e22878f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 25 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAOaCAYAAABz2rTKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9ua81y5qnFUMOa609fcM599StqlvdVd10S60GEwthIYSHEEI4uPxHOAgHGwmnJQwMhJCQGhwwusWgUlfXrao7nXO+YU9rrZwiMEBImb/n03fb4WZxf4+3Q7kyMyLemPbe77NirTUYY4wxxhhjjDF7JP2uX8AYY4wxxhhjjPkSPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktzdcu+O/+6Z/LF7nGQb/bdW5aKWtjkbKhZn3IMq5+LFEvyXGWsnHW+4dxgg/rM3Ojn51LL2Up6HNTgvul9XNL0PZoyiJlS+70XgHeDdqtCXq/Cu/WJKqD1rWLo5SFsn6//qCXpKr3aptByq5Fw+10q3X4R3//zyACfjf8z//LzyXYU9XXK1n7O1bon6jXdXF7Hdw/aFx37VHK0qR9mALE2FGf0QSI/0afWxeNsdqs61DgmeH5IkUl6u/N6hHGV9HrCsRrWuC6rP0wQx3SfNV3WdYx2530/k3VuM5J4/8V3u10p3Ppz/7wJ7uJ/xBC+N//xau8ZDvpHDVlrV/Xaf0KtH3czrPQVrGDeSxqzJYXbXsYUiH0+h5dhCWx0eeWWW9YN3G2QD3DRd+tNvrM1Ov9M8VZ1neL0HZzgnE8w3PHFylrwnqe6d7ou/WzjvfU6Xg6T3rd8Y0Uhdub027GwP/5L18kiLtB2/OatD27TPsMiO3t3gDWmNjqPDYVjbHyrDEWI6wpsJY3sEaHqvdb4P221X++aBvNF10DJvrbCbTbOGhZm/UZS9Q5J866VsDrhQaeWzb7mzbpNQvs93LQZ46L9mHf6v3+nX/739xN/IcQwn/+X/0TadTpVd+7whwVYV3I+Z2ULWU997QwaVfYG2fqj0VjthSNi9DAfmHQOTU3+tlUtc+3J6o2wxwL55EI+/YK9Zpg3slwzmpgLmoyjNlOJ4E6a4yWzfxETVnhOBbg/NQkfY+20374j/79fxfHgP/SaowxxhhjjDFmt/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt3w1p7XMkJcDZ90cNVdlpny4C+R1bv6fuxb9P+gF/oca/l06LEn/RzvBP2BPcMMK/5Rd4X/DS4E8ik3OKaTMhthAzuCi/7Y9Q17FAjlktdP/qU+QNzsFyHOF9hwgNzXm9YVlgv9Hb7UjZsgXKUFznC6UwLwjllljfYE8ohi1jHJrlgXaanNdhT4MkL98HbU9M8R67PSz8Qz5Db12WlMgx2fUXJ1t3miC3JNxgRiG68oz5EFAjh+kaYU0Qs4gTGEzvctIuVXr+zVnyIOE9g0TtHnR/poph2xnTFXbZbponDW3et3rrH05weTQbj46QA5aGrVNU0tjTIoCpAyF6VmfcbzT6+Kszxiv2pdlO1ag3SgXNtN6MlJOo7blEfLFygVysyFvbh5hLjpDjmz7vPr5APmrpwcpCs0VFpnLqxQNd3q/W/3k74wKa9TLALn4kOJ2hXyzEeK426wBE+SV1QHmFMgPWyh/FTZLBfrnCBsD2nqFGfJcN3Pq9UprHayTMEdUymeE9TSAE2HrSAkhhAKLQIWJYinQiZu1d4Q8yLjAGIaWW6AfXkds4V2RwRMwT+rUqB3kUvawxg+P+ozNnnzOMI9BjihRYX0PkNcfk/ZlgtzPDPFInoBtuio5ZgJ4EwrkuacG9mjiPwkhwRpQae2hulJyKjlVNtfNsB+Nle4F4w6umqD+X8J/aTXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1u+LmIC6UyA5P8KCfYRkqHhe8/DsknqL/A5SvKtkDC9UEJ8UdFDBwnCc4QvrwdhU4F3aTfyg7Q1i4QQBhIkQCJ0muFLheH3Cx200wzCgQSCqQgyAfxC+42EIpFcCJK+yY8AvpEvfSPxjoCKNPCF7tAGFeKExE55I96asA+1bxrow9xB4vsEdYCk/BmkSyQKqzAnzJv6dx0Iu0AscD1Du8HrjlkT/w8QOhNIDhKIeSIIsZoDtN1lLeag8TpDvXr4knUawxnlBfuihfdeDiAngtiuMFbQkZE2wjeIuwUkRj1IKBoQ2Y0gztnK80II4ZUkOTAfD7B+hM246EDCE0EcVUZYO2FtqxOsgSAjnEkeCGGWE4io3oAs8NPmw9DmMOxCN2jhK+wT8s7XgAbiv3YQT9AI1GcR5tntNmMBMUklWQvM4zjhjfpu5LQpCdYsqMOSdE8VNsKykkDAA0LJGTaFMIR5PQVpZ4WNRpyhb0DkV6DttiKuBGK2CfaOEeYwat8KbbI3XkDAmUDGU5uTfvhKnalzz7yRKE4gYmoWWE9gr0Hiygk+i/sqmCtRDgvPkCNVggn6CDG7qECWAjRlnZ+bqu8xNyBsQoGoPnaB+y3bMx8IeivENtWBRLOZNgVfwH9pNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizW74qYqogXRhHkAdB8nsDIopxGqQsLJukbBAzUFJ/BbFPhWTrmPTCCT68Tbj/fz6tJWSd2CQhL5CsHwokbsOvDWawZhRokxnEJwlEHxWyrWsCIUajcoW6TcoGiQRKskCuUCEBu+5cwrFA/GM/grCrgiiJJF7TpgzbBN6j5FYva0EAhBIWLRtBCFGgvym022793CvInzLIZaJ6BVCEMMza5udJy3Kn9SfpWCDZFbTdshGPHWhuCiBnghhZoP4kjNgbBWVK0A4wtQcQ1gRYF7atsIDAoYf3CCCnCSQog1dLMM9OIBy5DnAdyLe6jXxvgPWpgfctsMbgGABBRoVxQbLDlub7/kbKbh+gpd6s+6t9hXm8wNjJ2gA4d4Dcb0/EFuKf1mN1y+A6O160vlsZ3whTRUvjkGQtsN4v4E1qYC0aYE6tICeDbVZIm3G9wLy40NoZVEyzFfuFEEIBGU4ieSCMJ5LmVFizG1iQxo00qID9DCMY1pgI7zbDuN4bEYRf03QnZS1JH2ndozjYxG0D2yDaB4cR9vIBNhYgyypFB23NMFiy9mVLEqPNmhLh3BIm2LfRdSTthLkyw7vlADKyDFEK81MTj1JWl8vq5xHGU4bDTATbW4rU91r0JfyXVmOMMcYYY4wxu8WHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu+WrIqYZ5ETgfgjDVRN/+17L2qQfnjYJ8ZBbHypIYgZIym4gJZ5kMpVkSpBc3AZNmi5gycjzOqF5oTxwaMsCzzxk/XBDycugFymQMB8XkJr0eh04bELYJJYnuH8Bu0aFLPoUIbG8haT3HUGOBIr/OkMbw/06kMTEze+OZhDVxFnLOrApJRBzUL8mGCcD2DWgu0MksdemoaivW2ijADIQcNyECeQI0EwhJpBwgHAjwdyhIz2IDSKCIIQcCgVEJTT3VVQE7YsWYqqCJGIEYdGhBxkL9FHYtOEE0rYOxgC4b8IdyI6GTANZi0aQVaRGB9Cxh7l8Wt8wQtc28MxCz4Rx0cENY/tbyt7ACnUHAo8jjNvabQQ7ME8sMLZzB/ISdT+x7G5H0LqdBhBPQXxW2ivA2ruN/0xzVgLhG0r2aJ9BqiAtIydWgfke3TqbOWGhIQcCHtzvwcK7lKuUJRhPlfZx6E7U+8WqM8q8rK/rYC1aYMyh/IkaGNp3b8ywiaiztt8MIrsO6tdAh9SNaGuC4E7Q9gVEqxFksSRzDemg14FArmS9juKsKetdH3jNQg3abiHpbrGDNXCBFW+77oQQQsxa1sHGqoG1LYBobNz0DfVpgflpK2cLIYSEYszffg3Y92phjDHGGGOMMeb3Gh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbslq+KmJZpkLL5AuIdyCVPcPvuoHKGy7hJ1l30maHVe7WQCD2NIGyJmli9gNgJvBRhAmFTM0Hid7tukwzipNSo6mUBGUImMQcIi+oE96OkdMj9TyiT0t9hzJvfa5QJ+j5Agje0Wz6SmEDfbU9UsElEkGRkkiJBsvoCAqC0aasWOix1ENctSWNA9IRiMy0jcUQK+txDS2NnIyKDuG4qiGRAXgK+opC3ppIQQu5BnQSmG2jyUElCAsKVLfOVBA/6ubYFqVVHsg54uZ0xQQMOVxJZgQAJ1ooR+igt677MMI8tjbZpBuFEBblEs11jQiAPTagLyXQ0zjoQO+WNxGiEgdf3JPCB+YR8LfQr5qjvlloQiYyghQMpUgGByXZZmF/0mleQhvQgxDoe9ZkksdkTZ+ifyzMIhWA6IsnQVDRm82YNSCCNiSQyJBlMAnkkTPh1BOkWxH+D+xba36w/W2nMge5uKRcpi2Cyi9CYJEVMs+4fR9jc1aRSm2WC9XPzkKHq/UmwBNuEkGDNLsvfgr8dXX6UoulyL2W50UllyTro8xFEPufz6ueYqM/01bZ7jxBCqK2KkyLYw/Ki1+UG+hcGN8nNms0+vQHbXyy6BkSSb1adsyOYx2LV+6VCElzYk8M5YIa9Yd68c4X9Ls1FKJokHxZZC7/A34LRYowxxhhjjDHm9xUfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7Jav5rROi/6Ds2YghHCi/yuHLxxf4H+cj5t81Qg5OZS7M9CXfkP+xQz/t9629AW/8Fj4/+s26YX9Joettvpu06I5FP0I/ysP/2deINeiQK5uoiTRRv/XfBghJ+UA/6e+yUuqDXzhMeQqQppfSIsWRsj93RMzfDv0BKOG6lbpn/ch57RugqxCXFOO9BVCuMIXSIcF8sHp+52rxiLl0WHez+bL4AvkOY+ULwh5bwm+pLpAjI2Q0JQgB5nyNhZovAv1zbJ+boRcswr3nyBXt4PcvUQJ5zuD8v/PHcwDF22/OxgXBXKVmrLuN5oVbiAuXiHHfrjqPDtBjlwPeZgR1rF5gvvBPBs3qVvLRe//Al/cHkd9Zgf5kTMMxqVCHhHkYKaoz336pJ+dW82jStP6/SjfcoEc7gBjYIY8/27nY6Bcte0uB+hHmGcT9MUM+WAxbOdPyJeDOXWCObVAUhrl3W+fGQIuFWEeoa6QIr2dyyLlvcG9lqA3y5CDO0GslwJrBcRTBE/K/Kp5mk2r+ZfLJqmb/tJTIN++ge31BHu27fq/R5aobosmw77iqH3ZkmMA8ivb03qfTnuIMOteHkIgpEb7scL6W2BvTPtZEiDUqm0yb/KTKdeZxl2TdcKfqp60eliLI+z52nQnZcsAc0U6S1kLZ61lM29XWgMg4XjrawkBj3Ih/RY+kf/32t/6SmOMMcYYY4wx5v9jfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbvipiQk2O5h+HEb6UvAHpxPkMX6R+t07ePl01ibq91UTd4xUkHPAF1MMMX1QNX8Ad4YvqW0j+D5CUvnFJBfLhzCDTqfB7gxkEPgt8IXkDX1Qf4QvDO5CLZEiaziArSRuZwgLtSynUBZLeL5Aw32i1dsVA/T+QsQsqAqKCCeRBtdu0OznCdEiE5QpSIBJkwOtO0GuxgHQKpDkF5DrbL/huSZABTVTgfSvEJslFRoixDPI3EjHFGcYOyUr69bugMgO+RZ5kIFcIJfiu8N1xJUnCon0Evo0QTiBiAs9evVvHT/cM85M6OMLxBQQZIzyzp7UC6gXjfYb+pUG1FeX0cP/rADHbgawHRE8zyGleQGRGjsHjHch/QGJVQfaUbzZrAMRsAunSGUbLqD6c8A7mtj3xQnECEwHtlWjejiN8uF/Hf4Z9UjrpxzL0Na7IIMBayJYIYieyUVYUrK3LDjBGzhBfbQJRJMj+pkUrO8D79iBsw/U56T4uBZ1kYrt+xgixnqB9wZkTlgEEUwUmtp3RgNhohv0nrfFDB/ItmKPjxg7Z0Bp91LbKiw6MAqbJOGj8LDBWKk1SIGJaqnZwrpv3yzBO6NgFgjKSp42wOcwD7NHgjEZCLDovkAUwbUR7M5xbGtrzQR0SBAmtO1/Cf2k1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbvipiyje3UnaAPOVZc3fDHDTpfEn6yOGySS4G40YHnxshYXoskKg8gYjooOf1A0hnEsiTyqT1KnXdAAUkBCREApdUaGYQx0BieQJZRwWzUYRuXoq2XQJ5zLKpV25AakVSH0hSB1dBSCSD2BMkIoLf9YwJkvLhdrUlYcO63WOrMZdB6lBGiMOD9vW1aNlNQxInfeMWKnEFkU7aCIumQe9/8wBjGOKkBVlNewfvBhKBKWlcl1eQgg3PUtbdgFxgU5RhjIAvIiSQSdWkDZfC/k1Msafggzlq0bht4aMPNyAA2kiB0q3e66ZqDPwIa0B3hDiDeO+h6eNWihZCuIfnXqFih2V9HbgJw/0DyGnATNWPWocjtFsHa/FwC3F21Tpczi9S1r6HNSWu3wWWsTBnrdcR1qcMBiPwv+2KtoUxD9dViP+O1rwTrKEb2Vc9QtvBnxgWkF0mECctIE7byiP/70JY72Btm0CAtPUfjfBuTQbZX73oe0yv+h5Vg/0A+8LmoHWYzrCPAwHa0um7DJf1/SrMfdMEgsVl1PsHLYMlcHe8/cmfStmwqEWRJHhNo22zkMmsrNu+QMNsPUchhFDOWjZdtZ3DooeUicRG8Gq1hTkVbEfzZhCQQDXD2JlIRNR8lqICQqiQtR9S1HFBcr8Jxuiph/1nXZ8D4RgQCux54kD2Sf1w/Vc4B/gvrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdstXRUxlBuVA1gTkNmpybQfSmeWqibljWSdIxwqJ258123q8qEhivlJCs75HWbTqGZKhW0gubqomQ5e6FgcM0B5zd9J7gUgggPhhhH4oWet1m7RvQlLpzOdXLXvbfacf3YhnKigoKoiYOmhLaDaUGuyKrV0iBLXzhBAyJOUXkLVAaIe8MVQ1Re8/LCoCuMwqjWjmGylrIf5n6LMWBAE3vdbrBDnzedO51wUMMTAmbk4a/91J2/wziBXA1xaOo0oJhuMnKfv1D1r2tj5IWdrUq1C8gswgk7wFBFaR4mtnfAbhVwAxHJliSOLy5qx1PjTrslE9LOFHGAOfX3UNeNfcwatp7PU9rAEgigkg1UsLSPs2ArVPk65Z6aDvdmhhjOkwDgXibALd291Vx0DpHqXsLz/8tX72zT+QslzWA77CukPywAbkV8sMEiIaGDviZYLxDfMnSQVnkCKFK0jBNgKkMuk1I+ydphnkRO1RyjIIXEiaE0AC2IGxqQkwTjbxOS+6PpWsMpwW9iwdiNiWZ92zzJOKdCIpEI8gRXrVsm55o2Xduv6XUZ9ZR9gXkYSL5sh55yayEEIFoVC/aJylk4pbcwYpEqwpo+w1dQzMA+yDoy4WFNskQtzuW0JAT1DIYFssUeNg2cwLedb1KTQ6LmLSdiP5bMj6zAbkkw3sR6/tBykbXzQeb27fSlnbr88uI8RsU7V9J9jz0Z9KaX76Ev5LqzHGGGOMMcaY3eJDqzHGGGOMMcaY3eJDqzHGGGOMMcaY3eJDqzHGGGOMMcaY3fJVEVNbNdl6ggTpUiDr/EbPxHHRBN5+k7E+vWiy8SGBiAWSfBcQacxRq5lArrAMcB0IVRLIU7bXtdC0MyRlx0YTpiPIG47qLwhHED9Mt9pf8QziIEiOb6Fs2Uo4oO8TSClAPxE6aLf5XyEB+3dBA9KxIYGsIoKo4AjiIRgny0ZqcYUxMj2DRADaM59A/pRB4AJGKJoMDicNPHBOhCav6/oe4rqBMdwc1KaUSfoAdT1Q4v+d3q951Jrdtp+lrGt03ilpLbXJUd+DRDJpgTEHIogFJD974zuQLn2GYHmIGts9WLu+AZnV62ZM/fi99s/0CFKLCHKWTmO7VJXgkeeCnChd0XE8QZv043rs5ajCjdpqbB9uVQB2bPSZw6KfvQfPxflWK1afYfz0v5ay2PxEP9us5TQZRDcdSNFi0casRdtkqrC47YjbrPW9RqgbSfagf9pbnQfajdzqGkFU8wJCG5jbK0jwJoin2GhMdCRsAhklxXHcSNdIznW96LimHcCxqsQsBx3/h4PG+qWlQQEytQHm7U7Xz2GzHpcFJFyNxnCsei9q80GXnd3RZhVUVRA8UsAvKPKC+WhZ2+dG2I/WCdbLGcYFCMUqCC4LnAMi7OVKQ/slncu6uqkDxEBeQNBXtOwAcTzOWoe7G13bHmFKnR7hrAFCrBYMl9uQLzC3hwzzSQNnKlh36wJ9+AX2v2MyxhhjjDHGGPN7iw+txhhjjDHGGGN2iw+txhhjjDHGGGN2iw+txhhjjDHGGGN2y1dFTMeDZvQmsFXEi55/TyCrqEUlAfPndTLw+fJJrlkOeq+u0QTkA4hNnkAI0PSabAwekZAWkN0kTa6ex42IqYUEZzDYpKGXsnoDif7U5vAe+UdNDj8ftM3fnO6k7DZpe17m9TOWCEKDCg1X9N1GSHCfQTq0J1qSXYFM6Qhyotujxs4bkI5cH9cmhmXQm+Ws8VSzvkczatnzrH1xf9Ik/xZsX+Q4qFctvG7EHD2M/ctV63BQ/0D40IB0DWwd5aL1ejxqPL1cfpCyn7xXsUQ/6Vg8z+t6LSD5ySQig/E6gYgpgthpb7yFWHkGmVL7pJ/NNyCwADlT/HE9Bpq/+SjXnGedx5o3IApT10t4unwvZUtzK2V3b6Evi8bFpx9VAFU2kpWbBx0DyxO05Ue9FzgBQ3dUY0sFKdLjRdvkErRz/t4f/qmU9UnHRfm0bvcR5p2bNyoAvII46IdB+7W73khZ+Ml7Lfsd0fVa3wHmxR6EOod7jf/bpP1zfllfN190HSe7Yao6DgvIalASdQBhEa3RtG9pYMHbSG3ypHXIo+7tUtS+LlHjqeu07HXQPUt7VWHTGSROsVcxzUP3rb7f0/qdJ5BEhUUHbAt/E4owh11hj7E3jiA4LBOte7+dyOtwAGHqeT2oOhAyvsZXKWtAUBegv6+wb4soldU+iheQKMLwKd16EjiBAA2OMqFNOgfe3kJbwrr71Osz7s86GT29h3PFRcfUWxgX181+5pXEVNANMYOMDESzc0PqVsZ/aTXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1u+KmIKvZ5r+6oZt8+TJv726pcI9aAJt6/nx9XP50WTnssHTSLu7kH+BJKDBmQFCZLD01GbI1814fgMifj95pVT0PaIVROcM4gP0lkTq+de2236oEnpj0dNrJ61OcPNW0387m71Gell3e4TSDjKTCIBrdcCCdgt3G9PRBghx1HjaXlRwUj8G+3v5Z3G5/fjL1c/v3ymmFOxQF81Tv7qRYUr4yeQ/fz0XorStyqhaKIGz3X4UW83resag8oMbqO+b3On73Ezabt9DvrM6UeSU2lcvzzqRPTtT/W68o22cfO07osJZE1LJUMKDDqI/wpz097oWn3Hn40gywpaNv8A0pkHve7PP/+vq59/KHrNcQJh0S80VvSJIYQZhIInjffrCGKn+YOUPV41lutGPtY/6zheQGz4rnkrZR1I8cqg68510fGe8ju97lXXo+6n8Dvr9GspGl/W68xQ9d1ITHQFGcoIkqCh2fcYONL7jTQvgnzw1yCGu1FB0fcf1+vHMMLCA/ui2mlf9FnX3umi9yutzmUFHttUqCusC6dNfC4vakS7SfqA4w3ERNHx9bqoDKgbKMZ0PJWq8Z9POneMSftm7tb1uln0fUcQ5MQA4j1t8tCW/f/tqGm0rQrYvV5gXbiBdjiAWfHjRnYUR53vR5AqDkHvBe6wACEbRpBUxk7rGmeNi66BM0nerAFZ73V30j36T466Fi03+sIfPj1KWf9B2+ml+SMpmz/qeeF4r0LWCcZFt5FvxqLXVJgn5wJC2qxxk0Cg+yX2P1qMMcYYY4wxxvze4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd8lURU7Nocu3lRROQT3CnpHm/YVw0cf5lXCdXzxcVTjT5jd4fxCbzokn4PSQIF0jeDiBFWVpNGm4WLSthXVay/j4gN5qF30S91whel9eoydvhRuUC88ffSNnzoMnbIT5I0Sn9Qynr23Vf1wnkAlCHKYL8qmjc1LpvEdPH3/yllL2+qBCiXbSN//qvVUb07kHb7/N13WcvYM5aznqvvgEZDIyTUlTM8te/BJHOGcQZ76jPtG/Pn9bP+JOfqiCmPfyhPrMDUc+TDoAXEDe8ZpWQjFCH509/JWXfd7dS1pV/LGXNad3G8xVsDiQdC9puzUH7K8A42Rvf/0rlF7/5Xut8H1QI8c9+/isp++aPNZb/4ld/vvo5T9p+s34sPPQgzQgaF2/v/0DKnqLecHnR9YmkYmMEecywXgQfO10Ab4KOxXCnsfgrHT6hguTkNYC07+kXUvaLDyqKK1edxx6Of0fKbk5radnlou9x/rWOu5q0D+9uta5lhHGxI84v2v9PT1qWwEb1L3/xvZQ1rZZ92OyB2gHESVXb7u1PdL5vW431DGLI5Qn2RW80rs9VJTQHmLbOG6FQc9L+n0CwlEEwNU7aliOILIese8WxaPteq96vGbUPHz/qerRsRHQR9ywgl4H9Thn0/rHAhm9nRJDsjY9av5xhTqX5AiRtr5e1kHJedE6ZZ70XbMfDUuFAAnamCOt0jTpHF5C0tiAVO2yGz+FWRZMZ5LZj0Dgevtf14wz1mmlcbOSeIYQwwhiIFx3b0/c6p4xbGSPIGQ8w30eQb9KeJ8E580v4L63GGGOMMcYYY3aLD63GGGOMMcYYY3aLD63GGGOMMcYYY3aLD63GGI+ffCkAACAASURBVGOMMcYYY3bLV0VMkF8eriAnOoFk5HzW5NqHOxUlzY9rccZ00iTtm3wnZU9R5QKpqKxg6SEBu2pZhmToJqgAqR40mb5Z1nWYqyaRXyCJ/IdFE9frpGVN0racq4qdXjtN6s+zJmqPP+r7TT9RIULdJNGXUft+6UEIRMnWERK1SYi1Ix5Hlcu8PqvU5Bi1byuMnQ8DxF27bj9S/UxQOoGxKz7oOAmjSqKGBHKN9kbK3vR/JGVPzzruftL9fPXzD5+1jS6txsnzrI10iPq+3UeQfb37TspeQYr09kbnhPBBx/rnNzrvtC+b54IzIwUdX7XRC1OBWP9bIOH4dNbY/vVF5UzP8ZN+uGqb/lGr/Xb8s39j9fMPg/bPqWjfXn6p47O9VZHEj1Xf7XbUMXAbVU50POqa9SuYt9+lzTvPurwuT/q+//zxz6WsfdC6vr/qWnS4+0bKPh1V4PHNrc4L7Qe1Pb28/SBljxuBSZx1HJ8mLZtIOFJ0HNdJ+3pPjLDmnWEtb07aZ7c32mfHb34mZffntaTuOoLohJwmsO6Ug8bdMGisl0Zj+AHWma7R+e080GfXz5ihXyeQDP4apFOnrHubu0EboPTaTs+jPmM+a1mGPVV8o2vbdiEnyWYftY1y1H3i3MLqDjKcvTHNWpexB0kpyEbTBPvlk/bld3+y3n+UVxVqFZg/XiHOroPuZUYQPE6z7jVi1jVggb3rVLUOh808OJNw6kXf44eL3v9QNKaWouN9AhnhNGscl2f9bCr6fpcH/eztRqh01VuFpdP2aJOWRZA4zQuMuy/gv7QaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktXxUxjUUvOSZNJJ4nTSS+1fzgECAx9+FP1pKIw6zilD7re/SPmuB9HfQcfg36zA5kShmEKm3S++VOP5uWtYTh6ZNKGR4//I2UFZBVTK+aWF4XTUDP7Tu9rtNE/2tRGUTOmgx9HbWN0yY/+nrVevVgHMpHvX8JJLACW9GeGEAoBSKyK1RjCSo6yWRZCofNT9rG3RGkJrMKV8bHv5IyEjY1gwoTOhBYPIKEJ918K2VPm9gZPmuD/M34L6Qs3umYGx71mQ+txg6FTg/jKTZ/IGVdr21XP+tny2n93AXEUf2tjpuc9LoZXnh53Xn8hxDOLzp/9s1Jyj5C24RWY/kFREkveX2/P3ujoqfhE8hvvtM+u3yv8ycJJwaQYDX3Ot6nSet1q66OcFjW8o8Egrp50rk4dioDeQbpDPh1wvCiY3ZpQOLT3UtZfQNj5QLijLwdoyAUO+o4TgXkcbMKfC4gSNkTSyHBiHbGHEAe9AbEXjcgLrxf3++PYK4A1094uuo89vHXIH8ZdC3qW92z/QhCrb6ArKbXeBrTuv7Xq7bHBOtOjCon+/gZhDOdxl0OKhgrk3429yAtXPRdKuyBmk1R7GBPWLUtl6hzX6kgzopf96H+romwbz/AniQULTuBeKc96jzYvV2PlXetrttHmYtCeJk0fv7mL0CEetI1ZX7WZ3wsKpHMV5Vl9UeVPXWbMfUKQsEwaB2aCusYrVlBhUXt8hN9jwXWmVsyuWlRB31YlnWMHmAMHCGMFzhn1AoCtPrbjwH/pdUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG756j8S373R/4N+ftL8MkgjCEvW/+m/PsE/Ucf1/32/Rvjy5gQ5ohf93+jLs36JfG0h5wySkto3kF836nMDpOBMz+v/ef/1o/6ffTnr/9kvo7bvctYHVEho6q96vyvkTPaj5rPMB83neP4//kLKXjZfLH73U323Et9L2Q38X/wAeTr3zb7zmSrk/UAohgm+DD5M2hmXi8ZFyOv8gyvk/naQBzAlfbcRcijg+7hDedXcsk8X/ezzq/bj3UnH2Hkzhh8/6b3uIMevNpobFUedTD63mkMZXjVH9vGibXeKGmP5veaD96Pmbi2b+IdUnNBBYsgwa5BMGfJhIWd+b1wO2h9D0vmzHyDPr9Ocu7/4y19K2bT50vThqP1zD0kzF/iC95cO2v5Vr7tC3vHjR42BBHk5D290zoubEE2wKNYGct9mbcsO8pl+vGrZDLmKtKwvWcd7bnR+mpOWpXE9BhpYnz9Pmms1Q05rGbSsLpBrtSPO5H+ABNNp1r4tWeeepyfti2Ve922XdX3uG43XT5OOr8+v3+u7nX/U+wWdzIaodWhb7Z83kPsd8jouzoPGxAPkAT6Ft1I2Pv9GyhZo8/EK3oik9WrAzdIewTuQdMy22+fSUg+FddF7LbAGzEXXrL0BKfFhnCHPdYb1DPJBr7BRHa7rfUXuYA34A23ncdAxdsk6Bsag73HoIOc8Qa5zr2tKhnkwbnObZ427O/DpPBd95nzW+b60MGYnve466zMOsN7dPjzoM64wlx/XbdyAOyTAnqBUWAMyzDHNb78G7H/HZIwxxhhjjDHm9xYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7JavipgeL5q8m9QjEA6QwF/hS4Xniyanx43s5g7MMeOTfuHvp6cPUjZc9Zk9JPn+WG6l7N1Rv6i67SCp/1XlB9eXX69+TpB8PneabN1GTZjOJ20j+pLi2IOsoZIpBhK1QTD1yxctGzaJ1I+Xb+Wa7/5Ms/TTVevVHkBCMO37i7XjRfs6gzilP6goaAJJTLiowGXKa/lLjBqbIajAKS3Udhr/DUg9FpCk1ADiGBBYXC4aT9O8nhQyxvUfS9k39yoCeO5/rmUXkKm1MDdd4UvkD1rXJsAcs8CYfV63+zRq3yz3Ovd1IKSIR323qexbRBZCCC+fde69DjDPgJyh61Wychs1Rj+P63auB5DfLLrwzCCeG560H3MPkogB5CkzCItgnFWY3583Y+XYaHvcBBXRxKPOE/1J4+z8pNKZVxC7RFg/p54EOzovxKuWNRsB0Az3LyDZo7IIU+Ky8zEwzPp+S9W+TbAHKiDVo/6pYS37yXDNddC1aFlUHDZOGie1QkxA/Mde15l50E571fAPLxuBXoX+vzl+I2V/AFKn5Vb3Ih9+0ME+LSCrAUFOe6NxfXOjYqcy6bzWNut3GUmItoA8E+pfF61rWWAS2xk/fK9io/YAdiYY4HHWOh/PuteYtueFqm368huNz6erngPGzzpWAux5plmlQ8uoe+0A0su20z1fDetn1EljsXa6h/7jt9pu5weNn1/9QtttDvq+Pcghc6/teYJ1cRkhbod1u78WEOrewH4M9kHtCWSEEWyhX8B/aTXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1u+asF5SHrJBEnyZ0i47Q9atixa9rCRbjw96f3bToUGDz/RhOZwVkPAOGgS/uGg9Zqvmqh9BXFUBzKJ/vRm9fN90iT/pxcVmvRBk7lHSA4/9pr0nkn0c1DRT5f0Gfffqujj23uVJDx/Wtc/tZpE3Ry0ru0BBBQgK4iNJunvicOttmeCJPeYVShU77QfTy8qzrgM65hdriobaI56/zLqdXVU0Uut0MYwnpoCkohR61AaTZq/3QiVbuIbuebm7+iYO/Yg5/pBhWgDyREm7YcECf2pvpOy5qiCoP5W4/i6qX9O2m7LrO8xZhJSaPyD32F3zJO2SyWjToU5VT0P4TzoPFhf1zKNx6uKNFr4HesJ5s/c6Hx/LSpYGeA9DiCmaGDNmmBctGUdG02jz4z3sJ5EkFrBWMxF+6EbtZ1GELvUovWaYd7OVUVUl+t6PSZvDM3iJCaiskBSjx2RJo31AnNqyrCd0ukidCCG3IqXri8qUzqATClOGk9p1n6NEMPlRt83gogoXjXWh6RBcHtaf/YtzBv9dzoH5gTSukd9jwusO08F5G+zrovlTOI4fb8maF23sp5BLwkxgdQK5qucQZCDo2df3JDMEcZAgbocjnAOyBoH7bDeGz190vl5gFipWeesMurAm8AgO446fsIFREEgi2tuNRC6fr13uW91n5He6r2OJGkddG17gme+ftDzEs2z7ah7snDUc8Dpvb7LeFnPRzAVhQr7oHiAuRMEZTAsvoj/0mqMMcYYY4wxZrf40GqMMcYYY4wxZrf40GqMMcYYY4wxZrf40GqMMcYYY4wxZrd8VcTU34JQ5KrJ1t2g59+m1YTedNAk3K1z4iapTKlCIvj7TuUS4BYIV0gaDr1e+HrWxO+Xz5q8nU76fvMmF7qOev+7QZOyBxDH5KTyn3JRuUDbqkjgnRaFsdOE7rt7fZdv3+pzc7cWfYwjyCDgVx8tCBKmDH2/cwnBzQMITF40a3wCKdJl0ST/sX6WsrqRcPRBJRfT9EHK7v7wj6Ts3WeVGL0s+m4jCYVAEjUHFVgskyb+X86bumZ9Zv65ysSey49S9pJepGz6BMKRrMH+cKPPCCD7KlnrdXujUoJpIzSIReteSYgG4pMKbT4FEEHsjCMIe+qs89vwUeNn6j/pZxft30Nex/zhWcVbc6/tV96ooOwwqYysXGDOjjrOSBSYQZJRJpBOLeu14mPRet591Ph5XbSNmkZlHcugdYgziC5mqJfeLjSNtueh1fkjbOe2qjcDb1RIJOtC6RK83I5IHSxwsA6GAdZG8P9UmEO2fZZBxrgE3Yvc3kHDf4CYeKNz27zoM2YQ04BfJcRXfZeXzYWl03vd/VzLni46TkoDa+eTzgkF4v8Akp856f2Wqmt712o7Ldf1c3OE9YTEXCRAhLAhidPeePeggQze0jD8AHuDb7Tf4uv3UpY2Yyp/+qVcc77V9f30Xvet92BLHZPGxRkka3Ove63wCiKqi3bma9xINUFc+vBB55OfQ3sMSdvy5aO+W1m0DvetfnZoYR47/LEUvX/QNv64ifk4wrkI9jepgmQN7JMl6/t+Cf+l1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvmqiKmDLPxctWwsmuR8s2iC+UxSi+/XAo/fFE2Qfwvv0YJ1qYl6XQI5UZdApPFOE5CXy6uUvUDCcdzIdE7qBwkB3mOGbPYTCJbe/MnPpKyAEKuANKK9qIQgPmk7fWq1rnla9+uh0XaLlGxdNbE6Jk0Y7yg5fEcsk77fXLQ9L7P2RY4gf8ja7stG2DAGvabPKogIT1p0BQFBhmFen7R/Rhg7sdFAnrOKFeq8TsyPEIcvIOcawBDz0Og4TH+gcp0FBEhtC2MTQqwMOu6ujYpJ8uazGQROER4AzrFQe+ibBvp1Z4xF2yqB3KssKmIKzyqOGM5adonrGM2TNuDdUYVyddJYfL2qJOI6aN9OV5BqkRQF5rIWJGBlXF9XoQ5nENgEGHfHW61rmUFYlCD2oq6xFQwwZQLhW9Y1YDttV1pQQagXQdgUYP5rQbK4JyaY2+uiMTENWt981navPQhhHn+z+nk8a1x3EHMJZFqpV2HXeFXRy+srCAVhzUqzPuO86DPysBYqnUGA9xj1XvOkY/P2SPMs1L8DORvsOwsI714fNdbLHdiFNvvddISxD+OrDiCXgf3OoQVh2c5oQSiUZoiVAwiAXrR/v7/8Rsq6z+s14AoCsPsC8XOB+bnRsoUEaCDLHCLsceHc8gJjpW7m9/GgY/2SdG4/g8TtBCLbw5tv9N1GGAOw125gb7h80Hj/CPsg8Tq1ekZLRdst099Fj9qxh073d19i3ycGY4wxxhhjjDG/1/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt3xVxLQETSS+ftJE95S17PNZP1uiJr+fN2Kjw6ASgnhUYcnhHgRAsybwPyyabDyM+r6HrM0xvLuTsrvzZym7tGsrTgJpQmhVMHPbgOiq17oe32riczlrHa6Tvls5QRL9CYRNMwgG8rrtKghmEiTMJ5DkNPArkgoSkj2RZhWnjFetSNORPOlbLWs0Cf/H4V+u79VoDB/v9V5doyKAHx+1/1sQs5ReO7Kh/m+0/s38ImXLRmw2ZRAhXD9JWe4hDt/+kZR9962WfQaRRlh07lhAHBdJSjBpWdxIZ+BWIYG4QQxOIQRwU4U5wYDaGT0IgJ6fdT7KECth1H57eK+x8fH7dWwsLcxtIFOZQTJ3eQVBBoiClgjyoFkFMwMYQUaa9JZNBycQEZEosKpIo7nXdef0rc4B5VXHFMVx0+i7VCoDD03eyp5AukjOJXBEhVi0/pUWlT0BwqoKErwEa3m5gpyk1Rh7fdn0IzwzHN5JEckun0FYFGedF6dRy8ZJxzD2DojY5o1kKYJ4byy6dmSQYi4n3Ss9vP27Uvby+GspixBPJMZbYI0aLzoWD5uieaL5RUm0WCw6NkcQBO2NPOo7Pj+BfBLmhjjpWnELm8HH9HFdAPMiua3qRcV+rxB7tyBPq7Dn74vu0UatQugWkPttpKQzzHep6ucOJ93zvfn2Oyl7+PZPpOzjZ52MmuuPUnYBMWA6ajuVAQRtYR23LQn1Om2kmEGW2YHYrvvt1wD/pdUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG75qoipDJo0OzzodSTeiZDQe/tWP3zaSGzGt5oI/b7XV30ZVFZQQHQxtprQXRKIDuAMvyyQ1H+rdb2PP139PCUQH8C7NVnregVByPKqAoPpDCIq6NE0a/3PAwh2Gq1rDNu6gqgBjBt9oy/SgbxkifuWEFxniDGQjnRJE+nbVoUYP/vuvZR9d7sWbMQJZEJBZUqhp3fTvu7SN1J2+Eb78fK9io1+eNIk/xkkBzXebq5R2UALQrD7rO0Rj38gZbnRtjzq0AnzotcdKozrVmO9g7EY2s07V5ANgWyhAxFTBnlBavYd/yGEEEBWcbjXOfDm3U+krBl0sPzsp38qZT//y+9XP7/8UgUr/a32bbpT+cOHX+hceT1rv02HX0jZ/Kz9MV9BEgHSnbSNM5jbksynIVDYXYqO9797qyKe+aBtcr7oOJ5eQAg0w/vp9BGuG8sShHEIIGKMIASiHUcki9OOuIKE5hz1nXuQmiyDln3b6x6oe/Ovr36eW+3DU6eBMkeVAgUQrkytzqk3h7/W+/0A+4zXRymLWeMz5408CaSbTfzt9gpLp2vWw70GZw/rx+VV16zxVZ+RqgqxUqcxe930NYmeEuxtIggQQ9Jx0u48/kMIYQGpaux0/9EcQCw6azv87OHfkrLXp/X9yqR7iFPSe31atL+HC6zJHaxjUfdal2cdA/VF65phA3Io631gItndonW4P9xIWXunsX2CsTKfVNxaQQR7A1LdGdao7qQSwG4jzmro/ARrVm4ptuGcAVK0L+G/tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S1fFTE9B01o7hc96w69lt2BmKA5aiJ6vllfd7zqM1t4j1PSJN/rVctGzXEOy6RCjAACpJte73c83UtZ3TgCZs0hDy8zCBIq1FVfA4VYEwh7jiDsaSAZuo563RkS1dNGstSAdKaB9w3QNyMIPJade2iWUTtynkEccYWE9kUb5vGiMVDzWiZTivZrrSrcyJO28cOtCixiBBFZ1GT7AKKLNyB2iknjf+nW46kO2kaXpHVvGhXpJJAdfXoC+cVJZQPtdiCGEPoeJBlFJ4UE4qzSreM/zjBvkIRm0b6ZgsZIWb46Bf/uaXWu6Btth9ezik1uzyA2+ValZYdv1hKnHDUGCogkHu5UhpFuNVYeO5A/DCqweBlpntU5IIGIJ2zWighrTI1aBtNEKC8/Stlf/ajtG086fu5BANO/gWe86hywgCSj2WwTZpCLwBBAY10pKvCoMGb3xFxAxAVCocuj9u0bkBReIC7679Yd9FB1jBDNrA2fYEGeDnq/+vkPpWy4/V7K5knHYg/3C2mzBoCYhdb7dNA5sJw/Sdkvvv8sZbXX97ipsBbfQ1xfdK2cQDCTNgLNAmMkRS0rsN9bAsQ/7G33xgBx3A7QvyBkvQ26BnQgvErv1/NxnZ/lmmbRtnoL89gQdAyMsJcPF50YFxBG3sBcdjjqZ1Nax3LTaHyOIPfMrbZbA3Kvl4vuA6FrQttr++ajPqOFuajrdT4eNuLBvOg1C2yNCohsF5CR4b7qC/gvrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdstXLSDvNdc6XIsmuk+jnn9vb0iopAm3ZXM/TTUOIZ30czlCwjQkyYOvJty08G4HTS6eOq3rUkDEE9fXpUbb45RfpewK0pn2qqKb9p1KQ56iCnugmcJVHxHqVd9lGD/odcd1kvvbO00sPx5UhtJWTbZ+GSCZvSODx36YK8ipeh0UEUbSkFSc8uGTltVN/Oeq4pcexAXLrP1fJo3rkmEQD3pdBtnA/T3IFkCw0XSbshu9JoG853zVuk4gPulBVjGM2pZ91viMjbZdeNVZ5hpepCyN6/udTtrRLQgTQoI5EkK9b/f/e8M3HUgXwBZ3mlVu1X+rbTMPKtjo6rrtlwzXwLhbYA4MNyCOAQPMGeR2TQXJyo3Oxw2sY3Fcj58RBHjtEeaOCeQsRd9t+KwxG1+1XheI98NBn3stugbMsz6jSZv5vWrMVljHCghxIohaItxvTxxBzlPBPLUV9oQQQgsLcgHR5LwR9JwHfWbT6b2mqG08RO3XadRYXCaYP5OOp+6NCotS1veLG0FnPagkLMMcMUMcXiEO50eYZxuds8f4oJc1Gv+PE3wW2qSr63dOmeZ7bd8ZxgS4+EICGeHe6CHeyz2UwV47H7Uvz8/aEPNmzaxX/VwLssQK4rGc9UUSSE+PtzqmjvW9PuMtzGUghUrtRsQE4tLYwJx9gTrQGgPCpqnAnB10zaY1qy46jp/POh7HjZAz3ej9j3B+oj3f8wznAKjXl9j3amGMMcYYY4wx5vcaH1qNMcYYY4wxxuwWH1qNMcYYY4wxxuwWH1qNMcYYY4wxxuyWr4qYLgmkKJCIfoIk3wQyngBCjEO/fo2OjCWTvscAoofDjVbpAMKNCPKUJmqCcHer4oA46rtcN7KGp88qNFg6fbcjJOaHtyAh6DVhuu/13W5AdnQZv5eyX39S6VKqIGvYJJK/3KuU5A//3s+k7FA1YZ7a9wKCiF0BUouy6O96+gqJ71H7bJlVzhCXbXzC56BvJhCiHUG6dIVk+6loHeZJP5tBGhBHEGzk9Zh4hHnjDupwdwCxQtWxc3lRaUYPs1cH/TBftA+fznq/Fn6Ht4TH1c/XAep+qyKyBOKoDn5FuIB0am/ETt/xBGMgd1rnh6yfzT18dl636xB/Kte0EItPi/bt4Y32x28etb9D1HWBpuO06JhN4U7Kcr+O5SXo/W+yxufN/R9LWSk6P5dJ19g561jJDckItWZ1hLkI1tl5o0ZcgspqctQ+bUHeEuHdwGeyKy7jk5Q1M4ghi84Nba9reQdyksOmESrEZgJh15BUWNb22qDL82cpq0UlePMEIwA8fh0I//JGpDNBexx7kME0b/WRIGtbQNo3R42nCpKoSwDZ06D9Wq6w9uZ1H6YMEiEQyWTY7yScc/b/t6MaQCoHArUMZT3UeYI59bBthwRiHzg/vC4aF12rQVtgTaapp8D7dlnrRf7FshlT51njs42657kFQV8LgqlpAk0txN4BhvEy6xr4dH6EC0G0t2n356LCyybrmphAvJdhzzPD3vBL7H+0GGOMMcYYY4z5vcWHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu+WrIqa3IA0IAUQPWRNpHw56Jh6iZi9vdUKfQLjRzvq5Mmui8gKJ9AUEIT2II2LUujazvsvUQoL9tE6uTgeVENxcVGCxQLvFoInaBeRUhwEkHCC2uWnfSNn7b0Eu8aLPGDdtcgv3V41CCP1B2/wMkqj41Qj83RJB7NPCO6eDJsjnqjGAIrKN6eI1qFjg8qpxfWg1hi+Q5D4WfeERJCxdggT8UcuaFsbEJmZPIG4oCWQwIKYpRUU6h07bpE9arwTik8uTPjdHHYstjP+6ac8e5G+npBKFlPRel6LvttSdW2hCCAdYJm566MsEkggQfgWYt/u8fsYVhGWfZx0D5xeQu72oXOIMgza1IIuboS8LzL360XDYSOr6ASRmICzsOy0rC4ieGm3fcVAhRt9obJeq9wsHfe6QdJwNy/p+Gbr5eKMNkmHeKQ2szyRe3BEPN9rGHQiApqgNcwPzQIR54LAR6H0uGv9lAVEiiF6GV+1DGjsjyE8K7Iu6K6wLsAeqeR0D4OEKY9X3uG9B2AX2pwxiswHWp2Or0qnXUaWVKen6TLLMJa/7okJ7nO41RhLsWRfY2w7aJLvjmLVdIoiCIshXG9gvZVj3j5u18AzryQDiTpIZDrC/qY3OURHUexH2FeATDKnTutZxPd5bOCs1MLabHqRT8B4NjM87kKctnbbd9KLXNVAxEqaOZR2kR1jX+6pjrAFpb4A91AL1+hL+S6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3y1YzCd/eQ9wJfydsN8P/d+m/a4Rb+l33e5Cpcf4Qv1oZ8uyPcqyb9H+101fdt4QvuZ/q2YPhf6w+f9f/F283/40fINaI8qBn+H/8Y9bOQuhWWpF8W/Ap5qTXfStlP7/+hlN38Kfwv//M6n6kU+CLjG8hqhZyf66wN0GX9//49cTpp2zUH/d/9vECeXg9fJD5Sos/mcxfIFcH/+YeAgi+GXihvI2veaAmaXDNVzYeu8yctW9YxmyEm3sCXak/Qbg3kENZtI4UQIEU6jL/5jZTNSfvwm7ffStnxrV4Xn9Z54wVydY+neylb4Lo0QF5Q2nf8hxDC+/sHKWseNKb6s5aNUWMqQ65O3fT58wAukEI7QgAAIABJREFUhSs4DCAfboK5J91pWb5oTlsedU7N4HVoyGvQr+O7v9X732fI1YM1a4Hc11J0vbvJ0E6jug7mrP3Q3L+Xstuk9Xr99HH18wB5eRnywXvYXUxQr5D3/bvz93c6vtMJ8sM0lTQUyP/HPO/NfNH8qFcsMLeHpPN4C2PiAPmrudH4HOZXeDO9LkaNsbKs+3ZKes1h1Ph/gZiri873BfLvWoj/fIFcdUiw/el3P9PrYFw///jD6ufxqNe0DYxriP+x6B4zQ0773riDuayA16DVdPpQYL6ICfYumzVg+QSJpJDTSv6I7XoSQggL5NseIM81VDgbtPou5QzrzMYz0sDYaQrE+ytsZsDFUyBHdoa6Dh+e9d2yvsu7u2/0/Y4wfl43+xQ4txyO6k2gvecF2reD677EvlcLY4wxxhhjjDG/1/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt3xVxHSrTqBwoC+zrZoMvUB+OX1Bdt18KfflqInFx6TPPILQowVh0zNIoroTSJfgS+/py4dzC4nEG5lGf1JpRgvfsjxd4ct96UvfZy17gC/W/kv4QvsjyG6+eatyiZtbkH9s5DRx0XarYNyaOn2PG/UXhQjJ5nvi7qRJ6ccbiOGi9U0ghBlBBjBu4m6COCkgq4jwhfG1gS+pnkFYFlV0kWEM1/goZddRn3vYyCQeGkjUP8GXsoMcYLno/V9AQlMuKs157vS6B5A+3LzR+H9zrzKNuikDZ06oVe8/gnyiwBgu9OXbO+P9W+23eACB1g30EbTNGURjr9M6HiPJbzqQJF1UZHV/eqfvEUFqcVDpzFh1wQOHS5hBgnac1+9y6vVezRHWSQ13kZKEEMIyq9hmyfqM8aRlCQR6R5Iz3WhfH+p6jOZR23xZYC0mSRTMCw1tFHbEt+/0/eIBxjdI9krVuJsnWAM2ny0nkLZNIICEvzv0IPtqOr3fS6/9WK86HycQpwyTxn/eBHLq4F6w40wg9pxnLZsuWv/5oPK86xFkhCC/+kZfLxwfYP2M67lontU2NBVoc5jaGxgT07Tv+A8hhAf1fYURpEAZ5o8CexLwKobztDaZjS1I/EAA1EFQ5URjESbyAwiQZhBHjfrCI8jntsKz7gbko/C5CfY3TQOCpQFEXrMa4F5gD/ku6TOOcMA7wj4t3K3v18B4qiDZiyA2O/TaJhXOVF/Cf2k1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbvipiur5oUvKvXjWB/2bWBOkfnjR5+WMDSed3m8TcDz/KJZflDj6nico3B73uZtQ6TJDk/COIc15BznQzaSJ+7deZ6q+tPrNGTYS+g2z9WrTs+ahlH1tNaD68apsfQE5zbiGxfFGZQL+R4iwX/T3H4422UQvdfISc9wwShj0xPv5Kyj5/puR1/Wx61bpdwboSy7phKshKCsjEStbr4ghCCBRzaAdlEH00Qcd6gCT/kNZ1nSrIDOBWpx7kZEEDZTqoxWuIOg7DR23f6R7uBxKSS9BnHDayq9ezttEzSB8yyAw6mm5ByrA3EohjXkYQbWVt0zOsH3990X6LcR2jcVZJUon6HjNI0dIjyCoSiIjeqollTDq24wLSnUX7t+3XwpZatZ7DoDKpghI3GJ83IC2r2g/pqvdrQXj3Ov4gZV2FiWzziHkGEV1VGciQdX2CoR0KyFX2RDPomP8Ee6A2adlw1nZ/CdpWZSPLm8qzXBMnbacKwrd21nkGfFAhJhDdJJB9wVin8TSL7Ezjv4B0LEJhoQ0EjIl60LYcr/rcE+yVPl51LN7UB33uRoo2Xaju+h4z7HUjrBUz7DH3xgTr3tOsZX3WeHwFWd4HiOV+078vw5Nc0y26l40d7D8XFXS1sOfPs87jr0XrRXNUC3E7lnWczQ3MCa2uMYeoE+MIArSnRq9bQGKULxqPV5CKXpNeF6q23WGzB3g9az8/kzgL6tBDWWMRkzHGGGOMMcaY/z/gQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3yVRFTggTkw6zJxWMAic9VP/vwXpPT32wSn18f3ss130Ji/qfLo5S1L5qEP580ATtOel5/22qS81sQTC23KsS479f3mzRPOYRJE5XnrF0Qg374WzIpgNSn+SOVyfQgDXkdNcl9edZ2Ood1u1dIPl9A1HJctN2ae03wbvK+RTQvICcKCyTXBxCFgWxgBJlM3sT2DEn5FT5H7Z5BJlY7fd9UtN1PB0iab99IWQShwbSRlcSiIp3hDL8jKyoCmBYdE99uZW0hhPkA0owTxH8Dch3wD1xBEDRsBFPLq9ZrvrmXshbkLc1J5w1wEuyOujXxhBAKyO1eqvbb+SPMMzDmb7p1e83QVj2MxcsVhFdVx88A8pt7kOAd3mr8LLBWHEDsMob1uBivsO5AWx4SjNmswpEW5GlzC9KprO00F63DNMDcC8K3Oa77tUSSDIIkB9asFrYc4u/ZGZdB5UTXUUVJQwLRCQhLXmF+77r1ulpgZ9bBHuj1rPPReNX5voDcL4IVq4FYbHqNxRxB5LhZU4ZB34O2RScIwwna8nAP0spOr6sVZHwQi/NVHzxdYY/Wrus/g6gnkmA00uQO8Z/ATrYzxkXrvCwg+xm1zpdH2H90ILLbNM3xqHuqW9i3DFWFWnmE/RKIxwpI5U4gy2pBPrlA/27fLgcQaI4gYmtAMAX79m9hXli2DRdCWN5r2x2TrqkF9inbvVwIIVzquh4R5FoziGbzBcbKre6XErT5l/hbsGUyxhhjjDHGGPP7ig+txhhjjDHGGGN2iw+txhhjjDHGGGN2iw+txhhjjDHGGGN2y1dFTJSA3XT6sUOCJPnvQFh00s/2h7XE6XbU5Oi+1UTd2oGsI2hi/gJJvuR+uEAufX8D53p1FYRu836J7nVQWdVQQIZQtA6f4KF3tyoNgduFvoNkcDA9NPf63Muy/myGRPgF+usAkpNy1kT4EeQ/e4KEXROYQ5ZKogutbwcxGzdJ/i1IU0qr47BK2n8I9aBl4E0KGcQch6Qx0YI0ZAG5wDbGEklYYN4gCQuJORJMVTdwvyaDSAQkMTCEQ4HBM23q3x61/2aQtbUZ+hAkH4UEHjtjABFNANFayNqZza3W7x38rnS7phSIuxC1v48wPtOtzmMx6/3aAjEKYy9EkL1A/9a0HqOpgnaGhDAgA2kySNYWvS5OMBfNIBIBGVuXdP0IILFKaV2PtlPhBrhpQiBhU9Hxk6Gue2J4VeFjrdqe5ErsG22YPkHstJvxBH0YD9qek7rjQoBuVUVMCBHm1B7W8mWi+ROkLnk9v6Ws+52CciJto67Cfm+AsQ4ywgq7uw72I3E56WdBRBU3YrPckvyN5nHtr2WGLTfEw94YQR4USTJE6682c7hrdE7tjmth4g1IYCvERYJ+pD5aSIJFG+ZGY/tw0PelNbBuy+BclEAyFmC9i7A5mkFieLwDIRSsFeDsCyOsd3QovGz6IqrbMszQXz3sb1D+BOeKL7H/0WKMMcYYY4wx5vcWH1qNMcYYY4wxxuwWH1qNMcYYY4wxxuwWH1qNMcYYY4wxxuyWSInNxhhjjDHGGGPMHvBfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7Jbmaxf8Z//lf1G3ZZeLXleXRQvzjRS1zTspK3Fcv1TRe80xSllcJn1mlNcNYZm1KOl5Pc76jJDhGXOWorK5XZNbfWbQeuVS4PbQLbN+NmWta67w2az1alvtm5q0rnlZf3ZbzxBCiEXfozZ6r2aB9231ff/T/+Q/hI743fA//bPv5aXjoH1WoL9j1T6bosZOLdu20nZqosbwOEO7T1d9j6Tv1jT6buPcSVmG54YE8ZTWbTIHfWaeNCZK1mfGpO9WKr2bXhezBmgpg5SFeJCibtHrYuzX1/RySWgivZvea4wa66dbjaW//yc/3U38hxDCf/3f/g8SaMMZ5iOoX4J4D422fa3rOMswdhaYx2HqCfMI833QJq1Z2z4s2pcpjVLWQB1KWT83tbdyTa76whHm4tzqOF4WGFNBx1SCuaiFuT21MAaq1jWk9XVHbaLQ5JM+M+q9Zlh3b261Tf7xv/ZnuxkD//0//d/kBStMizFBwwS9cIb5OG7mxgr7mBxh3YFWqoP2Ne13UoL1A/6OEYvWocLcWzfr+wR7mwJrAGwLQql63QjrLrVvhG3tAmtqqDQPwVjffHaGfggTBESFtbjou7UnjYf/+D/493YT/yGE8N/8j/8c9kEaK5Vim2KlgYV0ExsQsiHCHrrCGlNGWPOzXpdgfz9OsF+CPUmEfVDcxGOB/XgLa1uFuSM3GrNLpTXgt9sHZRhTIesYaGG/VDZxezhqW7awR2uyjqcB5pjjSev6j/4BrwH+S6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3yVRFTeDnrhyaVLsy9JlY3HSTizx/1uk0C79xAAjsl0ie9f5pBkNFqknwLooMCso5m0XP9BJKMrROpBE1mbspRyipkmzdJTVcF6kqJzxUSn2On3VwTvB8keZdm81wQ88RA/UACFm1f8nftiXKB/p9A6tJD3eBXQgUkMdvgSVXbc6TwJxETSIHIVrPM9HIkBYNceOi0ZfNRqkNIGhMLWDiWCu27aFynVu9HsqeaQex00fhfko6TuIl3cNCFloRoML/ErM+cK4yTnVEvJKPTOCsgf0g9xNT8qtdtBsuUdd2h+y8DSC1gWasgbMktrAFgtklR17YCMo3YrT9bQRRYoq6dqUAMwDiOJJNqdU2h8U7zxxEmqCXq/ZrNeLxCG/UgKqkk6APRzZn8Ojtigf4Bn1aIESRGDUliSKi0/ixNuxPsgdKs96o4j5E0R58BS3moML+ReHO7f6ogj6zwHgXWgAJrQIHxlKAjJhCnVZDEBRDvwRYlLHH93Lno3BRhTGQaryC/GqAP9wZ4BcNCFjwI3NxB7MEY2MZGgT1ExT067TVgb6xXwaoQUOY6w5jKNL9t4iwtEMcZJFRgOF0gtgvUFecYWnvgb5QJxugCY2VbC3DlhhxB5DrBnAXXveJkxPgvrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdstXRUxXSIjPLUgiDnfw4Ud9IAgmyrxOnF+Cio6aRZONIac/hEaT8BcQzJDsBXwtoRQQu4CtoG4Srkl8ELZSo/CFZHaQszRBJUkZnjEFEIRAtnkLr5dAnLV9v7ho/yV4Zkogb0kgJiAh0I4okCAPRaFCn0E+fxhBqBU3YicIV9APhJDB30LyiwhJ7g2IBQZIkA8gnYowdtLmZSJIl8BBEZaZ5he9jkQyB3rdSlMatB40XuxJ7LS+X84gFgKZWqY5B+aXZkEVxK4Y6Xeb5PGCNq0kLQP9RdmYHUbosngB+QsEVYH5iMQR5QKxDf1WYVyIoC6QUAkkFC2MTxKbzSD+AGFNQ2OKYhvGAAo3QJoSNgKsBtY/UslEmGMKSGci1GtPROgLirscSYii91siBPdGujLRGkBiFlhTA7TxAv2fYO2tASZVrAO83+Y6GPpYsanofMByMlgY4LNl+yIhhETTLPThAvWfNlKbTGtMA9IplGTBvAF7271RSLwXtS4NHCkKBTPMDdsFcqY99KwdWVtoZ3jmAnu0BH1ZKshGYQMyw8BoNnNApU0a7BcKSPEyTB4zjAuK7QWkqhn29xXOFbSoxO2CTOspbROgrgvUldbAL7HvE4MxxhhjjDHGmN9rfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbvipiynCurSB2SRdN3m27G/0siS6adTJwB8nRlc7XCwgyJkjypUxlSv5PJylrMkinINF/2dyPREwVkqgz/tpA26hrev0sJHk3IEAiGQS5IGI56v3CsPq5knQpk5RC7xVBkjWQ0WFHDJ228TSATAQ+20AMlAXifzMmatVrKE4WbDuQqyTtsxEEYzPICzI5Q2AsbkvIrTL/dm4urNdEcgSSqYGshMRmkcZTD1KGzUczyCcS9CnVv4LggGQWeyPBZDFB/RL0RyUbGQhKyuaj4DkKC92LngmiB5rvodtQbhchSOP2hYPWIYBwigQhKQ56HcwoBdbdAPIwEhu1HcnIQNB2gHl7M39UkIdlaA8SB1UQsCwVJoYdsbQgWJq1bhNsMyh2yFpVtsJDEPZQTNC9Zng3srUUul8lIQoIxWhfuBmfKI6CV8sk9oL3iCBOSrA/O8D+rATtQ6opSZbSZhGcQQbUQFtmGF8V5U/73gOFEMIEYsWFZFlQlxZkPzPNDZuiSPN9A3sqkE/OtMbAPgim6JBIWgbvi2vFJlaoCrR4RtgHwJYHJYMTiTFh3inQD7GBMwQdDrYSJ9ij4nRCsjeQWS50vy/gv7QaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktPrQaY4wxxhhjjNktXxUxLa0m72b16YQQXqWkzwcpa2+17PV8Xv1cRhBEtCpOmYK+CDgtQgFZRSh3UpRBVhCTJvq38JAuryVOUwP3GuB3BCD6oSTqDmRHJJjJEeQfJKLpQHQA7ZkkQRpEBdhuJFLQshYS/PcEyTVGEL3Qdf0BpCsQAtscfxJp1Bmy3HVohgSmC3ITkWGJu4LkOpT432yuIRkOyAagWg000qHRypJfbY4wJqBN6H4HaIB5E7P0bvTMUGAs5f+LvTfnsWRJ0/Rs8eWcExG53K1uV3dNDzkU2Bw0ORJBjX+BKhUC1Cjxb5A/gDpVKpQIgjJBlQoBEmj0YDCN7qquW3fJzMiIOIu7mxmFkdzfJ5HZUvmg3kcLg7sfc7PPtoj4nqPx3yXoxL0Roe2hf7cyun93HYlN9HltI15ZILhTBxKfQDI+sr3Q72dh+QMhBkq1SE6zmWfBQRESTBQL2DoozjqQZjSw7DVYswq0yeEA64zeGtq8GQPQ9xQPoQNpH1yXx32PgQTjm4b8Umg9hjaApoobwUpBuYre1+C62GkZjU2WItFnQBnKydYVJEFMBLvMDOsCiV5Q1kLX0XgihxXMTRTGm/APHawd1A+w7QoN1gASCu6OEcbtTIIlvTXC3jVBH8k+qMDcBnuPCHKmBhUpIJCr0PYdyLJoP1Pgb37dZp+WB5LKQszCuw50XID9QoQ2QWkf7Jc6EPT1sM7UjUAww1rcSLwG0qUIZbRP+BT+S6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3iQ6sxxhhjjDHGmN3y2ezXNqtgaZoheTfeSVlKX+sDj5oNnM9rmUYaIFG3P0rZoYes7/4kRQWSlzNlyWcVLEWQQjXwRnTjun4xaruFqJaLCuqDDkQaubtKWQLTR4Mk8pS1mzMk1sdFJVnb5PUIAp+QoL+ga2rVhusgiXxPTBOII14g8b0HSUzWvh1J9LIZT7mCnItGaiGxEak0QJSl/hqUcER83heILkDU0R80rhd4hyOIFWaIExK4LFVfjMRmSUM9pAqCmY00YTnLJSF12h49zDkJxhx85O4oi0ql5glkJNRHIJojEUPcSGxI9lNBENGBmyVAnFWIiwgRn8EyuMwkQCIx2vpzO5DwRBBOVFpPk4oHR4ofmFMr9FeDMUVtUl5IirRu5KsuRaGHPg09vNfDl4lJdgUI2pp27SfWMogxmI/FYQIilUjtRBMIiVlgziaJUQTxHq4MIJprm3szfOYC8RogDhtIrSLUl2RSAcQ/heSBsBa3ReOz24xFuC1E2DuWAos2iM7C/OUSmj8WJAqqC0hKe4hblB1pn88bqVwkgSht20EANMOYrQvsU2GvRbKwAjJCmFJDtxH+JbIfgWSQpo6eBHCwBkaIswLrWPeF7TnAdfNGqhdvZNCE+R7mjgYxQmeIT/HvwZbJGGOMMcYYY8yfKj60GmOMMcYYY4zZLT60GmOMMcYYY4zZLZ/9Z3r6YvWOvlgd/p85Bs3rPEzwv9Cn9fMK5BXgl0hDvmkHOXJLg0Q0+ubeAnlokPw3Q45TTeuchgb/y50gL2CbLxRCCI3ygyB5oyX4smT4PUSELxYPk35GFyBvdpOXRanAHfxjfIZ82wW/bB6SK3fEcrlI2QXed4CkhDZDbg18mfM2TaFATg6lEt8gJjJ8aXdomh9XIuTgQOdmmAAi5BtOktep990gEayDsX6B/IYGuYAzJBdRHkyB+L8+6fu3IzRyWb9rB/mrFco0SS2EEcZmprG+M2bIXwuQpxToi88L5Kt9QR57hfaLEO8LrAs5wGdCMzcYK8uidcMvpYe6FGknGOvjF+b90noHOWQZYq+DnKEMY2VZdAzMPeRubYZtgkTiAh6GDt5/gbjJkPu7KxatX4HYibCdSrAfKfC+m1S4kCg/HNpuhrkyVhgTkOMXYY6mcVdnmI9BstA2Y/06Q78uMFeSYCBC3q9eFRLl/cH6lGDuvTWN/w6uK5s5IUObF+r7TtfdkPW9Eu13d0YDt0eFPUSFeaAWilsYP9sxAO2coW8LxAAoRkKAfOpSdc87FfK9wDwLc/m8GXsZcs5BdRI6SnaGfeACue4N9jcJ5t4Ke415grUS2i5v9l9x0LHN6zr4PuC6SPvWT+C/tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S0+tBpjjDHGGGOM2S2fzQDvEiQ+b60BIYQwaWLupdck53KhBOHNF/JWELaA6ClAnn8bNNm6B5EAuApCBOEIiQnoG+3j5guDE0kOsiZbpwUSoSGJWiUfIYw9CAxA1jBAL8+QWF9AiNBvXpW+pDwl7YgFOqeC+ICS4/fEDb+AXSkQ//Gg4+QK1x2P65/bGQRL8KXS9D3l1P8NLE70hdQov0BZDYhEpvV1qQMpD0jHFpASJJAXzCDXoW/GbjC/HE4wnkC4U6Gd8lbYRtIlkK0EEE3M1Dfwhed7I0EMTCCBixPIso4gbJi1HdJmkorQPw0kZvGmsVI7HRlxgbkdvvi90nwMcq8Kfd6l9b01POizErwDxHuE8VNAatUPKntJIA05HrRvYtT5voMvvk+buSfBPFFIEAIT5QvIeUhMsiemCG0Ca16coY1hnLQZ9k/Dus8ySvZgTIA0pkKMoZiF5FwgzVlIlAXxX5f1u7am47DAmCMHywJbU3r/AMLLDvanFSSAGfYetQO5Z9y8B8ztA2woUw9jPetnziAX2hsLnAMSiH2ieitDGkmiCpKlTdPQs3IPskhovgUkRuAiC3ODsd1rrEwkZL3qZ5zGTWUKrEWw9YJhLPKjEEKIEMcNpLLkN0WxFdUPxlTZ9D/tCWA6CRWGLG154LU+yf5HizHGGGOMMcaYP1l8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1s+L2I6fCVlqR61jMQmIOOZQLIUNqKLLqlcQrK0Qwi502zj2/wiZXFU4URrJJ0BCcfWRBRCiL3Wr22T+qNKhzIlG8OzYjrrvZQIH+AzergOPreD5PhleZKyuV8naif4Pce1aD1a0fYNnVakzHDdjjhBTEyj9lmFNiggUxlgTCwbQVEOKo0pECf1BWRXIKvJBaQeENcZsubjoNdNVxgT3Xoq6SEr/3DSpP/LCwhtqsbECNapBmKOK4hPKshF6lUlcfE1CA02NpkMM2YjSRywNBDpgJxqb8StiCSw7KR0MKeCjKX2YF7ZxAu1SgdymtC0QwpIl8gIQWIOcqLUqLFCv++dNzaN1J7lmjzfSdkR5CKp6L39ANdBTEUQblQQbiSY2wrJAjfrfSLRDcz3E4mu4Lq07FvGdzqAFAtCrICYph8h7nqazD5vIkkwbshN1L5wTY2wp9jOdyGEkDKIt2CETpsxPMM79Qfdi0WQO+YZ9mLwrqnTdbGAOKrh3KFlHfR1HzfxD/XINGOBrIakY7F+dhv+R+fueJKySvsF2H8GGBcJ5HbzvDYvJZgXwR2HIrv5pjHbweSeQKwYC4hgD18mZA0bkRcM/xBhv5CgLcG9GRqM2QZx3G+FUCGECJLaBkLWShanjWiPxKAJpnGa72nu6FFvyvgvrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdstnM8AzJOFHSBoOgz5qBGFNDCB/2IgD2qCWg5hU/tRBVnub9DpwUIR40+TllMCuMGlSdoQE7LKxHaVZZUoDNHc8aH0PSStcpkcpA99GWEiQcgI5T9XPiFHrshU7zVcVDkWQHFRKDgchztz2LaKZQIhSn+B3PZ22Z6sa6wvIarpp3VYFYq7NGpvXpmU9BPsMQoheuzFEEGWdAsikyP6xkSfNULf5pveNYFgaIZ4uIOao8Du3exhjS/eTlP14/ihlb8u9lG2ntThr3RaI4R6kBDOYREhmsTdi0mCps84VJMZqIMYKJMbahN4ywdiZaT2F0fVOAAAgAElEQVTRmM1NG7+SjQ6qFnstHED2NGeSM61jHt9heaPPGr+WsgeSyZDECCQ2bYb2Papk71IuUvaQfyVlZSNZawXWGJjr4gJtDovWsoD4Y0cs8B4R2rgDQ08E8WSbQWQX1nuZBHMKuCPRWBZBclPg5gxzZQq6bznCnmKC2InxZXMNvAPIpMbuAeqh7XaDNs8g1zmByPNSf9ayGURRg4rSho2cqU4gMYT+ongoIOEZaSO3MwqMgbSVj4YQMqwBtF9uIDsawkb6CfuWBvsWEqh29Pc4aGfwMIUZ5tkB1u4BFvm0edcJ5sUO5IQZrIB90srNJNUEKdLYQNhUdM16d9NxfOrVdtWn9XtUmHgivEMH8VAriAJpMf4E/kurMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd8lkRU2kgvwDRxVbYE0IIy6zX9SDm6NO6GvWqycFTUkHAAgnYqYGwhZK3J3h1SCQ+Rr35NmlCc9yIMyLIOxpIDjpISE8gv6mQlJ2b3rtAYnl90feKIA4YOpXnnDfCquUKyfEgHCDpVMgg5qhfnoD9xyBFSBofNSaOIBuoA8WTttUxr8s+gsTo6aKxk6Ht5vEkZS1CX5C8oOoY7ka9d7qB1GMz/l8FnTda/yJlcdB2O8EcQcKiu0X74WOvUo/lg7bJ6fB7KXvotU2mzdy0Fa6FEEIPwpEOxn+I0F8g3NkbZdY4DiBK6UCK0kCKAr6v0DZjoMFcTEKcCqILEu/1JDsBWVxeQLIykGRIxUbLso0Vjc+uvdO6gbRvhPmz6zSOu6zSjI8HXT+WCYSKs9YvNa3LdFnPR0MGUWDRNsogVAsgYrwtn92G/FEZ4Xf7ZYR1iwRIML67UZ83pHXZUkmmBPEEa28IWrYUfd4M81EA0UuCP22QYK3byAjvs/b18aD3lUxSJ9jHRY27B3XnhWsGQc47kN80XY/GHsQxG+lMgbkvZpg3Isx9Qftw7zLKEEIIIF1aYC2ssAYkEKb2sD4exnXsTbCZrbC/7yjeoU1JDLbdt4cQQs4kXdL3irAstk1dOljHMo07+Ezw3eFfGQ/UDwM87woSMBBG9hnm7c362cG+GKciINOeGoSCn8J/aTXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1t8aDXGGGOMMcYYs1s+a0DoBz3X1gBin0mzcCMkHLcOJC5t/RlD06T5uWiy/i2A1CkcpazOIGwC4UDpNEm+wLk+ghBkq2YaQCTQJ32v/gwikV7rtvQq55khmb170Xe4gAApdlqXY/5Gysbb+3XdqvZDBeFGhNDK0G7hn5CA/cfgcIA+O6gQpVc3V0j3eu9bdTOEcF63wXCEMXLW9rzFZ/1MkHNdg9bjfnglZf2gn9FDLI5BY7FspDZjhHidNXYeLjpe55O+/2nWcTgFEFjctE3OST/3+6++k7JXhzspe35et+c8ansMMB8WEKyVBvNh0vG/N3oQTiSQe7UZhFQQB1thSwghbP1kFZ5/btqP10nLMojsbiDrOGUdjBUkK+DnCj1IkXJeTwIPRWP7dNA4HoPaZB5e6TtEWIueYHweHlWmdDnoBHU86ece0mspa5e1dGpJH+WaBeb7gjIUvfda9i1iah2IkyL0D8xRoQfpCEjlwmbejlH7tWsghPrS9bPTQTyANKeAsGxpGjuvT2/0M8p679GN+qwCIq50g/3eQdsIikKedQwn2GcsJ5DQJI27E1intv6eGaSLGaw5eQBBDqxZDSROeyN1MEZBztSDjIwkeHmAPc6maMi6dtRKMk8ta9CP0G0hgbSrQVwkEDENReu3lVMlsBMlkHG2m47j/qj1qHDmmQt8xouui6Q8HUAWduq0TJd22BTAOaBB30SQylbYt34K/6XVGGOMMcYYY8xu8aHVGGOMMcYYY8xu8aHVGGOMMcYYY8xu8aHVGGOMMcYYY8xu+bwBIR6kKGECvyb+jg3kTJAgfZ3WKcLDQs/XqrUI0oBFJRS103foj5r8X7I2xwyiD8i/Dsfjui4JZCADCCc6EOKMg37ADzdI9D9ranUN0F+9vsMQVRJSskoStq6nw0Gffytajwh9UweQQTRtpz1xDwn4OWh7dkUFQIdZpSZvem3338UPq58fL9qeL/mdlF2fpChEEL3UmwqGLt9r3JWsYpYA8psKcfL1ZoA+VBBOQEy8+jVID67abi8Qwy+/PErZAtag9KgikfyVSsfSqO/VXTeCFJAelKbyMxImFChLICXYG7nXMTrC0jGDZOcAc+pDr3Pvh7hu50LiLRCPzSAFm67aHySOeAYp4HCv4/MA83HqQICU1nPjt7PKJU5vv5Kyt8dfS9mS9R1+O4OY5Hc/SVlrsKaAYCreaRvPSSeVNK7rUm46nq4kOYF1YelAuAEyqT3RwX6ngVSt3EAMmUFkB0ve+WXdt2dokrSQjBH2QCiB0/hfDiDeO+l1h0XFLHHUutxfN+vMi84HCfZ/vU73oczaSB9gTFw//F7KLp2ud9OsDfrwoNeNJ5Df3NZtTJIs8O2EDvqL9liph5t3RgdSoHjVdrhcNfY6EPTdgVXrfF3PK/Os80xbSGYIYh86L8D+u4KIiOSTGeRpSwIh7WbfE+EMFEDSGkD+FBYQ2WUQw77XdnqGSWYEodrxCCIzkO9uJVkkmEIpHHmzYO3sYZ/wKfyXVmOMMcYYY4wxu8WHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu8WHVmOMMcYYY4wxu+Wz2a/18o9Sdrs+SFkDIUA/gABJc5dDWtbyh2vVROVKNYWE+AzSpQpypilpUn8MJy2DjO48QFL6RlZyhETzOGlW8gyZyvOg798P1OYgoimabF1A/NAd9f3LBZLNt8nVINiJQctSgAR0kEE06Os98f6nfy1lTx9VJNMaCFH+jbbB8Q3093ndj78sel9fQfwyqDgppzdStoCY5vwHLbvcqTjj9gqEGxCL502QpbcqdLk7gegJBBkvYJj6qYLUAuQQTx9+lrIff/d3UrZUfdcx/Eut30Yedb2BhS2DqCfCHHEEiYI+bXcsZ23T55vOs6HpPFtuOubP/S/6GdNasvTxAnIrEGSUCeQasKzVWesxPsD4AStMG2HsdSpsuu/WvTl+q3FBDo6X849S9uGq8/gLrKcLrTMghbtcdEydms5j00XfNbZ1pQuMxeWm/dVAYBQbiOJgXdgTlcRGYJ6qIGe6Pmpb9aNK5eZp3e63ooESK6yfEWIMNksNntcqCIsC7At6ff/XE/y9Y1O/2wBiqlnFLxXi6XbVd72QTApEOpcLrGNV36FddQz3H3VOSBvhDkmXYoE9EOxZE81N/x4sAhDa4QZzRd80zspZ2/6l6nz0/LyOvWkC0yTIjx5GLcsF5nHYa7aojY+yI+j0BDG13QsvMJ66qOeMA8i4atP2bdC+DeRhw+VFysDrFBYQqtVZ54Vts9cF1okO5kRoyw5kbA3kTJ/Cf2k1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbPitimhok9S8kF9BE2g5kFXcHPSf3b75Z/Xw+q0hifoZM8FETofuqconyrOKDVFVqcTppAnLfaX3PQZOhU1vXpYuQ9Vw0sfwMCenjw19I2T/rNHn7fNC++fEMIqZHTcrOQduzG7Ws1nUiee21PQ4giOgjSD46EEz10K874sdF6zxdtT1T0uvApxUORdvlmz/7j1Y/l7PGaz2rvOZXX72VsvftGymbCgm7tN3fHr6Tsi7rmPgAYqfU1vU7XFXK8+GDCjL+7uWDlL1uGsPLrO12fvNKP+Og1z3cq5ShvNP2/OmVyrTGTRyTgKADOVl3gPFFvyOEe/fGR5C4LM+6BuQDzLNF59kjyDRev13Hcj3oGLs9a1z0R50XrweVqSwf9d4R1qdXvYpdDjDnXUDud7d5r2nW9nj/rHE3P8KaCG10f6fzfXnQ938GWccQdFz0YOboehC0tXVfLxMIdiLY/pKWLZ3em+Z9y/hui9b5DOtnJpcIWHYmkDvGuo6VPIDYsul+J5PsCuL66QLjtdO+DlHL8k3j7nEmYdO67HLWub1rOr6Wq7ZRmb5MZHnJWrcJ+mZeYO74qG18eQV7lLJpO1g7qR/GXuM/wX6ypX3vgUIIIUK9SbS1wBI3gLQrgqVq2Eh7ulcwP0XYaxaQGCUdjGeY79qk60wrMJBBepkynD+2akVY3hvIV58gtgeQRMUA8yyJvA667t5BjAZY268gnmubdkf5Km1lkj6rI5EZ1e0T+C+txhhjjDHGGGN2iw+txhhjjDHGGGN2iw+txhhjjDHGGGN2iw+txhhjjDHGGGN2y2dFTAkkBDVrMnSjR0HSdB1A7NS/Wf18fKVn6YejZu9er++kbH4GURAkPkfIBr4tf9DPoCTvXhO1j8e1YGDeJu+HEMbDn0vZQ3kvZTOInn6cf5CyvvtWykJU6c7xrdY3PoPUg2Qdm7Zrvd5XZ5UcTCCYKYvKG+ay79+bNJC/1Kzt1HqN9QBCiPYWREmHr1Y/f/egcoD+9r0+/V7bc/rb30vZ5YOWdd0bKft4UIHHadaxM4Nk67KRTqSfVZI0RhVHXQYdw+9+0diJJx0Tr59VEjVmEJF9o20XP2oc98/ah6Vtyk46lvpR5SIZ5pwFpF7lkewt+6KBxKV2tHRo+00gdglHjbPcr+eyb04qGEpvdK64jjo/f/xBx8+StR590jEQksrCwN8RDr2OgfFuXTa9aCweYAycvtHrXj5qXLxrKnZ6tfyZlNXLj1LWQ3+1pm1SZ+3DYSM16eDdwXsSlqrxHpvGUikgF9kRC5hOukxSNRBqHbU9ZxCs9HfrOYT8hP1B2ynC+Pr4XuPp9qIxEUES1UCu04F4KBx07n26rvv25UnHUqo0b4A8E8Q010XjPx+/krI6k/1G33UGmUy8wDre1u+fQHQ1gHWxg31ybSCwgXjYGxX2aemk8dietex00Mmh9iAx+n4dB2+azpUN9u31rFK89x80Vrqq9Viqjp8WIUZBPBajyo7iZivQk3Rtu6cIIbQGYwzOHrGD+Ek6fiIIdA8wF5UZ3hXqvN3OJ1jrA4jioLphAdFVnb58H7TvE4MxxhhjjDHGmD9pfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbPpvT2jr9v+0EOX216f8kXwvkB33Q542n9f9u96PmS3z/oF/uS/lSl6v+f3s46vPipPfebppvQ3k5pzevpSwP6xy+1xHyeTqtx/OouX/zM+TqQq7R81nzXEODXKsF8mp6/Z//ywz5epv8jW7W33NM8I3SS4Xcwqr/t1+uWt89MZ20fhN8EXL3qPnFz9M/Stnlg46Jdy/ruOsGzeX4l68epOy3F815eJw0d6lALsf5puPk8UeNpw7i5O6N5gIeTus4fnrUvMJfwRe3P0Mqw1g0dgbIGX73/HdSdoYx/ADxf3gLX/Jen6Rs2sx/Y9b81RvknoSm73CFup0gP35vTKMmptxgvu9fNEbronPZ7UXjYJrXuf39qM/6Z3e0BmgezS3+ImWlA69B+VnK3v2kubTdnb7rr//8V1KWN7meL1Wf9epO55OfZ10TS9U89ALz7N//8vdS1oLW903Wpf446HWXm47bPKw/t09ajxnzxaCfpSSEhfK5dkRKkJ8OOZKUwLWAn6JBvnvdtN8Ej7+L2nqPs/bXx6uuOy+TxnqeNCZmyOnss1bmHvrsPK3r9/FF3/PVoG10zpoHmYq22xXW3QZek1L0vTK4Ge7uddxNjz9J2dyt63d4q22Uos5N1M8LzJtjt/814O577aPyAXIfIbl9DrDXvkAe5suH9X0n3WeD7iY8zRor757U7VKaXldhv9zDnm+Z9N5xm8AaQiib/j1TPjzMglPV9h0j5K/CWGwwVlqFvFkYA7XoZ3RwdmnLun55hL08zInbeS2EEOYF8pnhXT+F/9JqjDHGGGOMMWa3+NBqjDHGGGOMMWa3+NBqjDHGGGOMMWa3+NBqjDHGGGOMMWa3fFbEFBuIjUBsku5VKHSCL42O8KXRyyZButfvbg7PRetxhWTjfIYk4hMk5kOS/LVqgnS3aCJxKnrdefNF8uDgCF/Dl/YeDioDON3pzc//oMKFR0gs7+CLi693+rmnXkUn8QzJ4BsJy+1FZTUFBDMNJBwJRBXTl+df/1FYPqjYqE0aoO1Ok9APBxAWhb+UshrXsf36jUpeLkdNfL9b4MvRX6Cvjyo0SIv22fWqMTaAnKvea5w8vlvfG686vsq9yqq+u4MAOOrzL2RsWnRO6GZtp5eTxuIDxH9rWnbq1+05gyDluqhoIoNsIIFw4zbtW0ITQgjtSeMiwQRHa0Au30jZsWp8l249rxxfaayEe43t14P27W//ViVYxzuV4JH/J8ZnKetedB17fFHRx+Pv1/fernrN2P5TLbvT+nbfafteP2iFP970M06Dvusy6pjq7/W6WjWWh410g+b7AhM5CeByT+u/FO2Kp7OueamphCWCiIQcbRmuWzaClfsMkqyrxkldtGxeNE4yyYkaxElTYVNNGjtz1TFx2QhxGgiW+ofvpOzbXmP9clQp4PkjiGRmXQOofdugbVLhbzYXGLNlIzabg64T4690HxeuWo8E8rNS9v+3ox9+0XmhveiYz3pZuMJe4K5TweN5WsfUqwDXvNOxeAsfpOx6ARFo1Fgps8Zx6XUdO456Xa5av9ttMwaK7scD7D3GDsRJIB67gnTqEHU/CkcoFN7doI1jVRFoyOs+LLpMhpR1Xci9vkOCcdf+CX8/3f9oMcYYY4wxxhjzJ4sPrcYYY4wxxhhjdosPrcYYY4wxxhhjdosPrcYYY4wxxhhjdstnRUxd1CT5/l6T6bvvVJzRP4LEKajUo2yS38/vNaF3ARnA3aRn7nLQ5PcrJDQ/3TShuY+abD2C7Gh51OTl1G9ECtPXck3+CxJYwO8N9LKQKrz/VROrlwIigUXfaxlVEpRAdvVyWWfWzyD6iZBsHUlUATKEeN63ianPGuunr76XsnbQ+B+Xj1JWRxUxTB9+u/r5p2cdI91faHu+gTgZH1QisDSVRDyV91L2cNA4SUXr20BMchzXqf+vBpUNHL4H0VlWGci4wLj+6Z2UxVnfIV90/GcQZ8y9xl1/p/31vDGpkLynPH+Z/C3BUG91/yKm8U7HwN3h13rhUaVL/UX7rQNBy/RhLYB5uelcdPxeY/urRZUTfQ/imKjj55I1fu4m7bcWVOrRf9BYnm7rcfYGpCTpKx07Qw+SnGcVc1xmfeBd0blifoF1t9xL0VK1vxKsd89P688tekmoIL8JMHfUBmUL3Lsjuqj9EyNI8JJe14N4J4C4LYT18z4WveYAbbcsuhfJVft6gntvIwihPsBaHjSeIghsho3s7D7p849vNK77rGXdRfedtwqx/qhjvcHnxqb7nftvVVB4HHSOKZt9Vh5gjiCZElxXQVAJqp7d8Wed9vf1Tteuc9NxMdxpHy0wX7x6Wcf8EwgATxkke0ed2/tO5+d6AzFs0XdoH3XsPTUtq281fvrNWSMl/cwp6rNS1vboZm3LDgRtC0itLiDBDSBoC/D+ywnmp7Z+r+Ggdas3kJ017ftI03388n2Q/9JqjDHGGGOMMWa3+NBqjDHGGGOMMWa3+NBqjDHGGGOMMWa3+NBqjDHGGGOMMWa3fFbEdPxOJS75pmfd/KMm69ekZXOv8otY18/rf1FZRThouvr1Qav/EN5K2fT8BylLk9btBsnQQ6dJ3mkp+hlhnbz8++UHuebt77Vu74smjMcESc4gSRoX7Zs2gexpBrFVDwntw1d673nTdkkTpgsk34MPK5RZk8gjJJbvibe/VoHDbYKX+6ASn6UHcUT9RcraRljSf/w7ueaXonG43KkI4HXQ656Lfub58ixlF5AGnECac3mn9567ddnj8LNc8+ZvdFx/nFQiEEDYNb3T9h2zCkdy1bF5gTEWRo3Z8agSjud36/donfZ9vek4vINfB+ZFJRzXsO/4DyGEh19pnOWisVI+fpCyedR55rb8KGVtM5Xndzp/flh07ahvVRLVR43ZQYtC+ah9mUDa9/aocfYI8Xir67JniOO3v9M16w/vdZ4YQdhWryDwmPV5y0xCPRDegXTpAAK1eVrXry26Fi9V2+0A62luet0SdczuicMI4rCo/dM9gbRu1DYuRcdE2KzR5UXnxekAIkOoRwaJ2ekOZIwTzLO99tl8VdlZO2ssfizrGDuBEKn/SdvyZdG14hZh7/hR2zJW/Yy7o77rDBPA3VHlia/uVQx4Oa77ZgGhDYR6gG1caCCdoj7cGw+w124zjO9H2LsOGivTk46BNK/ngVh1377AnncYVHr6DcwpvxxADpq07eeL1q1O0Ecgc12O6/4doR7jDeaTWZ9VO50rp2fdQ+VBx2cCweMy6L2HXuORznfXafMeJ32vBGNxWUAm1WlblvrlQlb/pdUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG75rIjp9qwJwv1VRRovVR91d9CE2wUkBMtGUEJp6T0k794XTd792DSpvzUQJAQQqswgcQn6/gEEA7Gur7u+6Ge+PP9eP7NqIvTpAOIDrUV4Dvr+x6RJ3i2CPOlFZTq3A7zrJhm8g8TtVqF9iz4r6WthfffE+QXi/6wSlmfo7wMkq1+WRymL12VzjX7mqaqA4GVRwdDUdHxdbypCaCAWuAQYmxliB3Lm80YUdp3092EfIeYqSLzGk8YEJfSHXj9j2Bp9Qght0v56r56fsECb1LJuu/5BpUTHO7hv/rJ37U4qDdkbL4/6Lj206QWcWodJY+r5pmUi44Pfpx4HFWlUCsaoFXmZQZx00fFzmTXen6eTfu4FxINt/bnlSWPx8b3WrZGI6EE/81BAgNRDTMFanDt9r/lK76B1ER/IqDGbYD2dmpZ1UN8e5Fd7olVYyyCuZxJbnWmy1Os+fljvWwrMd4cF4v+obdeN2v+lan0n+IwCY6xrGosLrBXxtp4nZnjW4wsIMIuKyO57/cyHr1RkCVu7EKOOk0PSdoLpOHyEcd2VjSAowR6owH4y6wdk2DumCBannZEXreP9oo1/7eCdX2CfftPnvTy/W/38DNfcB5XitQed29qom81jg70MSBTz1zCnPuq8eC3wuc/re+esa8wA88nlpm15PIE46qB7o3oB0RUIppYrjNkTiK3udb0fN5Kl1LRuA5wN+qBtBM6+MPRffg7Y92phjDHGGGOMMeZPGh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsls+KmNL5vZSdpwd9UKeJyql8rQ88QDL09Wn1cxXzQwi5VwHKXJ6kbALxQQE5S4G833rVd7jdtL4RkunDJpm+gYSCMv8jCHbat1rfN9/8pZRdfiCxkyZ5VxB9LEnL4qztHjeSjA6EGw1+9QGPD+WmAo8ZJEF7YpjeSdnzWSUR40n77Nh+I2Xzw/8nZR/er8dYHvRZC4jI6k0FFi/QnjHCmAAxRVhA1lFVJEDeiLbJrqfwLxXmCJBaxKSyhbffafxfiwrhbvD+9Ju5mHScPH38IGX9RuqRO23zKYP4BuI/wBxZq84ve6ODMXB+eiNlx1cgvyi/lrLh+38rZT/+3bov40HXmNhprEyTis2eXrS+5PG6gWQtgATtCuOMHijjAvp7Iq8fyOiO452UHV7rejo9/0HK5gSipF4/uIF0KRQd7ymu+zXCfQ1kSiRYWhYtKxU6Z0eQdOv8rG2Qk86f54v2bTrq864v6zguoF68xVdSdjiDYAtmvLRoTBQQIMWi8+cVZJQJ7g15/bmp0/ngelXJi5q+QmivVfTy5tV3UnaD9alc9R0WmKMDrIttBtnZZjFrEMNd1v5KTZ+VYO/YYF3cGwmMV+URpFIZhGyTtsOvQED407wWbY1VY6A/gKDuXvtjajp/3jdd38+wOagd7L/u9f37R5CxhbVob4F6RJAukXhwKjCPw2Z7arDngTWwwt49Z4j3oHPW2K/v7UEgSyKmFLQs0+EAxFGfwn9pNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizWz4rYjqDUSVXkOcMmrw7j5qs/+d3KpN4+c26GhFy/MtJhUUR5AKnomKCawExwb1KAqb2UcqWqz6PJEahbeoSVRCQITG/GzUBee61jX79ViUM5aT3/uPPP0nZ9VGFA8t0kbK+1zo/b5K3M0itxqQJ8xGSuWOvbdnvXMJxBgFBiNp26aBtUI4qMftPHv6FlP2b53VcDFcQHIwqobi/h7b7g8ZwO+vYWV6pxGt6p3FdZo2JBknzcSssAslF+lIRwNca/1+91ri7Pqmc7d2ibV5ABEHSqb4DScZmWINDIfQwH/S9zi8FJBUR5om9UUAU1HUqZwHvSujuNPb+s1/9tZT9TVnLmfonnSvy6a2UHU467i5P2knXq8ZK+vXvpOz8Dzp+6qR1oVlrOwbiDWR3EANh1OueQZT217/+MykrL3rd7z+o6Gf6qIsqjIowdNpfl81HdIkkVBDbFUQicG+FuWJPvL/onHJ50TXgcND3HYuOk2/fqGSsvl3Pb6nq/ETCrgrrZwVJSh713v6kUpuX9z9L2fWsdblCf/d5HTvTrPeNR43XkdbYrDI+kvzcZ510zhDZ4DEMNYNQqWk7NZFiwZoIgpwK4s06a1mmyu2M6ap1fCE5EbR9umrbfPft91J2OK6fl6OeMx4GHWMfis53L4vWo0KspEHn+/NVhU1PH0BIGUCiOK/XoxJAZFt1LRqCvusMkrUB9hrzDQSq241LCKGHs9yFpFBvYb3f/H2T1v8Ge7kIY4yWwAZ9/Sn8l1ZjjDHGGGOMMbvFh1ZjjDHGGGOMMbvFh1ZjjDHGGGOMMdkMwEgAACAASURBVLvFh1ZjjDHGGGOMMbvlsyKmVDUZeK561p0/aCJtGfTeX7Im8M4bac+h06TnBMnv46BJ7beir1SmZ31eVOFED9KQBRKke9Bw1M17ZUiELgVkFYMmL5eLyhD+9Q+/aD3uQXQR9F1Pb7S+15/13gsIEWJdl0HXh9K0bxaRF4QQQZK1gNhpV4CwZyqaqH79UZP3+6qCgPa1JuHPG0FAyY9yTb2oYKsskOQO/fM8aN0yWHNIlBWCJv7TnJDu1j+3rb0lhAAeslAjSCh+/1sp+rfPKo6qMF7vEozXV/oO05NKSK6jVrBv6+sSNHDsNYZncLUVmMNIJrc3atM5+wZzxe0HkE40Fdb8v2fto/N13WCx6jwWn/Uzv886Fr86Qt0Gfd7HBxV+Ta+0j+qTjuNUYX47ruf3dgGxFwiW2gLX/aLz/d/+9v+WsuXVnZQdowZfp9NOyC86Bm4jxOimu2YwaRxAwFKTxk0FiVsCcdCeKDPsbRZdU88XuA6e9/4F9lQbidEh6TUF1soe2rjAHqgPKixbIsgSxye996YxMQywfmziohWQ8vQaiBEEfRFEkR8+aJsf7lRQGReNseFB14oO5t4K47NshFIN1l2Sc9ai9ahV+wu6ene8h2V6pHY4aZ+/BUEfOJHC/WY66m4aFyPMlW9mje3uCP0xgHgrakXmSaV1I0lEB63L7bxuqAn8QjXrenKBs0FfdM2aPsL+I72RsuOg738Ydb5fko6BDsrOcSPLrdDmsCfoQLIHwyJU6NdP4b+0GmOMMcYYY4zZLT60GmOMMcYYY4zZLT60GmOMMcYYY4zZLT60GmOMMcYYY4zZLZ+14Az9WykbO01qryRnyZoMfD2DJGIrGIDn5wyyExAilaD1qOUrKYudJvrHqJ/x+kETjqerJtN3/fq6ZYQk8qQSgrposvV5/ihlF/XQhHnQetwP91IWB23P6ajSlAX6sE1rWcMCWdQvRWVFLWkyewd9mJqKRPZEN2j834EQoy6a5A7hFG4LtEtc/+4odyrIyJ3GekpatoD549irbKBOKntKQeNpPKjooi9Ql41I4NqrvCOA+CPAsyqIP6aLxma8g+vKN1q3Xj93bjrG5hctq5v2LEXbsrvpfQHmvtSBCALEH3vjbviVlB2jzm/Lov3RopY9PakYLG0sXR2I3PoE/Q1jgIRy0wzX/QNMqi8qyeiOMJAbrFG39fi5Qd8evwIj0gLzbtX6Pv0BxsCjziehPEjRcK/r2G3W9pxvOgfkjYysTipbmckmM+oYGCZty5j3LSOrTWM9HbU9G4l3mrbL47POvXkjpCtN264ftO0aTB/XovaX5UX3OyHpup1AOvVw0jWgwFgcNtvJEeRH/Z3uT2aI/8uzzqngZgpTVUHhGPUz+kH7q8D7T5OW1Y1VL4F0rC0gGAXpUoT1rhthXdwZ3xxg3wfvnF9AlvUrHT8ZJE5xI+2ZJpg/jtr2udPnH8GEeIWxeJdARvb2OynrosbZ5ax7geW63le3q8ZTKCrZqyBzzE33MuOosX1peoZoIEpa4OxVq4rX3r17J2X93XqOPv3me7nmzQB7+axt/gRLxfHw5WvA/ndMxhhjjDHGGGP+ZPGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvmsiKnrQUIAEoq+1wTpU69J2TeQdWxzdcdRqzWArGHJmpk/JE2Ojkf9zDJrAnYB+UcCs03q9DPGcd0mBdrjNSRbL+NfSNmbJxWEXJ5UEDKBcKHA7yHqrLKO5QKinJv2dd2YaOab1iNqs4WO3CUgE1qyfuaeOAwadw1ibMxvpOwe4i5Bu3SbPjtkjc0G0rEbSLzKvd774YMm/p8XSHyPWtaB6KYPOv5P2/gvmm2fi47X1680oT/3+l4//6z3FpBa5YPKIaYe5BrPMK4nvbel9Ri7LSpR6U4qICC3RgX5xK0HK8HOGA8gHoP5+C6rjOzUg3iPBGJ1PQ+87lRCETttq9KpSOL0SkVE755ULvF0A7kfCUJgqswVxsq4lizNs8bKwxuVAn5zr+KP+vL3Una7qVzjWkACBsFXQOyEa9tNy0pYj58KDbJ00B4ghQvUh/teAkItuualRcd8hjExdDqniN0thNA28rBBtw8hQXs2EEp2II4qIFxpi37IBPHfd7rPCDMIce7W46kmrUdf9fmJRIFZ63u9aT0qCNYiiAyvFx0nsWoc16KNPM/bPgThVoR2AxFbylpfksntjWun73yEeBzgvNAtem8BSdXpsI6pMcIguGlbPTeNiwMImxo4kUhmmkEs+upbFSAd3utc9mH63ernX551fNYZzi1R14XjG31+p5eFPOm6+wDbinn5Byn76Vnbbj7rulUfN2O7qsj14V/9x1J2X3SNPYAocYY58VP4L63GGGOMMcYYY3aLD63GGGOMMcYYY3aLD63GGGOMMcYYY3aLD63GGGOMMcYYY3bLZ0VMMalcIJHUYYTk/wmkFk3LcljfWyDh/uOsmcUNEv2rJM2zYGm6gewGjvARxATHpAKYsKw/t036mR+O2txvDpqUPYGYoFMHR4jQvndRhQNXkEYcisp0CpgeprB+r67pOyQQtXSFhAvar1dopz0xjCpYGqAvMpQNFYRlIKK469aJ9LcAMggQelSQYdSm98bDW6ibyo5mGMM9CCbuIRi7t+sY+3rWvn6CYfPw9ddSdrvo8+9eaeJ/edEx/BZEFy29lrL5pPPJ3OtYvG3iP4C85O5O69HN+g4N+nAq+47/EELoo8YxLAshgnknghAigWDjPq3bqzaQ/YAkaHrWtn961nsfgxosLssfpKxF7bcIL9EKCAU39pyhqCCj/Kgx1qqKPy5VxR934CU5TK+08Kj1vRZdA64gC8wgWZvjus5t0muOJ5jrwLCUQM5Yd/678z6CVa3Td4ujju9ugetAapM2spqyaEzMsPbWAvMdSE0uV23jGURcZMW6Jq3vIaic6qWsx87dQfcxNxAKRtjHkVByhHitsJ52DcSTE0jHItQF9rYlbNYykMtkWDsb7IEqSJfa/OUSmj8W33U0RvWd50Hf5QRjheRrW3fShwQCMJDiLbPG5wtJSqG/JxBvRZDskZzp4bWuFan7q9XPDaSt5yfdex173aMMJ33+cNB+OMCc2sFe/jb/pZTlo66B3ZN+7tPzerx/f9T5/lspCeFw0LZ8PGtbZpDUfop9rxbGGGOMMcYYY/6k8aHVGGOMMcYYY8xu8aHVGGOMMcYYY8xu+ew/Eg/wReV90C9vP0BuxXTQ3JqhwPO2eXOQq0nH63mA/+WGCyv8I3yC62b4//bhAHlokAsS4/r/6utR/8/+rteyG+VQUK88aD7HsUFu0cuX5Yf03/9a6weJau9/+nH181PUfJGc9b0eor4EZCiEOOz7m+UfIKc1H7TPjlHjJGXIfZngC903l80XyBkv2jf9AfLvIN96gLyCu0Xz7WbIkT2O+g4jfNl82sTO8qBzxPejfma+17F0gPy4M+RpwfeYh7uqc84FcqEefqPxfz/qOPn4ux9WPz83zeU6BMhtgdyTedG2pHyuvXE8aH9U8ATkRds+whoQYM6b4zqm2g1yuBeYKyHRM1/1M9MAOdGP2kctQ35ZgzxMmAcvt3VfpqOOgbvX32nd4Fn9K32vBPlcKet6GmEtLlH7q3ujeb495O9OT5uxDWvi0Gk9RhhPc4MYSZCsuyMOD/pudTtphxASeDcC5LGXBmvFJs87Yq475FbD2pthW9fSE9RD14WaIZcUxvoNqhc3eYTkXMhVc/wmcJiUWYO9zzAmYG+zwNwReshpHSAP+wB5s7f1ZzSYsmPUtuwoB3fnsf4pvnkF+znYkw9nDYzrqGUjtEPdrAuXH7XPOtgcV8j13sZiCCE8Q8fdJ8g5hesiuEIWmI+HTf77q9N/KNf85jt4r6h5nm3Sur10OrffQ/7qclG3TTeo1+C77/9Cyr75M+3Y8tMv64IreClefa/1aOpwCDfdLx17GLOfwH9pNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizWz4rYupAsHC6BznRVROfRxBYxARfBr2RuFxBTpJB4NDgWe0EX2h+VSFGzJpFPcMXCFfID04gGBg2oovxpE3bkQxg0fadF23L28sHvW7ULyR+AWkKeBTCn4PY6fCgF543Tdw9ap/eFhAawPvnOzAY/LTv35v8+ttXUtZB/JcbSMGi9vcENqqy+TLrBUQACb6UfAgg0hig3UEclr9RoVA5atL8kECIAzKpbSI9yZ/iQUVMfdDYeZlAGrJobMasCf2/zSovGEFY9g38vu7u63v9jLDu/+5Z61FBmDCAYGwBYUJp+xdznI76frHX+XgGOYNI9kII06Rzb9k8rsDcnsG81WBAtQNIgc761ed9/UHKQKUTIshpKonRunX9HkD08vqtlqVOBRnlpl9AH4qOz2UAoVrU9o1N2+T1Ky3rBn3Xy6av6wT9ADKysYM+hDmxQYzsiT7r+3YH3StMILHqos7b4BMLYdMVFUSGieRH0J4F5p7cdA/Uwf5sgn3WDQRTsepn9Js5Op+0X69V1x1qjgrimwQNVwcQkWkohgT9cMI9ms5hW4FNAwNPqXpfhD6EYRgmcL/tjdMJZEogLm33IMuC/fwygdxrI0y9HfSaoel8fwf7oA4ES8MZBJewJ30Gd9B9r5/xbtH4fr0RQb6F+X486Gferrr3AMdemBa9d4Qx9Xj8Wsrewp7n22917bk/QR/+ZvP+hYS3WuEE8tk3dyDTGmjlZfa9WhhjjDHGGGOM+ZPGh1ZjjDHGGGOMMbvFh1ZjjDHGGGOMMbvFh1ZjjDHGGGOMMbvlsyKm8qiyip+fNXl3GDR5t0HWeU6acFvCWjrROk02LuDw2cqPQuDk9+EeBENNz+vpSTP4U9X6TiS76deJ33EB4UBWaUZNH6WsdaAm6CCpP6qw5lZUnHMctL9+98PfStndrMng03ndr7crSDjai5RdblDfoPfOhTQM+yFN2v8vP6uA4NBBnIAA6XwDm0bYXJdALgPyr3nQdh+qtnsP8TTA0H+AsgbvUEAUlce3q58PvdoMrhWS9+tPUtaBsOnr/+Cfaz1AEHL96Wcpuzvqe13r76UsfHwrRUtY98V00faoMA5nEEFkEM7MIPXYGx2E7HTVwh5ENAXGT5l1jtr6ZBKsTBnq0cFc3H3Usje/ViHG7S1Y9v5WPyT1IF4b9Xlvvvur9X0XjYupQn+DYKmDMdZ9peI9kpy0M9jeQLBzvelY6aLGbds0fAEJT4Q5a+7AiANzQNi5i6w8/ihlLyBhiRlEZPByEdaKVjYxC+I98lV1ncZmBmndAHKVM8h10iMMsqPGbAWB3lYUdQWZ2FJBZAf7gghlrQMBINipbrBmDVnb8woymUPWvdJWClUh/icQR0V4V4x1kDjtjXLRd/nprLE33HQu++G97lN+gP6ox3VMdb/8Itdcg86ByxsQz1XdQ7yhOTXCueVe+/InEO89TCpjGzci1HjSZ10hCPqtiTCE0OAs0z1ozL6DffvrFz1XdDAfV/C2PkHYjpv1foKL3p+0HwZYtO9hfkogWfwU/kurMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd8lkR0zXrubarmmwdggocSoPEX7B6tE1Sdm4gWILk+uus11FK+3DU0g4EQIeDZiUPUBZBLvLxthYvTXBNSNpuh4PWYwma9P36TpOt4wlkUpd3Uvb+qonwVxDKlB9UplHiug8riJ5a1HcolGwN4TZPX56A/ceggmSqLdq3c6d9RjHwAon/w0a6EfVRIcwgLmia+H67gcQJfjV1yNruw53G+rio0CC80aKtOOMC9e2L1jdD7ByhbuMR5pKT3tuO30jZBMKN81Wfl59J/vGw+rnTaS68gKxtAJNQySB6A5nc3ihVx/wCfVmils0wR08kwdusC1v5SQghBBBpzFe9rgOJy/SsUqQ3IM759jd/rh876nWvmgpBfo4fVj+/zCrqWGBOOPValsZXUnYAoVgCEdWQH6VsAoHePIHsB9bZtJnbYgbREwihatF2y4PKGeHWXXEuIF+DMRGz9kWtOua7CG0wrsdEAolPrFqPedLrukoiRy0atvazEEKE/UgadO8xwrT1NK/H/3wBgRm4z3Kv7VFJ9pdgdzfqvXcgySqwBjSI/+uisV03f9tZUJoD/QX7uAj1iDuXUYYQQmgQF5N25hRAvqrTUXj7vV731WYv8Pz1t3oNrKuPQYVf+UX32uUAYqOi/fYtjItyA+HZK90vnYZ13M4zPOuq8bONsRBC6CHcH5rW4ysSfr3WfdvY9IHLWetC88dT2fTXrGv9FQRlw5Ougfm1Ci87WIs/hf/SaowxxhhjjDFmt/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt/jQaowxxhhjjDFmt3xWxDQuILUgs4vm24Y0aFL7MepHto14pSv6mQnkNBEkBKUHwQqIHk69vsNAAqhZb77BAw/9uiw3TTSPScsaJDR3HSTmg7Dm9kHfAfK+Q05671A0iRxy3MNc1iKiHkQa4Qr1zSDJmUDMQe+6I+abipgKxEm46LulXpPmX5G0Kq2flyGhvY0g9oJ2zyAdSyBH6EEuQ/KbbqTxpO9f0/reAkq0vmpZiyQggHeAOafMWlYriGmOIPVYVHQzwntd2lroMIz6/PwEEpqk4zpMKjSZYTjtjVhBpgASsApCiL7TmIKpQcZALCAx6fVZA4ieINxDBTnTHYhSDvckbNFYmUC6s2zGz/FeZU0jxGJbtH2PAwh2Jn3/BSb8BuNiBAHiYVJZR4uw3of1vf1B54Q2QbslEP1UENEMXy7h+GPQB1h7YX7Dd8s6b4/QxltnVQ/xmkGKGWCaoTmlgUyqBwMWTG+hQN/O8MF9Wr/rYVBr3Qx2mVrhWVqNkGGtCDAmGoh0eph0OpCHtgR7wLh+r0T1oLUtg3QUZDjts7vwPz7zAiIi6MsOYvTun+s88xr2Kdt55XSjmIX+XrQBj0eIPdi30V6jBB2z90ddKxKM92Ezb18CrFlJ995wzAoj1O0F5o4B9nIN5F4DtB21yQhr23lzWe6hzUHWdYz6rgX6dSGFLkgvQ/BfWo0xxhhjjDHG7BgfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7BYfWo0xxhhjjDHG7JbYSCpjjDHGGGOMMcbsAP+l1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbvGh1RhjjDHGGGPMbuk+d8H/+D/9D21bNt/0unZd4Om9FMX4Rq8bLutrbnqWrkmqETI8v0V9fJyLXjfqq8dJ7w2dFsZJ69fyun4pZ31WGqUoN2231ut7BXiHGLUsUQNkfddufK3XwfPCXNd1S/peET4zdfqsoWk9xvtByv7b//q/gpf44/C//B//pwTecqlyXUzaZznqdTWetKysB1QX9L5SoK+TNlMpGk8xwniiui36vAjXZfpd16Yu3aDtkZqO4ZBgCkowJorWLSfohwz9QL+ai3rdAWI2bPprhKEZ00HKOphMpqDvcHfUNvnP/9W/3E38hxDC//y//u8wBnQeqJHmPG38nF/pvXUzBqAFKrRf6iB+Fo2fLus8E3oYU0GvSzBH9xBnw3Fdv+GgYz0vEGNZ14Wu09huAdaPoHXLnfZDl2n+gHe9XfVzN3NbP8I8UbWfY9ONwgJ9OBz0Xf/6r/7FbsbA//Z//Y3G/xnmMlhnB4ixEnW+iGU9XyTa79RZyi7zWcqWFy2bYW1vMB9PN+0zuq5Und+Wsn7/eb7INbXC+gTvtdCaddPrQkftRPtHmIc6HU+l6Wekef0ZJWtotgn2iVmf1RVYnwYdh//9f/ff7Cb+Qwjh//mbd9LQXdFxu8C6moPG3tJgrWib6yDuMoynCseY+qT90Xrat8A8W+BsELUvYQYIeTNvz1Wf1a46xirsq6luywRnqh7eFfbkXdJ3OM8wly80B6zr1w/69hXaLbUXKbtUvXeEBf+//C/+GseA/9JqjDHGGGOMMWa3+NBqjDHGGGOMMWa3+NBqjDHGGGOMMWa3fDantXum/DI96y495DpSHsXyTsryy/recg//Zz3r/3eHpv/L3UHdCqQzdUFzdyiRqlv0vZYj5O/Udf1qhLyqBXJwD5AP10O+IeQqwn/Bh9Zrl+YOcnCbvv8QtH5zXD+vJb0vQp5GhP9vb1CPeaHMgP0Qz5DLUDQXJkC+RIR8m3rTPJ+4yReYE+QUZHgW5AbUBr+Hgr6OVF/IL22Qg1Yg53abXwtVCxGmm1YhPwiyReh5HdUXcn9nyLWkfJwCOYMprj/4ClkWA+SKNMjjqUnLzqAC2BvxAsn+y1GK6qgT7Thq25RJc+7Spo8g8zO0DPlHDdaFoHMq5UfFSmuW3jpDzinlUy+bXDpIvwotahuRE2CCnK8Isd16vbdCXh6l0nZFr8uQ+7d93rToNdAcIUZt8wR7ggLX7YlygzmwwVoOuZT0d4EEOddlc12DvkFhxwJ7BczLhvxidGLAvAWfS/Nb2OZhQ7zmResGqouQIM81BH2HDLFYRghG2I8EGCc9PG/Z3BorrOHQz12Fd4W85wJtvjcKtMsE4yJBvvsZ8rMnyAHuNpPIUil+YO9RYH2inFnYQ9O8SA6D2Os7tAXmz81GZYEPaJSvXb5sz0PeGUr8bJBzPsFeo4ArJ4CPJ27a5Ar79jjDB4ATJcDZ6Hn+8jXAf2k1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbfGg1xhhjjDHGGLNbPitiOoPAImUQ+4BkJ1whoRmkKHWT5NvOIGICGUAEcRJ7aODLjeEL2MekcoUQQLIyaVncJNh3FQRL91pWwUKQQODTDXpvAiHABF/62yUVifRVyyIIDJbruqwLJ7mGowgS0EEmFFBesR8uIGtpVdspJ5Uu1EnvLVET5Ou0FiXNkKieIWE+QrtXqBvJmRIIy0iclouOsRnECtvn5YHGPsgMKki84Iu2E1gJGnxRfYG4y6gqgC8kBwHaVsxDwpk5aD1GkC2EGaREJCrYGVeQmJCcJYMYK8zQv9AdbfMl5/MA7QLjKYHYJEB8FhAggRcrwHfBhw7mgDmRnGbd5zTvVogLkpz0MBYXaLgDTakwtntYU2icUeeIZwzmmAZytgyStQp7hw6EK3tinjV2Cu5HQLwD70bzRd0IWxbYd3UgP6F2X2AOLLDORpDRBRBPRpCdLTCnpq38ZmswCiGkg9bjBnulBGMuw7jOIDFaqgqbOvj7TE9StAGet1mfU9T6ZhAbJhCFhgR9D++/N9oM8iBY4iLuNWAfBP2RNzKiZYYxBjK+ADK6AOLWMms89gNI8EggB46hBfYu2zddaH2HuWOGNWCAM9WN5lmSGKGIivZ8JEDSfUrdSFozzEVLB3svmDsnmJ8i7DM/xb5PDMYYY4wxxhhj/qTxodUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG75rIiJxCMNZA1xOkpZHkBYBEnIdVONBCKaBiKWACKB2iCpH6RLAZ7X93AvCHYivP9c1u869iAhiCrmCD0lQivdCcQM8ZWUjeWF7ta6gMRqCHdSVrrn1c/zrHKBlCCpHmQFXYZ+lZJ9QYnqc9WYCOBcAOdIqDPIgzZJ7SREKiDmaBNInRrIxJqKKQrUo8skRdIy8GGEupF1pIX6Gm6EsV5AehDhOhrDnagQAphkQggH7cNTD/Ff1m3cQKICn4hCLHQN7FxCEwKPUZJQpAXkRCQoKSBK2gg2OlgnGsxZ0B2h0ZwNzwsZhBAwfvKg82eGwd3360gQMU0IAdxEIWcSD4J0CoQ4HH3wXlHfayuOCgEdIaFt1ihyoZAQCIYA9mFBGciOGEDEddP36GiPAtNxAUnMdtJLDeZPaKdy1WfNIMCs4azPg3okmNwpdk6wMZzLOu76AcQvIB06ZF2fKshl8kn3mB3sbYase6B2A7kOjesAUqRNVy+Ttm8P81xuup5E2AOdYXnaGzd4vxlkPCS3CzC/Fbhu3txcQNxJ8+4C8rAMqxa4jlDReFtAKESCS7g3bOY3Eq02WAQSvOt2TQwhhApnngR7iEiiNDjtNRg/GeSGy2YP2aADaZ4o1Jawxk4JNtCfwH9pNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizWz4rYqog2SHZSes0m/x0UPFQPOhH3p7WkoBaQBw0gsQoklxAL+vwXi0be01yXoJed4z6If2yrnMeSaShSfhp0OeDHyRUyHCfpovWAxLQU9LPHXpNhq5V32t77wC/5wCvCstvEghHQGq1J2aoX6kgQJopuV5jIILoYiv3KSBc6ZLGyUyWtAkS/yGuY1FZTYM4qQneAcQKXVw/r3YQh5Pet4BcJMI4WUA2UGEemqG+46T3HqBNepAh1LCOY3itUEF802AQk8Qp0QN3Rj2ctPCq8VNhfLcIfZrvMAAAIABJREFUgpIDGcrWbUNukggCtALrSapaVsEwlDMIm8ieBvKg1sF1G2lZBQFgRyYmiFlwxIQF1h2cPUGeRhK0DH0z9iAZ3EhIOloDCuwJoL4dya9AALcnFujrDqRjE8haMsiOqA3m61p0stSrXBMhrklgQvuCGvR5qema0mftswr9HZu+Qx/XMbBEWJ8q1U2f1YH8agCxU236/hRNcdTSHvq1Qp3TYTOun7UfIozXDvbO4QBr4OP+14AZ9jcVJHh03QHcVgO0fQtbkR3EHeiPBjyjKD3tvWD/McCsSusYrRV5cy/NuxTv5KIjxV6jMUWSTpAp0XVUlwOMvW1fH7N26gL7VpJVzTBYehBWfQr/pdUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG7xodUYY4wxxhhjzG75fPYryAUW0GT09a2Wha+krKSblI1bIcYR6pE08TdCsvHQ7vQzhwcpO0IycBo0szyeQZowwjvUtdSgRhAELCo0iSQqGVUQsjSVTsUCYhcQzBwHbdBMQqwXKQpdW9d5AdlKCipXAJdOCJAIn0hWsCemJykqNxB7JW3PcgDRS0/vu5F4QZs0kDoNIJIpEawHAeqBwi5I6AeBTWogGduIFXKGFPwRpEuL1u3upOM1gnTqOoOx7KbCkWHQz+iPIOu5fl6IcbuCNARsOBN1M0g4ik4l+4PGALxgArldyFqWyDS3ETF0IN5qIL6IYLBoJEBaSOqhZSTryEHHVAfrYr+RasFUGQYwLMUKYxGkFhHGYhdVMJNgrHTYTnBdA0HbRv7RJqgvCOtIdBVA2Fh2vgR0i9b5etN+pHk7QxD0EMcvZXPdAhoWEMXlSnOx7jNwZlsgFmF9WmCOijDGYlr3d4Z9Ylt0L9KDriWDsKw1ndvzDdr8RGIz3RcOJxig0K91MyeUI4gCi86RMEWEeSGRECl39kWFOapeod96eJcK+xmY3/NmT55APnqAfdAC8dPD/r4stCmFNZmkS6BFImnX1i+1gPxogP3YAoMMfJ8hkcwUZLEhwhoL70o+QZp7Dpv2rDd99wIBD762kO9I4kT1ZXa+XBhjjDHGGGOM+VPGh1ZjjDHGGGOMMbvFh1ZjjDHGGGOMMbvlszmtDf6HuqMvgqWcmaj/p93Dv5WXtM5VaEXzHihvrFbI1YRcq7lqPW53kHMK+QwB/g9+qfo/5Hn7//eQ81Fnzclo0AUx62eOVfNDwhFysuprKSvw7+LzVRNYO/jy8rls8rR6yHmCLymnnJTYU2bNvr9Y+wqNl+D/7xPkV6emfRYhl6Pv12UFmqSDfLYZvuC+h3FIqZ8NcvcKJJdlytWlLtv0dw+5tRFyd4YRvhh81Ov6CHm5nY7hCDk18Krhcv0oZUuv/dWVTZ4NfHP9FWK9h3ctAXKyYF7bHzq+hw7iHdqecmYq5GFuh0+BXD3KjVqo7SGXvEXIwetgboe6HSDPr0LeU0jr59EXwZcA3oQCeVpwb4X37wbINYO5AvONJq3LhfKGN/lc9IX00A0hw0QxQG5YRznOO2KZoO2itt1woH2B3ps6jbH7fj1fQhiGCDlpN0h8m6Ffy033SjXovugCi0WBfcE467tum6lAPl9uWt8KfzvpKC8bwqQOsD8tuvZQnvd81bokyNW7bYI7jfqZlbbSs/ZDa9qxJUDy4t6YIK8bHBUdzFER8nhrDznw2z00rh3QP9DOdN3Wm/DvgDUA1u60wPkG1Qzrz61w3wx7wABx0cEcU2EDQq8awInQICd+mTX2SoPxuJm3G+SrF2i3DM8i2cPpn7AR8l9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsFh9ajTHGGGOMMcbsls+KmLpeL2lNk/pD0yTcS1XxyAAJ/GWThNw1SECGJOoIcom5u9e6FRUOtBlENCBKCSDTKZRIvZWVgNBgAaFJvqgQqcCXgy/wxdoFpFYtPktZBLFNWUAuAsnQaZOAPcPvOcBxECp8CXRHXzYO1+0J+iL4eQDZAEliQKgVITF/64hJIDCJIC4g4UTIkNBPSfMgUwGHE37Bd4WE+y6t3z9niHX4wusepyB9rymepSzCu45gR4DLwvUGbQfyqDCs64dfig4hnJLGw1RIpLJvEVkIIeRO26WAoCuAsKWBnCkWmDA2sqMM4ySRIKNonJEUjdanflRhU4ZxkaGDUwURz+YzIsyVGWKAxBQVhEWp+7JYKTNITmDOyiCAilHvbZu+IaEHzWsVBEsTyIT2/pvzAmOZpILLVeN6BInXRIKVzZLfw1Ykgvyri7oXOwy6pyBT1hX2QOBOQllLhcVCJIBZx9dtAVnLovW9whgeMswbV1ifjiCyDFqXVHRdnGGu274WbLtCD/PGpB8ZYqG2hAfujBfYa/cwlhsIgMChGAoILvN2r0VjACRY4E8MkWR0MG8FmO8i9DBN2wv4s/KmTXoYUOhhgrpVWE9rALkf3NvB3J6jNlQGQWGj9fm4HhcNJKCJZnLY81WQr84QS59i7+uFMcYYY4wxxpg/YXxoNcYYY4wxxhizW3xoNcYYY4wxxhizW3xoNcYYY4wxxhizWz4rYhruvpey1IGI6Qbp6VkzlWeQwoRw3Fyj5pTUQ4J8f5Gy64tKlzIIAQokW+dJM78jyC9iT5nl6zo3EBVEkK7U7k4fNatM6bboew0gIVlGbbu8aDfPJBIZ9boc14aIEeQa86z3UWA1ysiPYMnZEcP4tZQlkpNBpn7q9HdCpdMxEet6TEQQfwSQZNHvnGaycxWobwdiBZAuxQFkNSCm2fo1ZhAhHPo3Wo8FxA1FxRxpgbrBO0wkgtChE863D1L2cAdzU13POzmCHAHEcTOIJlD0BrKFvfFwp/12BVEQuEhCglgBJ1Iom1BeoCsaTBXQ3SGB1OEAEqf+oPPxQON4pAAi4de6guBwC0cQIk0gxIHQDuEAMiWYA54SCOCgLtOs79UfYe3dStZgKqIwTmCYqZ22WwIZyJ54c9J1FgVqTRu5P6J1RYrqRnZUQTpEZeGifQhOrABLEa5P86wDL4HpJjXYU20GY60qz8uwt+t7FU+W8FHLQAbUHUE4c6A5msogPqOuz91WsghzyVS1jSII1lrUd7g973sPFEIIr2DuWUB6WEBcOoIIdABBmXQvzDM9iJimGwU8iVaVIwguZ1jjGwiFEozHtJ0cQWo0nrS/rxM8a4K2hM9cqL6j3nt50vPS+Vn3Qa9+pet92uzTI6xjHcx/JBAtcC78pywB/kurMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd4kOrMcYYY4wxxpjd8lkLSDpo0myaHqQskyQCRAwLJNO3uk5ybgMkJfdfSVk3gkhj0aT+cIAkeXBrgK8pJEjy7kZNyq6bpPu+wjW9JmCTdCeO91J2e1G5Rg9Si5z0/aekfVhB/tEHFWwNh3VdLrPKFXqQa4HPJHQdSBP2/msTaLu4gDghU9+C7AiS1etGTlEbCMyuJCKDfp3IuAFWmyuMQ5AXZJBV9CDhWTbioRpVphS3tp0QQn//SspGGBN1VonAbYHP2EozQgj/P3tv0mvJlqZprc6a3Z3G/br7bSLixo3MygaJokQhIYHEP6gxCAkJJAYlMeCXMGOGxB9gwIA5A0BMqqisIisbkRGRETfiXr/enG631qyGQY3M3tflHqOwVLzP7CzZNrO11re6c8737Nw+QNnl4Q2UbZqfQpmvp1NkzKTNO5xMEpEzOY9lhQimlkZxOObdBQVyhYwBR2QdORJR0DzOyDVpJCKajHFhDjh/jjWJ7SO5X4192Q4o4rEBx0WYyXQiGf+pwnerKrx/TaRoXcI6FCKFuSbrjLFPUHR3PkDZZofv52e7BEveI5K+qWu2/pNXIx6VJZENqdvcPGeMYf68Nfm7QKb2sGkMWAwJ05O5/WiITIsJ+sjc3hI5UWCSFIv7AhvwueNsboxnlCkZgxVzJAAaJiIjhQ2ZX3zC/emwRgleJjK1mogx/VwCSqSjicxXmb1bwDZvyT5uaWQSP1Uikr1IxEakHQqZtpu59JGsjRdS1o1YtnVkDiTjM5P9giVjalVjv9ka629n55vHHvcoJuAc26zx8FGRNeZIRGnFkfHTY/1j/nsoe3zCfdDtNe7J6u30XYhvywzkbOfIOlbIfJ9JH36IpR8ZhBBCCCGEEEL8AaNDqxBCCCGEEEKIxaJDqxBCCCGEEEKIxaJDqxBCCCGEEEKIxfJREVNEB4fxGQUOeZ6sbowZR7y980QAM7/XBTN1TwOKJJzBbG5bSMI9k85g3rypybs1FhOExyPKCsIsaZpJSTzJXs4Jy0jusvGZiH4stu8Fc8NNIn1YCkkiJ/fr8ywpnYk0ahQ17EgivCUylOCXbeGIEcURzBERLJETEenCqmC75FnsjBljuDB5DUlej0ScZJkIgcizPBWE4HOjw3ptVjO5ABGRpRqfGYgRra5xfJmEMqWrBuOu22FZGfC5p4TvtyICuDyfYyKR0JA5oiLjOhJhQsfMNAsjkTqbQOLHYuxlEiuVJXODn342k0emgQlQyHtsiCWCjEVbEzEWiQu3/jSZlJuJh64qjEVviZyGyF/WxCMSC66xNwHj59Ki1CMecR5rNkQw5Mm4nc1tmayTNRkDiYgYQ4Udm+yyx0Ag26TckLXcYKf1pK1WRGQ3d3FdTkTgVOFc6Yj8xCYiRBnI2CTXBbYGEOGfG1AwU2Zip4GsT43FzUgqRHxD5Ez9BetQB4z1J0f2gPdEikT2Y1siSrzY6Vjvz0TGR8ZrRfa/lmw8MxPMLYwVMWaOZKO6Y7LNLfbbmsSZm4lV35G9weM7LAvkPSLZf873WcYYk+H0YYzLRHpKhHddhdfVs331zuOc7TKOnUxkXNvm00SbOxI+b3ZE3PkWBVC1+xWUbTZkvzgbA10mcluL7+bIOaO2+NkLafMPob+0CiGEEEIIIYRYLDq0CiGEEEIIIYRYLDq0CiGEEEIIIYRYLDq0CiGEEEIIIYRYLB8VMYW5IcAYY8sNlFWJJWozYRFJwJ4JLJrxFq45ZRSxRJbATkQSJGfYGIPX9RHftxAhgCeymzQTiWxIPQNJ8i+kDqFCuUAxKDAYSVK/74noosVk6GDxGRURKrnZO3ekT4m/xjTkmZaIaGxLrFMLovFEsGSwnQyRjqyImGOzIvF5msoFfIOfO5D+v5xQOOGIXKI3pK/J0O9IMnzd4/3W/grKvJ3GRUtMOjYSwVRa47vV2JYNafOBPCOcUXRzKCiOe/7yCyxrsOz9+9eTn8eC/ZCJwMwlMg7jEcqGgZjTFkZLpHLJYUyZHue3ymNfbloivJqFxtpjmx49xmdPfu9aE8lHTNjO24ZIMoigzBEZWePIGjCTb/kKr4kdSjiuI8Z2v8NntqQO83XHGGOGPc7RjxalhTdrfO6W9Ov+PG3PXJEJn0h9mE2rN0REQ2RFS6LdYJswwYgfsS88+WxLxDHlPBPMkH0Ha6UYUUzDBEvRY1w3RE52IZslRwRIgYh54uwNNy3OG03Bem3X2Ea1w32nI7LPfYN1vT3g+z7scO6tiD3pav0ZlIXzdP3wLdknZowHNkUSN5NJC49/Y4y5XuOLn7CZTbvHuFjtsM+fVTso67rpdWSLatorjIHDiOs7mbLNELEO6xW+W1OTv+WRuby54DjuzDTOMIqNKeQsU71/BWWOtNutw1jpeozHVY97rd943Lu8+PJzKHsZrqHs7mk6LziH64kz2Deo2TWmI7LcCxFXfgj9pVUIIYQQQgghxGLRoVUIIYQQQgghxGLRoVUIIYQQQgghxGLRoVUIIYQQQgghxGL5qIipalGKVGUs68e3ULbBHFxTiDiiH2ZlJFm/61FgkROR2kQ8h9vCxEZEhuAxbToZTDiuiFCpbabJxeOATVt7fI/tChP/Y4X1fxjxfd2ZiEmIwMTO29cY06wx2dqvULBVz9qpIW3pEnkmkaaElohziHRoUdREskPas1xQ9LBuUHSycxg7b2fysKrH+5/P76Cs6zE2SZgYWzb4WfJu1eo5fnaDSf6GJOHXYTp22ozCgLVFWcWKiLiSwfqfWhyb42/voWzuMzHGmEtAAYG/wjZ5IoIE00zj2JP707mElRERWU2EbUvDVeR3mzTO7qBslbdQ9txj7L1pZjGFXWYu3WsoywmFHn3CsRjOKHrYb3G+W98SeQyR5Z1WT1B2NUzbqe1w7GwdjoHNc7x/GvG6o0fJWP4OG6pviOTkgrG9ffUSytw1tlOYyf28JfFA5jVnmXQJPxrCsn933jLpUMYBcDrhvHhbERnfDd7vzUx41xORyuWE4+tE1p1siehsYKJMInWxOCY2ZP4cGrxfyNPJcT3iNbc3KLt7sUMZTFfhM3+9x7qmdyjhaZufQFkzfA9lq5rEZ4MT/FwK5044vkYikgkZx/+RtFsdiHFoYVw1uCZvydYgDjgGmjtcu6vPcS28S9P4/u4e+9sTsc/pCWPl2KDgsTuQ/f0LLOtX2B9XRHp6bHGMXp2mcbA9YXzWG5wEdy+wfW1B4eUh4RwQnvDsNVY4pgpul8z6Czx/mBWOgc1sLasSabeCATESsaEv5Ez1O6wBy14thBBCCCGEEEL8QaNDqxBCCCGEEEKIxaJDqxBCCCGEEEKIxaJDqxBCCCGEEEKIxfJREVO6/AbKLudHKMsFk3eLI0m4JPm9zCQE+0jEFCQR2kRMymYyJUvsD9YTsZHFpGRLZFKByI4qN03ebjeYCB4Kka4QqU0kv0uoPEn+dyjmYO9riTjKNUSKNGIyuKumZcERgQ/xCFiL/eAaTNS2FT5zScQzJrlfziiXKSMmyJMhYQ5HlAvsL9PxFDscN8Vi2+URJR/FoWAoRSLF8igvyCN25JHIzoibzKxm8dlv8SJHpEM543ukPV531+IYPm8wdlJG8cGJzFe+wnp1RyKMWE37IhLBVCJt5BKOr0QEa5FIfpbGcPgOyg5HIlBLaHo4PeJ1708Yj/E8/eyBiJhMIbFYcJ51A86VccDBWJG5t9gX+Izn+NnViM/1s+XU3eBYLzuUa0RS19Mdigd/GIkoLLRQ9P7hF1h2h+Ko0eFY2dp/Qp4xjdtC1mfiWDOFrGOBjB/LPrwgxp4Idd6jhKVYXI9/8yvsx/sXGHeH91PJ2CVjzOUe54+KSOsGi7FpSB1shbETNhj//Qbrepvxs201nT/XV8/gmisio8wdttvd2x+g7JHILbf4GuYUULo09Limrkj9zZ6IQutpHCdH+oYIZwoRihayxjq2oC6M4x7nj7szxkV1+BbK/s2/+i2Uba6xDUc7HSvvLjhXuAuuJy92uB/bXn0NZWZEyVh8RKlsJv2bWuzfipwrxnE6vw/PcU2sK5SgJiJ769/hvuWu4FgZG4zjhwOOn7v3uC7kEdfK5viPoayEaV1ZG40XspARKVwhe342Vj6E/tIqhBBCCCGEEGKx6NAqhBBCCCGEEGKx6NAqhBBCCCGEEGKx6NAqhBBCCCGEEGKxfFTENHgU7+SMQgwfsMwaFJs0K0xEv/5mmiB994DCif4tSiMGh8m7ucZkazPgdZac1xtP5CIBm8iSo36I089mi0nUY49tdCBCD7vB5PDWE0nS1St8D0OSyEn9Q8HkdbPBvq6raf2LQaGDyfhuVSDvG4hwoyL3WxAXj0nuw0jERhUm6vdnjKeWCLBuXz2f/Py0x1jPJ4zDFRlLlxsUEMS3KFEolgiwiPwjZIzPE4nj5vB+8nMKRDBGJF6XR5Q5tCt8ZnPChP6HgG0+WJTfZCJnS2d8v/oaPzvE6XV9xHZbk2k01Ng3qcKJI0es/9I4OhwDPbEHOUMke2TuuSXCt+bVV5Of7SPeq7/gnLqqcB4b2i+x7An7Njqce24rjLPPGpQnnQvGTxWmIpHT8B6u6d/ie7x7wFipIhHAHXHcxRWuFccVtq9vMEaHd++g7OlHOAe04zRuLZGMlUDmxEDWXTLvOCZZXBDnAeeZe4fvvCZlhrjDiMfKrK9/NC2wZF7oUcLSZpxTOo8CpMMBRYE1VstsKlyzLImxC5EFrs10fQ8WK39/wJg7PmJsXmV8jw3ZP1iyg70LxID4gOO1JwKkeoPtWex8DcAOXBsmWMI6MBlhXX+6hOb3xQ8k9r59h7Iff8GydiAixIgd9/WX03mbhKdJDxg/P93gGvBwjVLVeIfzcWywv29qlBM9Lyi4/O6CkrVn1fTs8vCEz9yfcSyemNjP4r6tPeK7vd9inD2NOH/UDmO03GHfvLtCkdlqJl4beoxjTyY7W2E/O0fEleRM+SH0l1YhhBBCCCGEEItFh1YhhBBCCCGEEItFh1YhhBBCCCGEEItFh1YhhBBCCCGEEIvloyImN2LyeybJ+sZhYm4hIib/2Qt8id3Lyc9//gUmpt/8KSZC/7bGROjX//I7KBvPmAxtDMo1osf7GY/J+k1DREnbaVNeLthGKaDQpGLuhogJ2CeD9d9dYR1KxjL3DPvwurqGslWN75dmsqtg8JpA5CWOSBiyx/colsiZFoSPRAewJnKyAYdSCpgMP7TYfp2b9tkf/xj7cFd/jvdvUYbz87/4DZQ9XFBeYB0m/ruMYprcoRCmqjD+c5jG+/EtvtumxTHRjSjNeDyh9KHx2L6ZCIKqgs/wRIBUiMBkOOC8U7fT+FwF/JwzGP9jISKVQmQDy3dwGHtGeVAJKHXIbL4vOH5ii7K84KbijK9e4cRYuZ9B2XmN93//NyiSiBHlF3ZAWcflEcf2OaIYza2wrn2axsbpgHHRH7Fe3TXO98NrHBdjg2vRpsM6VDWuFV1AOU+0KDwcjjh+cpzWy5NfdW+I181UGO+pEDlTXPbvzlPB91sRwUg0OKdutri++Qb7x66nc/5PidhyTZ55yji3v/4bLKsHHMO24DzemXsocxh2pm5QCLO6mo4JZ3A/ta3+CMrCZ3hdd8Bx4lY4Tq4GlPCMFxx3+QrvF3siCxyIZGkW8BURQmUioRuJ6K0kXBfTP4BFYHgiArVIxD4e9y7pFvu3+eLHUHZ0UxnZlzXuPfxz3AdVL7HtL3/5d1B2f/dbKLMv8D18xHWmvibyOYv1P87MYPnyEq6pDe69uw2uT/1rIgEla4A/4dzuE8Ze1TyHsuxxDcgHnGdKM1sXyHmvYXsCS9axC77bBaenD7Ls1UIIIYQQQgghxB80OrQKIYQQQgghhFgsOrQKIYQQQgghhFgsH81pXc++9N0YY8aAeT/NAXN88gnz6y5nzEG6DH8/+fnp/GdwzX/6x5i7cLjH/JvT5i+hrJxJfpTHL9WN5H/UA8nTcjXWf5h9tie5kJ9d4e8IDh7vlS6YD5c9/n/7U491cAlznDYOn0vSEczhgrlbPk/zNxz58m3vSI6HxRwAls/EchWXRP2jr6EsjZjf0bzBXO3x6W+h7PAe+zblbyc///1n/xiu+ad/inkL70ne9EP8V1A2JIxFW0gSwQXzqMaIZdsdybWYfWF0GjFvYdti3WMi+RMF8zYu5EvAS4f5QZ6M18bj+Ldr8kXYCfM07GxcJ5KXOpJc4ELulQvJfR2X/3vDcYU5eJn8vjMcMUfuOPwCyk6PGAfV+fXk53r9Cq754xucP044ZZlTwLy8keTWug7ziC4PeF0/4trzlcW83MM4HVNv3mMO3rOG5MIOxC9A8kHLNY67/oAN8J7Ua3PEPPENyX1t3vwcyvbVNE/t6iuS90py5AP5EvnTiGPgqiVihwUxYLObocYF1J9xHTxnnMvGI3EH7Gfz8We473q1xf56v8cYO1To8DgYjOttwc8eH7BvXYPr3c2auDPyNLaPZK/X1jhnf5+x3aqMeZD1GueXJzYfX3CeWBMnyPoK328Ysb9sPY1ZT+bscyRze8R5rqvJ2nZafk7rY4tzavJY581bzOHsD7+Cst9GnI98NV0rXIVj7N+7xrn46QHb+b5Dt03vsL+f7tB/8PoNcWpc4Rj40U9+CmWX9XSyON5jzujVDbbbYyI+CCgxZlxhaUe8I90Jc0lXA8Ze+5zkjht85zFMx20T8H2NwbmjDDjej4Y4HIZPXwOWv2MSQgghhBBCCPEHiw6tQgghhBBCCCEWiw6tQgghhBBCCCEWiw6tQgghhBBCCCEWy0ctOMPTGygrBpPwyzUKgFqHXwTcjCgeOuZpMvS6YJLvsSdfZmt+gLL0FsUUjSVJwyTv1xlMwLYkG9qNmNR/itPk5fGIyeG9wy8yfrZDocc+fYtlA/mC+z0mqm9aIkDqULrTExFTuWCyeT2TxwwDEeLcYp9WJCk7sHfL2K9L4vQGE/VTwi9WzxXW11QoWLl1KCrYj9NnbNYYEw9vyRehF4zXdMT2bMmXoY8jE2WxL6AnYg4iZzqepu8SEgZYIpKwm2t8twsZm4cLEYBZMl6JmKME/GwVcAwbizKZUk9fpnTYD2kgYhUS1oFMJtksO/6NMWZ8wHk2JxRi+B2WVbsvsKz+Eu/npjKJ2xbFY2ON8bnNKJx4/YT94Yk4ohB5iiOCkHLB6x4uaOc5vZkKkJoRx8D69qdQdn2FcTHglGr697gGnAO+r7vHMXC22HZr8mXwb+5Q4tS56We7C0qytj+BInNVsF4u4OAemRVwQdy9fQ1l1uMcHQJup9qAa8VqJMKmNOsfss7en7APC5EbHu9xHV9t8D3sGT/rCu5bwgXrFc84D353nEpX4gGFK7dXPyPvhtKl1UusA9t7xAvKKGsinrRbHFA31zjfD0ecJzavpgKf0z2Krk5vcB4aByxjfydKhl23LN7+zV9DWSIyOv8Z9m+3/gbKXq7/BMpOfhoHN1+h3LJPOAfefob7rPGvsWxL5tmQsb9D9w7K6iPOUZcjxt7j3fR+wwFFRJvNvw9lX6xxbOevcR17/w7ndpNxzrZkW5GITOrZFa7FsSNz1kxcWcg80RORZ9dhf0UihYvp08eA/tIqhBBCCCGEEGItadIQAAAgAElEQVSx6NAqhBBCCCGEEGKx6NAqhBBCCCGEEGKx6NAqhBBCCCGEEGKxfFTE1DYoXTAViphCi4KZ1QYTiYcek5fNLLH98htM1H0XMOn7xmBita9QBpIMCgfGiEKIDRE2FSKsGQeUEFzX0/N/Diidqa4xEdo1mNBcPeJ7rCuSwF/jdY3HZGgb8bPtCrs+tPh+ZZz2RahIMjt+zASH7ZaJnSaXZYtoKo9xHWpMys89in3W32BC+/CA8R9eT5PQ99/+C7zXF8+hrCXCCU/kT2nAmBgKxnBliUwqY+d2Z5QLrOqpmKb1KL5wGzLdOHw3f8YYawZ8ZiLj3xWcc1KHc4LzOHbqighshqkcYjhje1iH7eaJdCmRutpMxAoLo25RiNEGjEe3RelSE1GyUtU45o9395Of9yPOz02L4pQ6YfutVhh7sZA4brDfNpnIXo4YU135Dt+vmc4Lt1tcs+o/wvcNZI0JRyyryVpxfn8PZSZjvRoiDtq+xPnp1Us0Kl320/nJt9hG/YjzidkSYSORrBW37DVg48k87kkb17gG1FckjgsKisb7qUzl7off4osQe2JjcU6tVtg/I3nfc0MESB0KxvKAcZcvKKtxZlr/G4PjcPcjKDKxYJwYfDUTH3ENcD1KaBL5bH/BPeVQcA4rK/zw+/upZPP4A84HPRnDJmPf28QMOWQDtTDWGzwHuFtcF9bNV1D2bIN7Ekua6/H1v538/P3f4Xz/sx/jWIzf47iod0T45nAf0O2IuPV7vK5mJ6UDxl5dpnXdOvzg+muU543kKFadsew84NzxuCfiKGKztBHjvdtgPPqAa8rhPH3nM3qjjCNxnIh0KVks64ho90PoL61CCCGEEEIIIRaLDq1CCCGEEEIIIRaLDq1CCCGEEEIIIRaLDq1CCCGEEEIIIRbLR0VML75EgcPlggn23T0mCB82JHG++/+gzNqpnMI/fQvXfP8DJge/vPoGyl48w2TjfEFJxKHHBOwyYIIwS5AeHvCz3W6aDH7jn8E1V4/Ylu+G91A2OEyizgfMXG8DkSKtUPSRW+yvdYVihl2Fie/n81TCUIhgKRCBjysk094Q0VNZtojmxfNbKOs7bPfjHWamHytMkDf7n2OZm8ofqgve6917FES8evY1lH2xxb4+EOlSIvHkRhRMOIPCIpJbb84zyVAmF9m3WK+u4BwxFqxrImIeb/F3bpsVkUnZHoqyxVg3DtvOzERM1qIwwBLJj2VyMhLqiYzhpXH7Oc6pQ4dtNR5wLosB+7eMD1BmzbR/w/F7uGb/A5mziAxkQ+ZeV2Hs3XVERkbm+8ahnOb8hHKaSzWNldzgvfLfYvy8ecK1bazwuvMbFH9YIr9oiIzsuCLyi4xz2/UG16g4kyf2JI5LxnnCEvlNIuEeB2JnWhBXNyhmOY7Y7v0dERbdYNkY30DZcJi2cXjAteM+Yvxfk/1ORWRxocGYeNgTUSYxIDVkrjy9wetymI6nc4trTPwl9vUPjxjXjsikTI/vWxUiz+xxXI9k/Xi7w7554VCgeTlN72cj2WOReGiJhKcQEVPxZEFdGM8+RyFlHHBe7H/4DZQ9VCgeMkTIGvvpWhHIvPibiG36eI1yJk/2N9UG153H73BvYCzW67pGmdSbO7zf4Kbj1hGB6viXZAw84PnJo9fNpEccdx6HhfEWnzFEbM8Kl2dz0+K60B1naxuZi7qIbd4wx1jANsm/g5BSf2kVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYPipiOo2YrJ7OmEQ9HDCjN5+x7DxgUrbP06TeA0lWb06Y0fvDgLKOmoiCco/J1umCib+XjMnLNhOZQMT3M7MEaXuLidVDvoKyMePvDa4qlIvYz/GznogJzgkTtcOI9os44HNPJHk7+KlgyrdELkCcS9aQNiJOn6b+aAj+XhkitokbUMTlEsof7LtfQ9nxhIINN07boCM56auEjbc/4XucyYfHjO+Wz1ivRMRGTJxSiHjIzcbYGElsDliWCr5HSwQ2VU1EL+Tl+kSETSQULwXFPGNNGt5N4986ItIg46YUvC40RJzmmKlgWZw7rIs/oEwm4lJhVkQMdyEdkmaxcY44Z7c9CmEckee5Fu8fSX+YM05cjwUrUVnstyFjLJeZsOWhJ/c/EBkZmbNvVjWU3b7EdWFMKA1xHdbBJiJGe8Q2+fVvX0NZ6KbzTN6gIcQblKH0hgjQEs4xsSz7d+eHAfvHnrBs7LHd/R7b4MmjEKXfT/dUacC9065HQUo8EtGTIVJAItkbTvhuB7LfIc5GkxIZ7PP5vSfCwidst4HM2W2H8R9qfBGyjTGtJxsNIuPr3hE525as97N523+G8b/p8EVCxDHXr7Hdrt6TBl4Yb+/eQllzwvbbH3C+39zgPHMkMqs4m/MtOQfsmBQvY/slg+PnxISURBb3QERee4/9FkeM5TKrQyZ/F3x4h+O/kH3Q6oSyw1BhbMcK90tpxOc6cr45vkOZVNniWGn89Bl2hWuxJweB3JH1eY39uvWfPgaWvVoIIYQQQgghhPiDRodWIYQQQgghhBCLRYdWIYQQQgghhBCLRYdWIYQQQgghhBCL5aMWnOqMSclHzCM22eOtGvMzvPAWE5r3P/wwfSmS+Z8sJu/2FxRC7ZnYhVRzTESAQhK6syNyBSKYiGX6jEgkHKNBCVXVYGJ1s7uBshdfvIKyJyJhqC/voSxHIleosSyQJPcwuy4Tn0mo8Hcf3mKbO499YwK54YK4JmUPD5ggX7UYO9v0H+H9rv4llH0/i39v8F5ti4nvFWm7gdlwChGikPAvRDqVidTDFSJRSNP+9paM4ZEYuzx5EYfSsfbmJZRdjjj+K0+kU5nIwzwRrKGrxMydHpk1HJEzWSKcMUSOEBORPy2MdsQJf9/hvFXXKF5Zuy+gzG5RoHf//rvpNQ7vlSuM7XzBtt/3GLOVQ7HLQMQUngTBYFAmUSx5lziNjQuLRSIFpKabVxjvP/vmP4ayNw8oSOmfUJIVehQDloCxd3rE+4XZUJnX0xhjAjavqcgaEIkkKzwuW8bXEPHkA5HnNCushz0+g7LdZyg/eThO292RvycciLDr6pEI74gQypHxFMk8XhIRFjHpEpEn2dlcbkkdRrLcWyL2Kztcea8//xrKLvcY64aI8RzZAxWyfkYiLKtnc/maSDz3Gdt3ZbFe/QnjJkbs16XRnlDQ9vSI63R1Tc4B+R/hdV/jPPPd//P/Tn62Le5HL8Sx9czgPPtYcMyaI364J3JI02H8jJZIl9hmeA6JgVKIFM/gdbnBMfDiZ/8BlB3ufgFlqSPyuArft/JknSWCy7nraS5ONMaYXIho0hBZJjkb9Wcyx3wA/aVVCCGEEEIIIcRi0aFVCCGEEEIIIcRi0aFVCCGEEEIIIcRi0aFVCCGEEEIIIcRi+bgBIeygaHNFkvC3mDTtLNoZ/sOf/SdQ9nd/9W7ysz/h/evnWyjr+jOUHe8wybcO+G7NcxQWnU+YvHy5x6TpsMLrqjx9P7fCJHUbPoOyqx3WK66xbDBYr4YIPGyLEqeaiGhYCnmxJGl69lzniEyp4LtZ8usQR5LNC5EwLIqEMbz9Ci9bX6OcyY5Ytz/76p9B2c9/OY3j+ISSgue751A23hJBzF+hgCAeiRCm+SsoO92hIKQM2LfZYZlzU8lBJs4lN8/mN8asaoxXs0Xp2OoapQTB47guI8rOXCHjpMIxnIiTIc/GRPY4chwRzhgi08oeBR4uL1/EdI5ktigoTghEnBHDAcr+/Ab799sX0wkjjCiEYXKWQORu4ZFIyzLG2fqn+G6nb3FdOFxw/KSK9Juf9nkg86KvcWKsa4yVsvkxlF3XuN7Vz3Ct+L7HMXAi8ovugOO9qrAPOzd7PyKragqW+WoDZXEksjdLJosFcRxxrmDrZ9jhWuFGjJ1/+s0/gbLmaRrb7kj65vYFlNUrbLvvfoPrRymkL758B2WGfHYg8rAcyLqdpnHiyZ6FlTkiMRwbHK/fvLrFd7vGnvj+HsfwsCcGnx77db3G+z3a6Vi/kDl7HXHcJCKmqWomMVt2/BtjzP2Ic09h0tMVjoF4jf3xnz1DSetf/+l0rJR3RFy4wjXghsyf3R22c0zkHPAVjs+nX+IYSERwyeYAkI+RNYD9qdAFjJUx4Nz+sy9wHI/XP4Ky1z9gHboz9ldlca3wuJU1hzSthzUYD5acH7Ij486RtZOtpx9Af2kVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYPipiskR2Qvw/ZohHKFt3mJh7OmFy8dWXU3HGpkNBxuDwVVsiWFolTDYuNX52jJjQnDdP+NmE9dq1L/E6N61DcHj/REQlI6nDjghrDBEWNSvsiECkMHWF/RCIEGNIeF2ZSTJixMRyEg4mk+siE/iMy5YQjGtMJG8z9sUpYT1eDCglyAHLPv+jaQvGR+zDQsxGrcX3+NOvcHztR8ys313+BMp+Of4CyroDJuo3NcaJbabt5Gpst5Sx7qXGceKJ1GU4ERlOi/VvHRHCeRJ3CdvpbFE4EuxUt0DcWqYizxwNigUyMTeMGdtyadiM/ZEMvvfhHuUxLRG3/YqIGAY3jY26QvFF32MMbIncakMEFvWaSKIaHBf+Bjupdii1qC1eV2bztkufNo4bIi9x93dQ9rcRhSbVNcp5Go+Cmc3nGLgP36EsMHoiy5sFPXWHkXFROrywJ+upPRPzx4LIESuXiYbl7h73LbsO/y7wi2siHrqZSobaCu91LDg/XRFJ0hWRn7iGzEcW5X4Pa9I/pK5sjrbrmbRxIOtEwPbwREKT9ygU/Jtf/AVet8Wx0xJp0PYGn3F+jWX7Aes1f+P5XGWMMT5gP4zkuphRnhgt20Eti0L2iyPZk959i3NP47HO//se2yuaadz6jHOg68j8TLbL1y2uARsiOH29/gLKLs+JpPEJ62CxyJhqGi2FSQzJHt0GLHM9zvf/9lf/N95v+wyLKny5zQ2uPeMDPjeSevnZOttFHMehwvunhPWPRGrVp08XsuovrUIIIYQQQgghFosOrUIIIYQQQgghFosOrUIIIYQQQgghFosOrUIIIYQQQgghFstHRUwvdjsoiztM6o8DClVWn+OZOJNE97pME+ejw0zg4DER3LR4f0vEH7EQEUvEJPkvblBqkT7D+mcimHJm+oyqQhlCTkQwYzApeUXEBDuS1J/JZ5kWqbbYTskRkUiPAoO5dCMQkRB5XZNIlnoisiJHhCZL4jmRWHVELNCeSWL6C7zuuMf69rN4HyNKOGqL7W5Isn2qUFZTGSyL32FfXK1RzNFe3eD9iMTIzmLMWry/r7DuY0/EFwGT8isifTA9SqKCxyktJxwTQ4efjRHfpaTpOA5EQDSvuzHG5IxjyRDnWCZyqqXR1rdQVpHuYOPbE+nS4YTCJj8LjZGIXioSP6XCtr+Qdh567LfxiDHgiFDw+gbHBbNVeDeNs3CNa8CatOU43kPZ0xElJMe3ZL074ti+aa6hzIwookrhByjr2fqZpjGae7Z24JyVC17HHIPWYR2WxLoia94VEVZF7G+/wfh/9/Y3UObydC0/DjhGQo2xOa4xriNZj3siRRo7FL24Aftid4V7IEuscvPQGdZwiVltv4SykjF2zgOOzfM7MrD3GK+rBh88Dtg3B/caygaypbL9TJRY8JkPlqwdZA2sPdss4Xq6NJxBadu6IbLRhrQD2X88kDHg7VTI1hhs55bsW481EWMSUdDjA5FxPfwayvIRhaybNdZ16PG5YSYWHYksdffZj6HMJlwDzieUkR3Jvs22OI59jTHlNljWEWnfhQyCNDsvjQP2Td5jP2eL81/lsG9KIpPFB9BfWoUQQgghhBBCLBYdWoUQQgghhBBCLBYdWoUQQgghhBBCLBYdWoUQQgghhBBCLJaPipgcEdGsyVm3qjDheLXGpOnYo8DC+WkCdkkojbAk+b0bUHbSOkyY7hMmA5cW3zcTKZBviVyhxjaZi1fenzGJupAk/5XDupYN1qt4TMCuPNYhjKzNUWowEOFCIk6kNE5f+kJ/zbGCktoTeQV5tzFgPCyJTOQv7Yh90RAb1YYJwDLW93o2nkaPSemBWHxOI5HGEGnG8Qk77dkahQlvH4kkgkjXAnozjDfTwr3F2GyHHsquWhQ9WU8kPA4fGkiSf5NxnhhGHIuPJ5QteIvT4TATpHiH75aJ9CERUclc1GOMMcUsW0RmjDFkajeeLB2B2JnqCvsjEYmTidP49oasAZbEp8e5kq1Z3UjEWwnH4jBiX9akj1zBZ/jV9J2dwza6aXA9SVco5tgQUeD7M8qZLh2KNM4OnzESMdieiKj8gGNqcNMxOnYoDWECwEDWrEhkXWXhMj5L4npF9kBtwP6uKlz0S4vXuZnEyW5Q/hUKfm6scF70LcZmunyaKGskwrt8IfOxx/F5tZ7Kei4N3uvlDte2vMM1YHiDgpy+w7mkJ5uqjsjEbI/7nf6CsW7JHi3N5EmRrGM2EykP2V1nT9rXEWnfwqhJTLmMMbBeY1/uapxne7L/HGfr6DWZx0NDhJcVzmOlwr49GRRvsfPIMOAzYiLiISJ4rJvpGCgB3+P5NYryrl98DWUPP//XUNadMfYuZAwUIu48HlDYdBxwXihnItqdSbHGEdcdU7DdPFkDmDwxsYH3AfSXViGEEEIIIYQQi0WHViGEEEIIIYQQi0WHViGEEEIIIYQQi0WHViGEEEIIIYQQi+WjIqZtQKmD85i8THJwjbNYaInAopkJUAaD98fUYGNSxKR+S6QZ3pHkdyIhMET+0QyYgO1WWIchbyc/70jCfVXjvQqRPIRAZCDkfR0RiaRARExEgFRIH4ZIpCkzcdAK86pNazBGatKHg2Ntzqwsy+HGERkASfLvWpRV1OSzI+nHMPvd0TFhsv1pxL6JxN/wUB6gLBG5xt7j/bZbFAREIn/ZXRPJVv1i8nOT3sE1uSfiJAwdU0hMBCZrIRKFeoVtF3sik2NCDNLGxk8FATZj3dstluWe2IvI+O/TR6fgBYDvuK5wIshkXDDPWiDSknom45sLsIwxZix4MzuQMTbgWOwjPrMn7odC6hqJaKutWJxN5SIViZW3FiVGtwHH3akm68cKy8qIcbYhc0A3XOF1ZgtlmUhTBjOdaAKRrNVbXIuJT83YhohELssWMTERV82EbA0RPg5kLiPilNVs73EmoppzxrYbiBmxv+BnDyOJ/xFj3VrcAzki2WrZjmw2FFfkmYcTClw2ZJLIHseEr/CzLRnDrUW5TJdR0LktKC1k4sXjrL9cYhI2bEufsG9cRaREn+6g+f1RULbpHK4BPmCMJhJnTCq2ne2DCmmYIWOb7k84zw4X3Bzd91iHrsP+zplsSjwZ20RuN84OQh6Honl8g0IoH0lc1DinriN5jzPORduA0qWn4RbvN2LblRqfsU+zfRCJ2WqF96oyjhVLYiQSKdqH0F9ahRBCCCGEEEIsFh1ahRBCCCGEEEIsFh1ahRBCCCGEEEIslo8mVF2tMMchB5KH2OP/t1vy5cBlnvhgjMl5+n/lHbuXwf+Xrit8/VLw/8z7iPerIv7f+sWT5qjxfXvy//Jl9tmNfwbXbF5AEc2jcyRnKFmS40T+z3444f+3kzRUsw6Y41Gu8XcY63neIPmf+qrG3KhC8s8ySXx2JPd1STy/JvFPYuL2hK2caiwL5Aupk5u26fmOxCvJ3csk/vMFv2h7n0muocU+G9wRryP5yjGRL7R303iyJF/u9gZzKqqEyRGJjHXn8LpMvty8P+HYjCRv9Nn1KygzJG92nOUpZpKPaR2+r2d575Z8mT358u2lsV0RdwB5bzdg/dyG5OCQHNEym6XKieUCYlkM2LeO5b6SBEsbcSzOv0TdGGNq8qvdTHwNZTbOzmRuq0kO4ntL8tB7kvdD1l3rsA6O5Ig6Uq+rlzgG1isc73evpzlYJ4tjrCLvtiHzU5eIXwGHz6K42WE+sG2xf+wey/rNJ8ZdmfbPeMZ2SgnHUm5xDrQVPjOs8X42kc96vK4NbD4m+XyzKa/ZYiytLNl3kGdWG+LXaDEncU38CrsTtu+exOfum+dQdkXa5Psfvp3ey2FurSf331qM/wvJhzVkn7A02h22qR9xXagH4lQh8egK1nmchYu7kHMG005ssZ0rlkdb4zzLnAAk3dYUsnYXktcd07SubnMD1zS7l1B2GnE8OeKNsNe4r7omsVd3WImROHDMZ19C0ZUn88Kb6Rg4kmTypsZnXpG5g60Bv4vaQ39pFUIIIYQQQgixWHRoFUIIIYQQQgixWHRoFUIIIYQQQgixWHRoFUIIIYQQQgixWD4uYtph4nj0RIhBpAuGyDq8x8Rn+DL4Fp8ZSOJ2VZMvOXeYqNyhH8EEIokKRBLVki8Cf0ua7SZMz/+bDZEQbIjUaSRCE9KUZ1IHm/Cbi3siimmJ2Komgq2WtHvaTtvYEakVfKu44XIFZ0n93bK/WP4aHRxmJF82z4RdxDtlhg7b7xindoGwIWKfGgOgkL64XeEXpq8GTMDfP8Mg2/RYr9uMz7i74DPcOI3F2zVKM7a3WGYKxqFJaFt4GkgMk9jZr3D8+x6fsSZiiVWLYolumImYyBeejwXHOvHjmMqhuCGR8b80thusTNVgH13OWL+GrAE5YnyPcdaXDRGWGJzbmMcqOzK39xiz9ZrIdCIZx0QcYYgsr5mtASYQ2UgmErMj+dL7EeUl4wkla67Bz/aBBJ/DtnsWsL/aayxbD9O5IhzxXiMRzAUyJ7Yt6bDHZf/u/Nkt1sMRO9fFY9mGaBB7IuM6zUKxkP2DJwtKJmtAXmNfbCIKYYYVimksme+J79EUR9aZ2X6sIfsz43H+NBHXokikbmP/CGVpjXK/yw5lTz253492OA9tNjjWGz+97rM7nDdOCe/VNEQStyVyobeknRbGKpD9IpHsHQ84b61JABGfp6lm54q0Jm1Kms+RvWbc4fjZ7r+GsrN/DWUpknUBH2sKkVSGatqXayJda7coXUpk3F32eF3uUJaZdzgGComz8YTt9NNneF1zjWt7O5vb/D1+rrcYI35L9llrvH98zdZ7zrJXCyGEEEIIIYQQf9Do0CqEEEIIIYQQYrHo0CqEEEIIIYQQYrHo0CqEEEIIIYQQYrF8VMRUiJzlvkexS5OJFIg4F/Yk4biUaRKui3iv7DDJtzeYqOyIEKIqmPhbHCZ5n4nV46HHc/2aCGDS7P26FT6zI+ncTcbE9dRjUvKe5OpHS+6XMIk8WSKNCJjQ3WcU0WzytB79iPc6B6xrXRFpCkm+D27ZvzdJJIgfL0Q4U7Afjz2WvRtJLPppQn8eUDgRBtaHRBgwEPmJxWF+TSQU/Qbf7fiEY70+o0yjXr2Y/FztcIwMFZEpRWyjUojUZYfij7HC+tc9zh1tTcRx2xO+H5FCxXpa/8xEPUS4k4gMyBIRWSbjdWn4Dtv0RKRLNmCsjGeM22jxujyTHVkirbKZfA5dGKa+EBFTi5+NGO7GWTZJ4YW+wrly/jtgG3F9YvK0IRA5DYntsmKyQxw/MRORRkBh0+vHX0PZekCRWZ6JmHqyJvqCdb2Uj24v/t39SZMviabH9nx4wnZ3ZD4+3mPcnRLO70OazlGOiLNKIetOg31RkXDyt1iHnohuwnsS6yu8oc04VzZhOibcgO0RyfiKiYzrQORnc9GZMcYZFJudRnzflcPx+qtv/xKvuybxf56KnToS/yVh/B/I34SsJ7LLfwBrQDm9h7KnI67TZNkzYyJrIdlrx1kYeEckYxbjYkPmxXUiC8M3KOgablBQdvwlxmghc3RuMVZW1z+Zfm6PoqfHt2+gLBHRpiVCyp7Uv7YoVDsP+L6+wnj8+a/eQtnNS1wrLvtp2+VMxkDBM8UwEsnamWityNr+IZZ9YhBCCCGEEEII8QeNDq1CCCGEEEIIIRaLDq1CCCGEEEIIIRaLDq1CCCGEEEIIIRbLR00JTEwRiDyoENlLuWBycajxumYmZ0otCgJaIuwZiITCEWmC8fjZQJL/b0hrNAGThvMOE7rrmUyqGHyP3BEJQSbCBZJsfUXanLiZTHqGyeFNxkT44+UJ34+IE06zj1omeiKCqZ4kke92t1BmLavFgijYF4UYXE6RCKruSFut8bqrMr2uazERfktiM42Y+F4ZIhEgToKKyE9eVUSIUROJwk9fQZmbCSZih6IjM+BDydA0JFzNjUUpg20xdsoa265KKHE69eT9iCTL+NlcRMYmccsZ4lExNmAn1ol8eGGMBaUOkdhzAmmIjoyLnkiW5tN7stgXFZkrI1ljHJE/jB4/G0gdHLnOVTin1gXres7TdylEiOSItCs4nNs7Mrtfb3AgW1wqTT1gbD8R4dmZrJWlx75OszFQWLsRAwv7jbgj8RCJc2dJRCJA6iMRZbH4H7HsiU1wM/FQJutnReaK8wklaT5j/FsSi4bsKTZE1uIblNVsCo6TU5m+SyJ1SEQeWZE95khkfKuWLIIrvK4+Yps8dii/Oh2I6IeMibk4jo1rNos70l9+YBIaEg8L48jGPOlf67E/ciR1Jgukn4kaLRGo1iNOFv0dxnHDhIkF9zfbFvvo1ZefQ5l7jiKv6x7Hxd8N305+3u+JFY04h9ZEPDZUON/XLV7nrrEtrw7Ydvs91rVnY2DEF3Sz80wibVmRveLlTN6XjOP4O8j49JdWIYQQQgghhBCLRYdWIYQQQgghhBCLRYdWIYQQQgghhBCLRYdWIYQQQgghhBCL5aMipiGh1CEQGYk3RFizJvILIjaqwkxiRBK3AxFYOCI+WHtMXi4FE4RDg8+4kDP8asekBiSJ3E/rP5JrmhrfbSAZ/Exocsj4bmsiTcjEauE9EUCNmDDvPbZTP+trG4nkhAhs2oD3yj2R2JBnLokhYiK9J/HvaixbP43jaMYAACAASURBVCdxRwQEq2YqOmlJInzVEqnFgBaWddPidQHjpCKxfrIYn21Dfq9FJByjm75zdCS+EokdIrVg8rOBiGkCqUMmSozaEdMLiWNDhGJ+LglZESHcmZmYyDMHjAdStDgcHQNE+DViPLoKhTUbMh/n2fzumEyGSChSh88cSZvWJFYCmaMdEfRlIs4Y2Bo4k4X5Cl8kkrFTiIUikDi2HsuGIxFdEXmcI2VtQmmZafH9rJ32YZjLyYwxIZF6kTGQyXVkqVgU3fEAZa6coawfsS9si5KYLRaZNGsrW8hYIu2ZyNo7kPk+EAHSOuCc2hJJGtuPDYFJzKZljow5a8l6MmKDBLKfdMTad3nAwdkR4aUl96syynXIMDHFTee/mqyxpiMf9ETYNOIaG2oiyVoYLRExDUS+ZkeyhtbYRyuyP0jVtA2rROYZss+yF4yf2GKsMFnWZsS+vCJGVn+5hrKngsIvMxPeXd+grIk4xkwkorCKyP4MkXt1b/CykfifyLRgjMF4ZFK94qZ1bck5qyaSPcPWbCJdCmRIfQj9pVUIIYQQQgghxGLRoVUIIYQQQgghxGLRoVUIIYQQQgghxGLRoVUIIYQQQgghxGKxhSX7CiGEEEIIIYQQC0B/aRVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVjCxy74v/7iTZmX2RGvy8GTm2co6y1eZ+P0htZYvFdIUBbx9iYez3h/8m7e4/1SqqCs5A4fgrczJcXJz0/nE1wzXrDhElbVxDJA2XDo8bMF381lbJRcyEOaBp8b8Rk2Trs/kT7NI9bLBnKvAULJuArf47/7b/9L8sK/H/6H/+l/hJceOoydMkYsc1g3719AWXbTmPXY/SaSeA1knGTLBgV534oM/YH0o8PPmkzGiZ8OCudwkJSCz3Skp7Mlv0uLZNIxGGPB1PiMgPer11dQlsi48+Ms/j2+cBkxrq2/4L0S1r9aY1v+8//6v1hM/BtjzP/2f/xrqGAZsU1jwbKSMH6yW0HZGKdzmSeLTN+TMTZiO/cXXAMyeY95zBpjjC9kcidjzxQsG8u0mYYO14DKY3xaj3GRCsYUG8fW4XWBDCpLxpQN+FxryHNnbVdIPycy7ViH46kiW46mxbL/6j//Z4sZA//z//K/4hrQkzYgYeI8WQPCLV44i/dgyTxjcExUpA9NxnHCJlpnSV+zfjQk7sjaM2+RbHF+jj0+IJJ9TCR/T0kDjnVH10WsVzQ4z46kroasAXa2Lo6ZzHNkU+hI/elaEXBO+O//+X+zmPg3xpj/81/8Es8BZAOeyf7ekz1JJPFjZnvoQq7xDmOb9WMhe5niyHznsT8ymaMcGVOFjKkwq2smcezZZEnOKLGwgxaunSGQ8W5JXTO5X7WBojqS8Vim89h6RdYYg3NdsGR9tjgWN2tskz//45/QMaC/tAohhBBCCCGEWCw6tAohhBBCCCGEWCw6tAohhBBCCCGEWCw6tAohhBBCCCGEWCwfFTGNRDhQRpKES8QrfSAJ8R0mDbu53IQkPTsi8WHimOIwydcRkUAkyfSWPJclUpcRE5XzTMyRybuxBO+UMFk/M0ECkc6QHHKTiMWnOEz0N+S5juSHp5kgomQiKiDvEQZ8ZgkkIZ8lpS8If8ZE8tC3UJbWRLBCBASlfwtlVZrGWNwQ8QWRHjARQD1ijMUKY5EME2OIxMtHvN9YEcnWLHhywQdUmciPKmy3EFBgwyRJgY1NJo4igV0yynqqTMbnXPxAPmeJcKuisioiL4psrC8L6oMgZbYhy0mNbZ86IvyaiSMyaefk8HPjgLFimIeGyLiIs8vYkcyzJM4GSyR4s8uIX8XYkcRFIPFTYbyPiYjCiLApE4kP8dAYT2RSgUgy0kxYVYg4x5JnOoPzJBMCLX0I2AtZKyMKUUpNpCYrsuYReZibdVAq+DlbY9slJhgjc29gIhkyXi2RKib2tw3yXFfN4iQTmRSZY5k0hvnAmBCqJvu4VJH2TUQCSaQ+7Yj9OsyKSo9zThk+TepGlmcTI5mwFkZK2M6ZtKkh82wi+6BMLVjTeCnECsbkq4UVejLPsicyeSARQBVyP0vGaJzNxyWzQCaTMXkPQ+bUmMgZZb7wGGNKwjnaE4mkOeHcZipyv9n8cRzI+YmsiUyKZhw+c/87HAP0l1YhhBBCCCGEEItFh1YhhBBCCCGEEItFh1YhhBBCCCGEEItFh1YhhBBCCCGEEIvloyKmTBKwRyIUqTImnbMEaZbkbGbJykPEe1VEOGEbJnpiEhusZkWEEGPGBP5+JKIDYvoos3fuLH5uLmv60P09kRXkgoIAn4nEiQixAsn+z5bIUAyW9XNJABHMOFLXQu7PJDmO9PWSOJMY9sSAVTyxrpyJwMMSUcFMbpUu+ExHhFWuQoFLJm2ceyZCgCJTE0EAk2zlgci45jIAMm/Ya3xfk4iUwOL4qojsiMlFRkdkNUREFjyOCUeEVWM/70OUyziPMcwkXM7iuzFh1dIYmVSNDNtA5gaTiLTPE4HFTNB3IRIO/4SiBzanJBK0iYw7d8L+aIjUIpP5bSBCumoWB/lCxDHX+G4DcYcxeR5zSQUi6OtJ/euC8V6T5b9u8CGXcTbOEokHIg3xbAwQGZ9jIp4FcU74ft6QMd+QuSES2VXBtspxOvdEJs66kL1TTfZFxJw2kDauIpmjma2GrIGRyO3cMH0/EtbGBaxD77ZQ5okQLZDrmCUukjFRkfhfO7Iv2uEj8kzE5ahkj/RpJjIcJiUqZF1cGkR4xdw5jrSzYfFO5tT5HZnnyZL1xJA9L3u7TPbVtD/IGSUTSSXxMBlr59I6dgYie0DSHLCnMsYkYq2rieyJeZ0K2aeEQCYaspWdnyEcOz+Rc+Fc4GSMMYnsZSu2CH4A/aVVCCGEEEIIIcRi0aFVCCGEEEIIIcRi0aFVCCGEEEIIIcRi0aFVCCGEEEIIIcRi+agBYajxkkKSgQeSNB2IraOQ9HxIQnafllicR0wGTjRhnCTrj5gM3M+FE8bQbHAmkyqz5H/ikjFmIIngmYhz5vIjY4wj8hLrUPyw8ljXTJLSncEE7FyI/KM6T36OFyJwInKFYFEcZQPWqyNJ2UvCEYlAIu1kO6xvqFGwUIg4Yi4vYHKVwrQHEe+ViCjIViReSay7GuvlSaJ+bfG6OItZV5F5Y2AiACIDiViHhkjXPDEG1GycENGFD0T2RtouN9O2Gy8o4HFkPgikjZiE5nfwD/zeKKT9RjL3ZiI7qogkowxEIDdrmhWZFwcyz5iBCN+IZCz3ROJEhlSq2LyF9aJjIE0fTFxfxhKJUUukPo6IB80KxyKTjFX5hPcjz6gDvmCwayjL/jD5uY9kfiJd4y2Op+DI+kHtPwuC9MVA9hnVhQgaG2xjS4SMxkyf4cgeIBFRnutxPkpkbbeGSCZJu1eG2W+ItI5eNx1QgUn8yJirmciRiD0DE10RQd+OyHWMI/udCl+wNWT8z9aU0ZzhmkRETBW5V/bYr9iDy6NURNjD9stkLXREKMTMQwX22izGyLuRywoZn0wNFonEiBTxeGfMxxSTAjI5EZMpkYp1pA6ZrJUNWYvZglQCjp+mxs/GmfDNkmd61hGsH0hRJGe0D6G/tAohhBBCCCGEWCw6tAohhBBCCCGEWCw6tAohhBBCCCGEWCw6tAohhBBCCCGEWCwfFTEVy0Q0WDZgbrrZbMgDPREl+bmEgMg7SKJycpgMbD1eZxO+byRVL5YIQogkIDPpRJwKLDoiq0jjp0mSKtJGrB9yQrkIS5BuHNY1E3FAJmaSeW58iHivZIkMxaJewNZ4fz+SBP8FkQORUzHBWMD+boiIyVcok+jO08GTEpFQ1Ch1SJ4IV3oi+8KPmlKhcKUhoqxssQ4t+V1XHqftNJD4soWIGxzGRO2xzZuKxCYRbgQi8CBD2NRkjBFXjalm5pAwkrFERBPEX2Vsg23i/wFYOMoa69d2WJeRxG0mfV5h95o0TueyPBBRGJE1JDLuzJGIx8gcZRwuUGGF68eYsM/JVGZWMznTmcyBLuP7RiKwaIj4pNRkHTujYMdkfG5dEZkU6YiU8X7eTe/XkPHPNBou4MBjThbHxuKCyEQqZ3HpNYW0uytEntWQ+s7CgvgfDdnamNwSkV2HZYkIYRyRiWWH8WnJfq+QydLnaV17i+uTjUTESe5VEfkZcY6ZMpL3JXLHigjADNkXmnLEz1bTfq2JqKYn605Tkbpe4XXxPb7G4mCmNSIGG0jgOhJ7nuxnx9k8GInEh90rkXt5sq9mniBH5jLPxGDEBMvOAWUWU5nsUSxpo0zWTkvm4pzJxoKMi2PG/d32gvVqN1ivhsiZ5uuWJ7K3joxttqdiUlnHrIUfYNmrhRBCCCGEEEKIP2h0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVg+mv2ae5JMTtwXzpJEYiJUsiQ5PcwSrklOu3HEQuBHkpRNkogzEWLknpg0HEpyCkkGL4UIm9z0XWxgieD4boYIQuJAxAQtESdFvF/VYpvUFTFitUQK1ZH+GqdlI5FflYhWikiy1Fkfkrz6ZUFkVyOJ69peQ1llnuPtKjSWuZkAi4ShcZ5IY2oiCGmvoKw0RKZEBllVYeJ/6vAZkQjLXDNPzEcJR+7I+CIJ/ZVDc1RdEylBhzHsHMZds0JTgSVyoULi35Xt5OchYMCWgjGSI5EtOCInI2N4aZQR57K+J4IyJk8J2JeBrAHHYSawGPGaSGQVTcD4PHmMqUTEHMFuoaw0RBbH1goiGoszIYgjEio2BjyLKbLu5J5IZ3oimCGii0CkS57ImUxP7CJ5+s6ZtFGVSF3Z2skmfCJSWRI245wdmfBtJBM3m3tIf9tx2n4+kb6piHCGxUlNzCzx00Q6ZDiZRMWYH5dWsr+IlMLEhjjWKyIA9EQSZcicQBxmpvEopilEumb3bOxMy0ayP01E4EQcUeZyJOMkLv9vRzFiu8SezFukKplMn9ZiO8w9oIWIQTNZ360h70HsRJbtXYlgis1GzhHJIJGe2vk7Z6ynNxjb7Jn1CvdyZOk0weP9QsL9UkXOAa4hbUeWFDuT5UWyJmbSlqS7TLXC902eVOwDLH+0CCGEEEIIIYT4g0WHViGEEEIIIYQQi0WHViGEEEIIIYQQi+WjOa1xwH9wvgT8R+Wq4P98j+R/0kk6h6nd9P++SZoCfVHyHdomkP8hHwp+2tfkS97Jgz35RmKW0xhnOR7dQP5/nvyfeSG5MS5jfoQj3yHvW5K8YfD/1jPJe4rDA5SFERs0znJhWPpRcdi+LB6yIfkcJM9gSSTyf/oV+cZk61j+FnaaHzCv0czykG0m+TwW7z9GzNtoSDx1HT5z2GDsxA5jPY6Yz8ViNo3THNaR5GqHgnmuJuB7sM+2J5IbssL3CGUHZYWMxT5i/FeJvMs8N5vkbUbyZeE+szwbkpeeSQLJwkgsXzFgm9Yk76dknBtGkgNcZvmfOeAzA37M9HRl+LQ8YZaDM3YsNxMfXJF65TRd7xLJeTKFzAkOx3EiH92Q+TOuMGe4Ifnv+EljxojjsSXvMl/HExsDZE70iSwWgSzuC89pZfmrZAtkPMk5dWQ/Ykns+FlR8djGnuxjxoRze6jI+lRj/nYuZF2I+Iy5c8EYYxJJVMyzskjyyL0h6x8Zw+P4hFeRfYzdsBxpHBM9WXe77h7KalL/bjZkmZtjIPHgyN6xsFx9kvu+NFLCtu8c1i/YT4ufnox5b8Z5Ad6LFJJ0W+PJuyWyvy9kr8HyXFlfsjTxucomO+LYmQ92Y0zlyZ6KzNqBOHtYRmxbYw63JWvq5UJylRuce6p5IjcZA5kkNFsyBzCNjWfJrx9Af2kVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYdGgVQgghhBBCCLFYPipiGkgCcmCSACJxqTEf3nSRCFXW0/sF8gXn5DvqzYoIElhKe0OShmPCqheSvB2JKCiSL3l3MyFCsPjFwIMhIqaC9xpJsnkpKMRxZ/JuDZa15EvEmYQkBvLF3zPBUEekMxVJwB5JuvW8jYwxxjomk1oOFfm29UzEU+wLwjuDfVsR0UOefdF2IIn17Eu7ayLdcissMxcUyTiHCfIjmw2IRCESqcV8SvAGBywTs1XkXiMRFZyINKS5kBhm8d/gu8SeVNa3UDSf685EQOKY9IGUMTlTIrG0NDKZP4wngr4DXtZ4nPOeHvHCFKbXhQOKWDyRVXgi16g8xnvX4fyZLb7H2JEvfifV7yM+o5mtFXHE8Z+ImMKS68YrFIp15AvYWUwxQUpdky90Z2OvwrFiZ+2eLN7fstgm8jg7l60YYxzp1yVRk/1OtESgSJxqQ0NETB0RVM66xw9kniH2p5BwziqeyFrIvssxWSKZZ6l8kewL/Wzd8uSDiUgszUBkf6R9i8dxYg54XbtBwVggwr+4JwYfMnasn362G0nfkFt1nkhoiJwtm2XvgYz5wH6OXRjJdaR6kcSPaaZxW/Ukxipy9iBvUogY05A5yhrc8xYykJlTz0YyH8+e60kcW7K/Y3NlsGTOJmcZJo5KZP6oPdkvkf18IPN7amf1IPFeSPtm0jdjImI3Mu4+hP7SKoQQQgghhBBisejQKoQQQgghhBBisejQKoQQQgghhBBisejQKoQQQgghhBBisXzUArKaJ+AaY3qSNBxbTPQ/kyTkakU+m6ef9eT+FUmsvgxMnITvMY4sOZwkeZNk6zHvocxaTNQeZuf/y4WIREjysicZzaXcQVkmAhhPxEmuxjYnOdkmDURYsyMXjjO5AkmgjwNJhB+xjSLp1zQsW8LRbH8EZYWIEzwREDCBS2aqsLKe/MjaxJIk+nnSvzHG9ESuUrk1lI3jI5S5C74b8VcYQ4QgKU5jpwQU8NQZ5TK+JvNBwXeLEWOzWaE0pl3hlBbJ+E8F43/zHMtSNx1jocM2GljfZ6xXJvE/puVLOD672kJZd8L+tTdkjlpjnYcnlIXlmaQrrp7DNa4iAocOpSv9BcdAIMKrZPG6YcB6ORJ7bm7OMcbEMo2VnoieXMSx03scn/aC605PFqiq4BpQtXg/Q+Q/hRhSMhOklOl1xRCZjMX3SES8Fgo+k73HkmgajMWQyPgm86Jh0j5y3VwxVEg7GSKeNJlI+y649nqy1UtEWBYS6duayDNJHcbZPNgTwVKIOG7YGpM8xn9KpF6O3K/FuDvisDaF2JPsDuuVZ/MJ67/DmVi4qFsG+2EcyHhdGHWFsTeS2MvkRJGYRdIRiep8j8sMYMzj1eM8Xhx+1hL5qq/JmkLOAYWcKzJZ97uZyMwVfGZbYcx6rIIJbK9oiUCTNG+pUVo2kH1Q1+F1/gXu09xMeGgr0r5MdEW6MJN193fwMOkvrUIIIYQQQgghlosOrUIIIYQQQgghFosOrUIIIYQQQgghFosOrUIIIYQQQgghFstHRUyRJc2SZPoLkYwUIvvJTKgye414RkHAbx+JnMZg8nvwWCWWcJ+OUGSSw8IVy6/PKBhwbvrOXXwN18SEsopNjUnPayJx6Q2RJmQik4koTTlUD/guESvm3A2UVdVUmjKesY3ciGWWWAi8x3pZT5LNF0RYkRjusc/qDcadI8KuQto9p2l8poqInsoVlFkSE7VtoSxW2MZrizKcUOH9XCbPIPc7N9M6+D1+zpDk/TagTMkTg1UmdW0N2jWqI/ZNt/kOyi5ETLMbMf7XzYvJz0NC8c9qROlan4ngoCbjf1x2/P87cNw2hghQiKCkaYnwLRIJh5nG7dmRdYf8irVuMS4OTBaX7/HDFY4VZ3FuTyOZA/wPUFZm9aqIgKRYvJct2G7E32MskyRZrL+P+OFLhc+tyTirHbaJr6b36xLGtiViMxY33mPf+0AEgAvC1aRuF2w7S/Yenu09SDyZmcRk6LGNs8G5zXpcT2yHc2qsyRp9Ie/R4nNXRLSYK5x78yy2q4TzoiHSsU2DZYEIxjoydjZkLXLETFNfodzv0OF4Wjlsu7CbjomnPda9YeFv8d1ShfWyRDC3NAYWsxbXgMTsU2SfWshcVmbzhU/Yfpczfi6PGLMlk3Ym60cciFCowjFVVTi2AzGrzve9GRRrxmRy7HINxl1N5JtnIjj1huyrCs7jXfkeyp4ecV28vv0zKKvm4tqIfdolsucn81+xnxYjH0J/aRVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVh0aBVCCCGEEEIIsVg+KmLyBS8ZVphI+8xgwu14jdKFmxN+Nq2mZ+f7AZOXj48oQLEk0T1co0ylpknfJEm+32NZIUKZeAdFPk/vlzuUEATyHjnjdTuS4N0bTFS+qTF5+53FthsfiezlgqKTzQ7f71Cm1xUmpqowIb+NKKowAQUG3bBsEU3qiQAsYduZBq8bRyxjzoUwS/K3I/ZDN2D8O9bX5N18wk47kWfUCfvHe3yG6w9QZmYinZEIhpoeZRhDwDH3DJvNHEj7Pisok/p1i+0UHzE+yxHf5aVHecN+mNbVnvF9B2IIujI4D6WCkp99QonC0lhV2PbDmokTcK3A6DEmEPFK46bx6BzGT2+J+ILIVHyL1w3pOfksjpVMxHAh47ggjhATZhIj4q8xjgimksE1wBLZoSPzbCASpy7gdfmMz7VEjNfU+Nk4EyWVjJIP4n8zOyLrIp4n4zyZTxdEJPO4MUQURuJ/JOO7cbg2zsO4kPsPRHhYzu+hzDoUNhlPZDgR360mkqF8jeMpn3Auq8t0P9L3RFrn8P4jsWKuyX5yT8R7NwPugV7viPzlB7IHPOHstCPrzNNMnpgPuJ7GT5ThBDL+L2n5IiZnyTmA7L8daQfXYFkg46LM9r3DkUgrI7Yf0fqYTIZsZh47IjKrItbLkjpkcjZoZnNvSwROYyYWWPK+gWyha7IWf1bhPP5I1grzFmWedYPC2G1NxFZu+tyRCHWZeLS2uKdi+9Yuf/oY0F9ahRBCCCGEEEIsFh1ahRBCCCGEEEIsFh1ahRBCCCGEEEIsFh1ahRBCCCGEEEIslo+KmFYkAbmqiVzgCROEX36Ft3+2w8Tc9GaamBxJAvZQ8Hx9ITIAN2DZm4ifvd5soSxnFAfUEZOtK+JO6ftp8vYVayPyO4IVkSm9avC6W4v3+77GOtzuMcl7/4xIGE743O36cygzp2mbHGtsozRiErUnCe7sNyRjs2wJwVySZIwxxWJCuz0R0QkRFfga22X+hDqgqGZP4ronsVkKEdiMKLAwBvv/klCw5EnCfUvEQ2k2/NeBSF4smhDWHtv35Q4lAn+ywnnj2zXG//N3KLU5fQFFxm5QVvLZF/8Iyq4ep/erKuzneCZ1bbCu9z2OwxyY0GhZ1A32kaVSOZyjLPtsS4wY/bRdG3IvQ8ouRM5kiMjORhRoWYMxlTORUJD1oyFjIM8sdWtPBCQeYzb1RC7iidgts/hB2UtF5oW+IuuuJTOyxef6eTtV+B6VI/uEQLYXRAjkVsQ4siAqR2LdEtlVxrndO7yuCdjfczGNb67hmvsBpUtjIjFscB6PZO4pI8ZiR8R7+Z4IVgL292Cmccd6tSJCyS2xE14TC43PuGadb3AeXz/gGjA+x/XTb7H+q9tv8Lr99H7ZvYFr6guOm7bCOiQimOqI+GZpsINCLETGcyL73uc4fpi0J/fTsthjnxki8iojGWNkHnsq2B/rmoiSyLxdEYvT2pC5d1avlux5GoN7dLfHeyWyb1/1+L6JzDEVEUYeArbn569eQNmtxXHxeJn2taXSJWzzksjcSSS4sXz6GNBfWoUQQgghhBBCLBYdWoUQQgghhBBCLBYdWoUQQgghhBBCLBYdWoUQQgghhBBCLJaPiphqYh3akQTs8fEeyja/QqHM6gtMpv833evJz/d7TKS/v/wWyroO360kTF62Cc/m/z97b9Jry3Ke6UWX3Vprt6e7HXnZiJJcpSqWSh7YKMAwbBjwyBP/RgMee1Sz8kgoeCBAVRIpkhLJ2552t6vJNiI8sCeZ73twLkdM4r7PbAdyZWZEfNHtvb9nTeMWyswW3/epJ2KbBpOQi0Xu8tVbbNoXz15A2bOrH0BZ3uD7/uM3v4My/+U9lI3hp1A2DK+grNpi0vQBnQ7GxHldqxbbaDTYHpkkoFuHbVkQIdCaKCpMkM8Z226aMMn9jEiGtuR+9918PI0Tijq6AcfXMGFC/0i8Pi7ie6SMsg6WIO+JOGUqmHhoXoeKSGOuL59A2Q+uPoGyAt0a5l+++S2+xz9j/W3976HMv/4Syq6eEGHEGcas93N5w/krFKSciFgoESFWaXG8EufD6igcxntPhCptj/NARYQN11sUYtwtPntqcY1JE7bp4YQCh9MRJRTTgGPKE7mdaci6MOJYiYmM7e18XNgR2y04rLslQjEmLDoOKMGzJ6zXkHFOtQWJ7S1eN3ksK8y8HrYk88SE9XKBzPdM7Nh8cBvyR8U5Ij8hmqE8oShpQ6SK1wHj4qWZ9yNx4pku4jo+EUHfSMar67BfP1ofuQAAIABJREFUk7vBsgHHXS6wH40ncp1FWTXi2L88R4nhxxdPocySNeYX36IAafMNGdf+J3i/m5dQVpI9kK3IGLuct8nVm0u4pm3wPRwZw4nJ2VoU6ayNQPaGVUeEOiNK4Nwj1tltUKh0bOefdQPG2DDhGtpl3JPmQIR6Hbbz4yWu51VFpHUG6zUWRNxq53NF0xIhaYlxV52ROB7wuvvxDZQ9/hrr2pE15TTh+159jJstd439enk/HwMtOTpOA8Z7NFgv9qfSwrPDB0d/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVo+aEDYv/xnKPvnl3jWjd0/Qdnf/QYThM8uMDF3f5wnVx8xR9tkIn9iUotE5DcuYeK8JxKncotSmD0RtnxsUChzaeeCnYufYwL2JuG9UsTr7r/6Fsq+2RNxTImJ5ZP7CsoiSY4fBuzD01tifyjm7Z6ZYCZgwnwk/WVJuDmD162J6QEFYG2Lyes5Y5J7CCRZ3WBbTd1cPPbQExlGIFIHIs0wbEwk7GtXEiFKhfHPpCsXFT7js8u56OPqBQoOGvIe9YD3/+oVCjde36HAalsTEdvm91D2cESJVX9EUcGzb4l0bSGOaQPOX0Um4qyeSCoyPtMZMuZWxjRhnPV7ImdJOHHfH3FcpBPKY7r9/LP3Ee+ViaGrn95CWewxPqcB5UyJiOGcJ7/HZdKlDcZjU8/XBV/dwTXjAcf/aLA9jEMhVGFxTPURZWSJ3Y/ICNOI67MlJre0mMdCxroTd4kJHvshkPFjiThoTcT+Gyg7PWD/TCwWT1i273A97o/zftyfcF9giSjPJiITsrgvSCOOJ0fiv9x9CmW5InsZgx3+yXYeny9+jvukhsRmReaSf3mD+5j2hM+sGywrLnGsD3uM/4FI4tp3ZN+yGOsng+PGZzLmiIMmVNg3hcf7rY2RCL8ORyKfGnDOO3U4p/gO562b+7lkyI0YF8TvaEyBkjGXcK4cetwv2AHbfrzE+/UbjIunZA9dLERMpy2Ou6bEuSOTffCJjItHXD5MT0R2t/d4htjvcR+ULZ6N6vwRlBWL+iciNswTjidDhKxFgXNWImLH96G/tAohhBBCCCGEWC06tAohhBBCCCGEWC06tAohhBBCCCGEWC06tAohhBBCCCGEWC0fFDF9ub+Bsv0DJluX1T2WZUw6f1ahxOGHLz6f/fzLWyLvOKK8YGMxCb+tn0NZf0LZSbFDKcqnT6HIlPULKDsesf7BzxOuJ4N1uOux7OVXmNB8njB5+SnJ83/cbqBsGPEZ+YgJ3SFj1/sdET3YRQI2CRniGzJlxmTrscKk97rFfl0Th4QJ8qnHWPc11m3IKAM4r7CxPn7xo9nPrwaM1/YWxQV+h2OpLzGu4ztMkM9EwrEtMAG/3OJzMxGK2XE+FkPA+799h3KAwy1KCbZEEtUEUtctil5O9gHKRiKM2E4oJXj49A2UXcR5PR4t3qv0KDNwAePGOezXmkiD1saB2C/eZizbEslYIjKSd6RtyoVALHiM2TRhLBYWf++aGxRpRCK8cxnb3iWcj5otEX0QIV2K8/o7h3NCihgDMeE8ngLOn1u2XG+xzZncrh+wv3LGcWZqXLNDNR+jEzHMMI3GpsT12QXsL+tJkKyI+4h9MY3YjzYQqcuI/VM2WPbDn382+/n373DfFb8k+xiDMTx+/CMoS29xz8L+YvH07GdQ5l+gGK99g7FYL6ZoP+Jc/HqPUqs3X+EacEHiZEMkg5FIaB46YvI8kD3bhJ+NzzEW0zTffLktji8/oCTOTmSOqHGklKd174GMMeZAxDsnIlMKRCoVyBpXF0SseD7fa/d3uJdl62oyROyzwzV5IHsok/AZvsfN9vkGDwc9mfS8me8rxhbHQHvCNtoTWVUg4rXhnhwELjD2To7IUYlksH+L+6C7T1BkVh3m75eZBJRIcENJ1iciaS3I3uF96C+tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWywdFTNMdJuVmYt6ZAiYDlyUmHLdXn0KZ281FTP/2U0w2Tj0m7z7Yd1D26r+iTCVHlBqYE97vFqtqKszTNkWB9W/NPLH87mv8YEhXUJYaFATc3eG7HS4xUfk8oegnJJSGtBeYCF+S31cUEZ9RhPlnU0MSxkkO9Wix0BNBSHIo/lgTuSWxaFCcYonYyjlsq+nJEyirrv989vNfbFF6UCWUmuwbFAvc/wOWPY5fQFl2KBbwl5j4bwYsK3fnUHb2bB53xwllVdH9CMq2L7CuTMAwfYbvsXvAOpzeEUEKkV/1JZaNr7G/ho/m9bok7pqCiAXMButQdigDOllyw5VBXGSGeIJMO2KbNks7izGmr7Avu81cxnLdokwlnOM8dhxw0n75NcolzIDzbMwoLbMZ+21PhCPVFhugSvN67U9Yh0SkSwWR9USDz0SlhzGBSKdMhzFVb3CeLR3ORfUW1/Fimq8VTcB7FQ7HsS/wPVKB7eaIYGdVnMga4FEeFArsC0uEKP4K563JzEVM/+HPPoFrnv3Zx1D2zTXOd9/+LcqOHizG09kZPsN9in02jThvbV9gXXfP521yOhIp3vSv8F6foYjoSNbd7Sc4v1zeYyzeHnH89zvci/oS61AMRFC5FP4RySBjKPC6nMl6xyxmK8MmUmdHZJsTiZ+EfRlH3M9M5Vyi+uIJfm5q8P7DCd9tf4fjwhHpUmkwfiYijjrdv4Kyioz35WI53JB1JxIpWIPrQnzEWBknLHMW11hL5HEuXENZRcaFPeLZoOvn81ia8P67KyYFJKLETNa7P2AQ6C+tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWywdzWltHctMq/J/vdMD/q96PmG/Uv8L/Db9YJIqF6SO45q9fYP7m/oj/3+4uMJ8jO3w332Oext0rzIVwBV735MlzKHuo5//L/niL/yv+2VP8H/3jFvNc7RFzdf05/v/8ocNnnEi9KvLF8vUT/H3FOJIcjMWXfHvyhfQkjcHEiO+RSrwwkpyfNZFI7l6qyDt3+GXW+w7jKd9hDNh63t+hxvj6eYk5IG/2mLfRbvGZw4B5G88N5kY8nLCyLQ47c/Ec71ct8irePeL7PtnidPMV+YJ3R/IKtxXeL+3IF223mJO3GbC/XlzjmIgG57rs5/m7qSV5rxZzF4cj1iE2JLenX/8Xy6cSx3I4x1hp7rGdxxHz2jKZj3Kcz2WtxX78iKw7b1qS/zx9DWXWYzu7SGIg45xqSH5+WWF+kK3mOX3Z4LttSLsdW1yfMqsXcweccBzHRPItyby925Kc+3v0P3g7z7eqnuKYrSfsm0zmmJHk79b1utcAc0FynwP2Wdhjjmjs/wHK7u4wFh/u/3H28/7q38I1//Nf4WScI+bHfu3/M74HySPLA64f4xvMt0tbHBOfXuIasN/P90C3t7jH+OwSy752mM83HDD+r8ga0F3gniWNZG6acEycn+O7nAacy0s7vy5P+MzKkty9CWUAscIxkcn8sjbyGXosCot1mY4kV7XHOaW9wRzraOZ7lxhwH/TEEGdLizF7OKHbJo0Yx4PF+8UO+8MmnN98QdbuaV72sMc1/7wkOaMex0BN5uxjje1ryT6znXBu34xY1pTYr92rL6FsX87f7+IS26MdcJ6sGmzLjoyfOhBxxnvQX1qFEEIIIYQQQqwWHVqFEEIIIYQQQqwWHVqFEEIIIYQQQqwWHVqFEEIIIYQQQqyWD4qY0iPKlEyPSfjuDJNwixITqTf2b6As5pezn3cVCgfuibDnfIfv8dUDJghfP0FJTjpiIvh4xLqGDu83jJhc3d3Nk9Jjj8nG/fhTKPukIsncP8WE8dc3KBeYRkzADiN2aUm+WP68QQFUJF+YXi4EU2OPYpUpEskDEzENRFaQVv7N2h32j42YIO+vP4eywqBMZjv8ayib7ufysC2RHvQ77P+rGuPwi19iX+yePoGyymKcFCcUJpQHTJDvSnzur47z+B/eoHCt+RTH/vUFtu/ljzE2b/bY5rfTV/iMinzp/VOM/x+/eAZlQ49z2PbJPD67M2yjx9c41tsR55eBmHRSsfL4N8a8fINyu2gxRqdExoUjYqyWyBnMPKY2O5x3OyI2sgPGTxpw7iktEVhkHFMHcr8i4LyVehS2nBZz2fGE46S+fAFl1xuM94PFup7e4TNHQyRZZGynjHF2OpI15YSfbRfhfdijhOjyBQqBthHbvMSPGuIHWRXj22+hLCYiz7rEecBY3Mtc2T+Hsr6cy8Oenv0ArrlD741pNhgT+R77dbslwpUBx2toUWIWBrxff44vs389j9l0JGtH/RMou67e4jNfEOnShGXjHufjHRFDNmekH65wv3MiUrSqnu8Ljx0RjJ5wHE4T9o3JZA/kSdysjHjEQeqJjKzY4fwRC+yQ8pGIvOK8L4PHdaI9YaxMpD/SgOtvHsk8bvGztiDHooyxPI5Yr1O7eC6ZA4sN7scuyRowNBjb8RbXhaLAsVh25O+RgYjx8CpzOGDbdXm+LuaI73v2gpxlDjh3+IYIEP+ANUB/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVo+KGKqGxSWlFd/gRc++wyKqtMdljWY+nt89eXs5/tHlP2U/xrf49kNJiXvtlilyWHZvkDpSiAikWwxAdsdyfv5+f2uGxQfXPwZvm8VUDhib1Em00X87F2PwqaaCEISkaaYhG3iHSZS52kuE+l7bCNnUCRgMYfeOIvZ1jmu+/cmZ+coTim2P4cy9+knUGZfv4Gy3RbbeH/zz7OfH094zc1f4/iqX6EgpGnuoWwM2Gdfn2EH2beYNH9hsM/KHuuVHudxXAS8V/FTFAGcE0FOlXBMtA84l7y7RylDNaAcIkYUsd2TeSgUeL/Tu3ezn29viUSgw7acBhybpiLX9WSgrIyNxfno5Mk8UKFcoyDzpxlQIOdP85i6vUPJVnnCmKo6lB0VBcbA5DDO2hHn8WDxGWHCepkRYzl089i4jLjG1CX2d1VirMQjvm+/wdfoT/huOTN5HEpnwjV+drfBvjaLNi5r7HvWz6HGNYauFWbdY2B7gVIkU19DUQg/hLLNFtfj4hznnv7r+Tzz6lf/Dz6yxj3QJZnvLInhA5E4pQL7rB6w/1OPYpaiIKI9M3+X0uO7na5xHq87fN98JPuCFqU5OWKsM7flQOSJxxrrOk34jHaar6n9A7HGOIzrNJH9ZMA290ToszZqsgZMZNxasp+tN0RmWuC4sI/zOS+dcC/jS4yVpsR5bGxQdtRZjOOYsQ7OYn9MR7KeE1HUxUL4VTNh2wuMlTpgTIVHfGZT4WcnIskKBVmLJ1wDGiKkfLJFaVn/OJ+zsidSq4SxnSsmXyXiVvfdx8C6TwxCCCGEEEIIIb7X6NAqhBBCCCGEEGK16NAqhBBCCCGEEGK16NAqhBBCCCGEEGK1fFDE9OwvUQCTHvCsG2/eQdlQY7K1mZg4Y56obe9ewjU3/wXlL905JhG7HpONXSaiiz0meTuSXHzu8H6HB6xDLvazn/sKk6PTL/D+X99+g9eRz+Y9JmC7iEnem4BJzn2PcoFmQKvHpsSyUz+v64SvYZIlv/vIRPRkSKJ2JqKWFfH05xj/5QnbaXyF8d+WGCdlQZLrFxKv/A4lNF/8LZbV5xibfYvyl2siRbr9EsdmeQZFph6x8Msvv4SyfiH6KIh0yP7i76Hsv94RGc4W39c9kLmkQ1mNS3i/44D94Ddo6/jhE+zrw+N8XIcB360jkhOfcExMPZkTiAhibTy7QgnHzSPWZbxBwUx3tYcy26PEJbXzfguvbuCaww7nlIJIIxoiWOl7fI/e4lhMEeejiUgi/CP222O1kJEVKCAp9lh2/4h1HTJKQ+yIMesyjoGCWPCmEuuw26E87vOPsaw/zMfeSNqo3BCxYSayDiIv8UTQtyY+/gnZA3W4Bjy8wT1F2+DkMLz9FZTlaT523OG3cM1X/4wCs/HZT6Dso+YplE0bjPXbAWOsNnhdGHCMHd/iZw+LUDwP2P+73+I6+c09ljkinLEjzi92JOIfLDID2QO6Cd+vNLimTu18/cgk/nPCdSFGMre3ZO+88vg3xpiyJvUjbqKRyIO6HUpEU4vSQz/M+3xjcC0fLQqGLq4/hrKtw3H3cE5kUkRmOhDxWH8ge9wJ43G/uCwXGMcXL3HO/vb0BZQdSFyMj/huhsyzVzWWnci5Inpsz8sNSube5fncE1uyb5+I1IoMgUTmhZS/+xjQX1qFEEIIIYQQQqwWHVqFEEIIIYQQQqwWHVqFEEIIIYQQQqwWHVqFEEIIIYQQQqyWD4qYTvcoMdnsUWrx0GKC8CVJ8u0dlnULOVOMmOBcEenQkVxniKyimzAR3HWYgH0yKKsYLSkjz8gLG8upx2c+7O/wXgMKZjYlJr3bgKIL26AMgoldcsb3fbhDOdOwIUaZNL9f8mg5sAW2UR4xUTsVGG5VhXVdE90txrrb/wbKbo943UcF9tmQUGrx4Oftnkfsh407h7IcMKH/nMjEDhO+W7XBuDucsC+ORJ41djgnjGk+FmPCvv7mN/jMwRBRQcZ2cwHHfyZiHnfE+m8tkaHc4ph4V6HsrXJXs5/HSyKhwaFu/NJMZYyxG5QNXPwJ/Nqwn7AuTcK5LJG+tESgdbNH+Vx8mMePJXKJrcUYGIlkiwmALJGiuIjX9UQIEYmwpU1Y1zzO18CixmtaIrIz5Jm1xfHuNjh3OCKA6RMJSIuCmWpCydqQ8X7bxTpjiWCKtZGzWNlIJFnVdt0yvv0B27M6oCSmP+Gaagbcozzsca/kFvNlR/YiTYdtfHOP94ojvm90uAbkWxzXd2ScVMSmchrJutjP22Qk69/9EeNrIEKoTYlrWyhx7+jJGpgMPsM77IfjAz53KnBdsHYes7YhokCPY9gNZC9W4nVb/8Ft+B+dicSFnbCtxoG0HxkrE9m7n/r5Zw8Rx0BwuOfNj2RfMZB4T7jnHyciViTyoEAkpS2Re9lFLHsyBsYJ17ETEUztCny38CMU5ZHjCF0/N6T+3Q3O0d/E11Dmp3ncZrIGlORsYAyJG7Lncf67nwP+BLZMQgghhBBCCCG+r+jQKoQQQgghhBBitejQKoQQQgghhBBitejQKoQQQgghhBBitXw4A/zhd1B0//gUyvyWJPCnH0JZ3GGSb/vF29nPmbwWk/hsT5hYvB9RhuAcCifGAj/rTyiEGBITHWASuV0ImxLm25thxIRx6/D3BvnyEso+/+zfQ9l9e4MPOWL9Se6/SUSS0Q9YVi6bnQlISGK5ZzKEhHWNREKyJqb7L6Ds5v4ZlNkSY2I8fA5lw0cYA6dfvZwXeJRL9BZlBlc9PvNxfAtlbsRx2J3uoSzdY59FQyRGxCYDr0zy6sdI+jqjgMD9GKUxnz/7Gyh7+OK3UHbqcLz6gHUoifwitjj+czOva0PkEG3Ge20cSgmGFj87jSgSWhvFCd/x4R1e5z2uAeOIIopNhfH9brid/VyRa2KJciLb4zz+2GGcBSIUw8nNGN9hbKeM8ROIeGWK86CPREZnyXgqCpw/N1fXUPbsGY7jmwOOY9ffQtmQMPaKSyIym/C6Ic3r4YhcKNVYh4qIGKtA+oGsgWuiPOE6e/MOxSxFg/UN+SO87mNsv3ffzNcAT6RD0WPbdSeUx52IiKuyZG8zsI0Bxv8QcO0xZC8TF+t7yijDYRuj5d7JGGOmAteA609/BmVtj33jEs5DE9lnODInsH2bX4ioStIPU8T5viACqxRxXhuIKHNtVJnET4uLfFWRfbrDMeAucL744vSrRQl2hidyIkNEi6cJ47gmf6PLI9vP4nWjJfFDzgZ2YRmKlsQAkUSFDcbP7vkLKPv0IxwDrx/JmYfNCz22eUlEqHnA8RjD/Do7kb2Sw88xEVPO5P7kfu9j3auFEEIIIYQQQojvNTq0CiGEEEIIIYRYLTq0CiGEEEIIIYRYLTq0CiGEEEIIIYRYLR8UMT32mORsiOylKTHBvK3xur958hmUffFX82TdvEdRwfEM5UTnJGH67uU3UJaJFMV8/AkUudeY1P/6FpOcD4YkYC+akjhJTCA55KEiSd/nKPr57AWKCT6asOz3r7+EsnjEbmbJ0CGgNGXZxJZIDsoC6+ANxkN2REySSUOtiANJ1LcJRSeuwLrtm1dQ9j8++XMo+/rfzYVNNw8ouejKBso+K1GIdvMtvkdvUaTzw49/DGXtDq97/QrHcLsjcoHFY2sSJ2ck6X/YoOCgNxiHf3mBYhLzlyim+cffvISywwGlBJ7Ma7sSx8TdsJDQBIyHYsC+yRbHRPB4f1+uO/6NMebxhGKXSGIqbLF/64Rl/+oHKNX6bTk3O9l7fGZ5hvKbfYmSmIasatZg29ekL/s93u90xD5yBdbfx3kc+BrjoggXULYpsF6WrAHVBZHTEGFPSldQVhLPRQ5k7XFEJhTmZSmj/MqQ9o0FifeM8eAy9sOaGC0RSlU4b9UXuOb5Gvce/+7z/wBlv2wWc9QDzk/VMxSRRSJOuvvmDspCxlisf4pz5f2XuLadyPg3FgeZXcSJpZJFIiwiks28xf3Z+RmKZM4qHCfHDuVkRUf2gESoVBW49vQLgWAiMjVLJDQ2k30RkTM1ROC1NmLEtnI4HZndDuPMTti/P3qBsdzkJ7OfMxEWbTf40FMg8ZnI+DH4bsNTFG2Or/B+0xHnvKrBWAmL+T3gI40hMqnzgPXy9XMos0ROtfUYZy2J4yZi7MWI61gme/flG3synKIje2WyvWF7/swkne9Bf2kVQgghhBBCCLFadGgVQgghhBBCCLFadGgVQgghhBBCCLFadGgVQgghhBBCCLFaPihiiidMkE0kZ3Z8ifKksUAp0t9VmOTcD/OkYRvxGntAIZLx+Ex/jgnNZx4Tnw8bTOBPDSYIV+coxUktNptfiGLSHu9lifgieNIFIyZH/9PLX0JZ+RTrsCOCiOICs8FPbzERvmNZ0wvBhiNJ3y6QgBhQTDARgYF3xE61IsZ7TC5PE2kn6iZB0cV/vkK5QJvmfeFP2E7Et2Hu+l9h4RZlFT8MKHp4uPwRlDXmF1B2TsQpPfldlzufZ+a7LzAmmgbv5Vscw+aAY+6//Pb/hrLjsx9AWfYov3ryHMfY8St87on4MKo8F1B0CdvX1Nj544Qyg5hJv648/o0xJhsi3ooYA/sjtmk9oLHhHj1Bxp3PJRxliW01JGxnMh0ZTwpH0s4u4rwYiOylthiPVYMSNGPnsVJ5lA4aImxjsp6aCJb6HsfUbkskNqRvykDWmUhkfGStTItQzkw8RkRPpsdJ61TjM4sJy9ZEmrB/ksV2OrUYsxt0Apl3lzg3lBcfzX62DoVII3lmQdai890ZlHmytrfmZ1B2rH8DZYE8w1myLi5eL5E9QHIYm5bEuutRJvX6De6LTIPj9QlZ70rcKpnpQOZo8n5hIdXLGT8XiNTNDDheM2k3m4nVZmUMRDRFvJrmRPr8gsxHA2nDs0/nZUVH+sfiQ69aMqcSweNIDH2PEYWUm3M8t3QG921XizXLGGN8Ne/LxuNc2UWMAV8wgRWud+/e4BobiQjVkdhzRIxnBzwHjKSv40Ke5AacE3xBnklkZJmI9yIRd74P/aVVCCGEEEIIIcRq0aFVCCGEEEIIIcRq0aFVCCGEEEIIIcRq0aFVCCGEEEIIIcRq+bCIacLkf2swaZgdf3si3nn15Rso83medF9WeP/tDpPVpwIFGWOHycBdeQFl5T1KcoYerQmXDSZbbws0idSL5P/8KQoCtrvn+Mz4AGVvHvHd7r9EOZXbo5jgMqH8Y7u9hLLH9gbK2oRlIc3bLlRYLzuR5HDb4XUjiaWK2G9WxMQkIUQskEny+om0wfFXOCZcmsdAucFhuStRMNZmTNS/u0UpwZtn+G4vwhdQVmSMp80O++eZw3dp8jzGps8xXs8+QnHSmFFw8Pt/wXf79T9iu40G5WwXW5SQuAuUDeSA7/f4iP0V3XwuGokvILbYbtEcoKy0RJiQyVy6MgpPpBlXZD4mwpawgyJzPGKf+ziP5WnEeZzpeoqwgbLU4BgYiUzHBLxj5ci83WBMsXexxXys7Dy+m8Uik0ciyHDYlsThZLoO18AiEzlTwPpbIsToBiK76ef9X1YYxwWRgTA7UyQDKJTM4rQezrfXUDYRqRxTibgL7Nub+7fks/M11CfsV5+IsIvIZSwz5JA1qyCyo+0O37epP4Wy0eD8aY7zz/Y1EVuSvd004dyeLZHWdUTilHDOnkocr6XBOaz3ZP/Y437M5fmcwERMYyTvy+yJEfvGfXgb/kfnosS274gsLxJBT9jiXH58JH3u5u01EnEhWOGMMY7sW6YK90bW4D61PGHZ1QYn6fIa9zwmkbGyiO9MDkZNxmf2I77vNOB11mKsxBHjOJDrTMS93BBxDphI3NrF+LEe75WJ/Gkia0zyRNxKhE3vQ39pFUIIIYQQQgixWnRoFUIIIYQQQgixWnRoFUIIIYQQQgixWnRoFUIIIYQQQgixWj6YAe7QN2CsJeKICo0bTcCE2xwx4TjG+f12BT7UB3zVukIxge/xmeP0DsraRzyvTz0+d/AoZ8ojJvrvdvME7NahcOByh0nkU4PiJNehEOkNkZzYFhOwY40CmPvDLZQdHki9euybfiETcoEIcYj8piAikeAxbtq8bhETjX8iprFXGP9nFtugPxFxhJknqxcNEfYYkqi/Q2lMeYuCjLbDz779PfZ/d4vvm4gQpz5HKUF/OZchbJpncM0nRCZ1usD4H7/8FspeETlAMmR+MSi6uHsk4hM285G+GRZx3PU4vogPyzgiQ5kC9msi8bU2CiLhsESc4Ml1TSBSJCLECAshVaqITGjCGDhFIqsgcitl19chAAAgAElEQVTbs/ghcyqJn6LGdyFLm7Fh/i5TRlFYPRIhlME5sCIPiBnjszQYQKUlkqAR4/Z4wrmiYBPeOB/b3YDXVFucizx5DxbuI+nXNeHYWkauCw5LC4d1y2QuCwspmHdsXSRiqwmlZg1Zn/oR+zoP+B7jgGu0D6R/TmTtKefXdQ7fI1uM612D64IlqrOYMXqiJ21iUczzOOBY7Fpsk9Thc5fdmiKunY7M7YUjkwQZABNdjNbFSPqtjDguSo91KT2JdyJp9W4+N1oivPIO5/vjiPc6P0eZUnci+9sC+3vosF5ui/17nnEuD4sxuidCMeL/M03A8R4t1nXsMbYDizNyXohEennYo3jMk/NdNPPxHsic6Arse3YvQ/ZGkYmj3oP+0iqEEEIIIYQQYrXo0CqEEEIIIYQQYrXo0CqEEEIIIYQQYrXo0CqEEEIIIYQQYrV8MPvVFyi1cBalM3ZHEtGPJOu8INctErrHDpONY8bk3eGEr+9aTEo+JkwGHkkSeU0kFCGgJMAnlFo8PM6TzXfFJVzzpsZ3uzLYvgeHYqPrcygyccJ3uwwocdp32F+uQJHCaDCxfLDz5HVL2u2KJKmnE94rO0xKT+R+a8JblAGE8gmU5ecYi/YrTPyvHBFd7OcJ8uOA8e8qIs24xz7MJ4zh9oRtfByxjElYXMLrxgHjf3yY1/WMSHm+fkBxmPsU4zU6IrXCIrO5x3iyB3y3noynImBdLbEitdP8nW2P9QoVabdIJDQk/ocJ57W1UTginCAiBuJJMYYJmya8X7GQtPUZ58qBiM1MIrKXTESBpL+ZrMISIUQZiWCiIutRmr8zk1A4ItPxDueJkSzNFfkdc0EcHDGgsGnsiWDK43ML0nbjQhRXRmzzDZOSeGzfzuA8Flf+u/NAhFKW2VRIZxDniHEW26BeSIt6IqoZRhSzRCJT6omcjCwB9LrCo8DGBLxue05kgeV8k3IR2XqP92pqfGYaiYgt49wee7Z/QLlMSviMQOI4E+nUYBfCJjJvFBWKdAKTTHpsk56sxWvjosC2IltIMyas34ZIqiZS53Lx2eNE5sUJ4ycNRHgZUbw1kLEYiQSucBgrZYexV2+voMwuzkv1hHueTNrIE4nRGMm8SIRYBRPZlfjc7hHnrIKIzBxZyJOd199HnO/LDdkHjVi2XE+MMcZEspC9h3WvFkIIIYQQQgghvtfo0CqEEEIIIYQQYrXo0CqEEEIIIYQQYrV8MKe1vMacy3K4hrJqwP8/TzX532WS5FEs/p/bdeT/m/Ff2Y0jeRWB5JKOEXMccov/o2/I/+3XBb5v35Kyxf/apwLzip5O2JZv3+EX/k4GcxoNyQ8qyJcPt/fYdpnk33zy8U+gLOzwutuvv539nMgzL2psc0vSOY4DJkGw1KA14T/Bd65bkuf8GnMIEskFyqT93G4edw35cmuDtzflc5KPUFxAWT9h/EeS++o2OJ42BXZk7kiuySLX8568cJGx3d7dkZzWexw7Dcn5uL7Cvul6/GxNclXTBeajXGRMHE/3X81+PlUkD4xMJdvlN9IbYwaSW2//BH5tWJPkJUfawQ+kfpj6YiLJo8mLfNXM5vuEbVqQxmc5QwVp+9pgvWhmTYmVSCOOAe8WnclyxD0pI7mvFclnImFsosfPDkeyfpAvr78kOVnLucgYY8rjvD0jy4WsMel88mT974g3gnX2iqhq4vUIOEc7Ejy2xELL8lzNol1a4j4gzZkNi3+yxpBc6uCwXifyboEEniO5mXY5Pom/ZOsxTkriP2CNGcn4jxn7IQx4v5bk0Z2do5uCDFnTPtzPfu4rMvbxY6Yk4zqSfMGCXLc2npxjnqetcU7xJ5JzWjKnDPZHGud9Prwjmx7iQ7Bkj2KIK2Iifo7C4howkD1/GjFu9yS/tgrz95vIPB4yjtk+YXvYAcdAItcxL4w74j7IWRzvFxfPoKwosT374yKvmzgnfIH7O1cQ98VI9gl/wEboT2DLJIQQQgghhBDi+4oOrUIIIYQQQgghVosOrUIIIYQQQgghVosOrUIIIYQQQgghVssHRUy1w6Tc3RNMkD4dyJdGk2Tr0ZAvXF/ImcYGk6PLikg4Kkxq78mX2bvxIyiz5Auoo8Hk5RMReCTSbG4hxCjJl00P5AvD2ZfZTwe87uFAvtz4DEUa3TkR53gs+/wJll08xQTxJsyfkeIRrjEkST02RLhBvmycfen5mghE2FXtMHb6HpP3yxNJmicJ7OMiWT0R6VBZ4XsE8qXdJ6aSCRgnhnz5tp8w1ntH7B81+V3XQjBjGxQ33GZ8ZneDUgt7xLIj+UL37hoT/4szjLuOiK0+vSLzUI1StKUk7mKP/TyQL5EP5Iu2fUOEM2/XL+G42GI85grjbCpImxJDyUiETdMwj7NpKTUyxhC3lXEer/OOjJWJyV7w5Zg4wxGx08lj/ZvF2lY3KA2pK9Lfgbwv+X1yzyxRHa7Fe2LsuTD4LtUO47YhMjZ3Np/f80hEWgk/l0k8lGRuM0TWsSZ2DYnrEts4jkQKRMSTicTisPgskyc6EhNFZO+G/WrJMzsiCvREumSJOSqTfVFTzNtkG3Af50n350AESycsG5mgsMC9x4HMTXnCsiJgWU3mOmvnbVJ0ZP5ie8KAY84RMZ3tmcZpXZyfYZ0zk5FV2OcTM4+1WOf9Ym8ZGyJ6Gskemuz5G7L/KIicqSNje0OkZRWRHfVkjRr6+d6lZAsgkaqWJH7GEq8bWyzzlrxbRYRnFteZ7Qafu90SwWU9b6cQcY82RLxXciRGyDw2RBIj70F/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVo+KGLqb76FsuMdJupuq3MoY3nz3mEidZ/mF4aSJFEnvNkuYPLuZkIxRT7H7PcbfIRxD0RqEPB+Q9VAmY3zJOfYnuCaPqO8YSBiqlBhvawnidXooTGHI8qkthXKk+7Hf4Gy8QGFVWnxzuOeiKM8Ch0skRw4S6RWxPOzJqa7d1B2s8eE9k2FnTE6bINIRExmkYReEnHWjiT0X+4wrk9EzNITucY7UpZ7HJueSBSCx/ivy/m7bIjU6XFAmZgnEoqJCNZ8ie22LTDu9gkFAYG4EN6+xPEZttiv8TSf1yIRmliLz8wjPjSS3xEyUcvasERg0Y5EbEIEE+OAnz0y6cJStBWY6AY/Fj3ObTni+HEFPjMSgUVybI7CdylJv01pPjcOgUhDDL5bQYSFPTG23HkicSISo3AksjcijuodjoGcL6GsHuYN35H7PxRYFsjaVhKBDxNzrAnbYowdiUHQE7HRiFODGTPGRV7cbpjwGrZZm4h4L0wYO96TOdUTwdKAdciJDDxiRct+vi/0RDpEtnEmtijoyyR2MpFxRkvW2A6FeoGMp9vuKygr0wWUuUXMjgOTVeG+qyP7PUtCPZH5dXWQfdpji21fWrImn3AQvCNiLLvo80w2h8SHZFLAfUU34DxbksYviMxyJH3UMcFlj3UNC6leJnuDxGR/THZG1sCixr3XOGLsxRH3gZZIAHvSXzniM5ZzedcR4SUR2U5kPXWkzSNpk/ex7tVCCCGEEEIIIcT3Gh1ahRBCCCGEEEKsFh1ahRBCCCGEEEKsFh1ahRBCCCGEEEKslg+KmFoiTigNJg13GZNwfcYzsSeJub6YJ/X6pZXAGDNFFBPcPeB1RU9kFZs7KLKk5lWB71ufX0PZbsRnfHV4mP08EQHJSGQ6ZwVJaHaYCH19hmVug3KB6xKlEQ+Yk232dyidcScUc0zmbP5zJNeQtqyJcSFXaL+yJJbWREuEQsFg3boSE+4tsQZYIn8JC4FNjpj0/3gkMq2WxD8OQzN4HDsTERtlItexJEG+IeKIBzcPstOAQTeScd0QkcZYo0TBE4nAlLGs6vF+pwMKDTrSr8Ujvp9b1MsQwVhBxFH9iG3EZCgTM5OsDpQ1JDIfT0R0MbTY9pnMg8VS0uZxUgmWyR+ItI6WIYHEz3Is/n9lOFeagHPvMpQduRc1zzkipiBj8arE66oK3y19hGtWmXBM5R4ni/6A47YNi/mpxbnOEYkZs6YUuzMo827dYyCS+DcJ4z+SeXEYMMZGInXxYSGhYdKtxOKEiVTw3RJZdyybP4kkpSASyOX7GmNMXIiHpkgEdVBiTCDCQkvGXBUwhodMBDZEvDeSebbbM1MUjs9pIZ2ayJpliWTSkHkoRHy5P4UlIA1EIjrg/DFmnLf7BxKjJH42i3kmNtjfGzIU92S/5MgYS0RGxyRwGxKldiRSrR3ZQy/2S6eOjEUiIswkppzF+Nw12G6WSEBzIptyMqYiEa/FRKRiaf7OExHFjURO6MianUi9DDkrvg/9pVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGr5oIipIgnmkeScu4FITCpMzN0GTAYe6/kNPZEXFAWRBpCk7PYKq5QHTHKuiZpjR97XdnjdXUIZkY3zRO1NeYHv0WBS8jChYIfVy9T4+4W0x2Tog8EkZ+K1MVV/joVELmGLw+znssL3KEnStzOYuO5H/GxPErXXREkkIZEk6qcjdpqr8X67kkh7FlKCRIRFVYVlW+wu8/Ac+z/d44XkNcxmQ0Q6Ce93zEQms4j/uiRilguUibX3mPRfkIT+QHxdLZOddURyQoQ4u5EIbLYYx9M0l4nUJQ6mOrHGJDFC6kCmw9UxEvmaiyQGiIQjEHGVI+Ihv5DgFUQA5ti9Rmz7QIKb6d48kc5MxOJSEVmemYg9pZq/Xyb3KgO2EXEdmkykO5PFYGl2OD496RqivjCniUjFyHw3LsZPIvVicpGaiMdST8QfpB/WRJxQTpUTth0TJRUFtktBOjwtJDSOCHt8QTqWBDYTEWXy9wkmCivYh5ljiHTZ0sNmSR3YByMZS+yTicyzYcD7jSNGeyAC0NKiGJJZkdJiL1OQeLUjmQ88Ee4QayVbn9ZGO+I+1RFplSV7o3KLZWdEAlY283ZNPdlD1uT+RDRaNhgDI4m9QMo6EgPVFjdzluzT4mKjwmR/3pOYJZsDR+RpbF7IRIBoHda/JGNgItcxGVnK83nblngvN+G9siUxQtbskYrMOOteLYQQQgghhBBCfK/RoVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGqxmSQTCyGEEEIIIYQQa0B/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVrChy74P/6v/5iXZUOPZ92UPZRZY/GGdkM+281fykW4Jie8l7f4zBgH/KzFz2aPZTYXWGbxXcqihDJn0+znoqjhGvJIYx25lyf1J+/m0kjuR+rlEpS5gPczEz532YXOQjiYmLHMxAmLDN6/IO/7v/5P/z1pqT8Of//LO6hcGLEeA4nFMmC7jGycLNoqZ6x+qPCZ1lZQZk49XudJXJOPFpbEosMYM4n8rquYv58l97It3it7MpcUpP4ZpypfYoy5iPfrLV4XMqlr1+J1Zt5Q4QouMeVIxmbRQVlLrqsvMUZ2m2Y18W+MMb/43Qle0rUYj6PD2K4KnHvoWpHnfZTwYyYEfGYi4y7tcQwkMs+w1S+P5N08rimsDn4xBjp8DWMGLMyk3VyJ7+sixqzxZLyTdxssjj2XMB79eMT7mflaVm9xjFUW17vgcDx1BuvQXOAY+OT509WMgf/zP/4neMGpJXM7iUVDlkZvd1DWx8Ps50DWymEg8U/WophO+BpkbWdjJ09kbid7IG/JvL0oshmvsQ7LvMeBaMkeKPZkDSR7m0DqFQIZryWbAMheadmHJDIzWROdw3WHtVtV43v87//b/7Ka+DfGmP/0t7+GSM4T21fi+PYkfrLHDYhfrgFsH0T2VGwzEyYyZ5M4c9C5xkSy13CG7F1InC37fCJztvvO74bt2w0s+DDODBlnxuJze7JfKsi5wtj5WlGR48PyGmOM8Rn3QT05j23IfvS//Tc/o2NAf2kVQgghhBBCCLFadGgVQgghhBBCCLFadGgVQgghhBBCCLFaPpjTalgeQcR/QLae5AwE/J/sOGEOjjPzzyaDn8sFycmY8H+5c2RJJOT/xUmCKfsffUv+TzuS/IVczss8+V9xW+L/tweD908kF8JOWK+J5Cp6kl/qSDdbki+QSG6VWeTWRNI3JpG+Ic9k/2Y/khySNTGSfpwGEmMlXnckuQb9iHkF1SLfro8k/4zkUHmSX+wS6UOSH5Qw1cCUJXkuiac44YfTImeqJLkiZLgaE/HdpoHkPZUYJyTDz+Qe29eQvMqxJ33YsXlnnpcXbvDdthuSV0XGqx8x16w7x1pgxtsfl7Ync8WRzG8bvO5Exjybt8tF/s5pwH4MmCJpCo/3mkjMJhJnacTryDA2huTgxBHHgFvcL04sJYfci65j5JNkHJdkUh1b4nUoSJ5vS+KW5qvPP3s84ByzOSP5Z2y8k3zLIeJa8Qn56B8LeyQ5eSPZOjVYVpE9UN/toSws1uPJYDtFkuufyVrhyfwZyXt4ti4Y4gkgPpGJ5MdZt9iP0HxeNmuTNYu8L0nzNY7kqkeyB7Sk7TzZAzF3SFrmWtI9EPGLkL1jJvmdPV0Y1wXTneSJ7CvJ9DHRvTbL951fx2JxHMk8Q/bLieRTJyJKSCRGWY4oyy/NZN+7vIyeUUgjTSRfnQ2LxM4oJL+UnbPYgGRroCW+G7f4bEdeNxBvQmK+G7JmH2hHcPSXViGEEEIIIYQQq0WHViGEEEIIIYQQq0WHViGEEEIIIYQQq0WHViGEEEIIIYQQq+WDIqYh4ZeGG5KsnguWDUyydUk2fVrIBKZM5AIdSdQlSfiJGCwS+YJfS+5Xsi+gJ4+dyHVFXIgUAkk+J78jYPfyI/tiYCwKJLF8IsnbBUtKZ18sztpucZ1jYiLWbuTdWAJ6YOKsFWGJmCR6kryeWdY8KYqkzxa3i+QLpC2RCaVAZFdkGGYST67CZ4wk2D3xGiXy5dt5Uf9M4t8TOVPXklgv8bMtEScVpJ16EteezFfWY3tWRIpkHuc/Er+SMURC4wZ8j47EyIaMk7XhJjIGyJeBj2SsBCJYmEiZXQosEhHgEWlbIJKkbIn8YiTSOiIIiSRGExEqRRJTbtGXTCSSyUQ+EalPwcYAsV84Mp8wUaCJDX6WyEXCDseAWziBMmtz5tsg8d6RtixWLuM7MaEWmWeCw7bLAzHeEbFR7Od7np5IksKIZZHssTJZj0cicDFE/hISWVNIfyeyj1uGE9OQmS2ZS8g2tCAxXJL9gyN7xZG0nSdzmCHj3zm837TYF2UiDktEimjZ+CexTvd7KyOTPSRrv0yPFCRuiQRrKbxjawCT4iUSaWSImRTJvoqJAklf+ojj3ZINXlqI8RJbY9i+ncyVjq2drBvYHEDGMamW8YEIFSsi+Fzs+5kEdFl3Y4wpyHuMTBJF1uz3sf7RIoQQQgghhBDie4sOrUIIIYQQQgghVosOrUIIIYQQQgghVosOrUIIIYQQQgghVssHRUzWYYLwSGw/jomSSBKyiSThdpFcHVhiMUn8zRPeK2UiUzEoQ0hEppPyBspCxZL/ySPs8hpmcMKiTAojkcQ4kghvA5FBkOdaIsWxJBE+E4uPS/O+iAZFBSTP3lhDhDhE9JOIDGRNTER2lEkbs+42pM+IN8WYhZyBJtuTRP2eSLEykyMw2RcxCg2kLJC+jeQZ9SKhvyN1Z5MNcTqRyDGmTeRFSJkr8IaezGHW4zxRbogkp5m3yRkRQrH4n5h8gkwclpneVkZsiMht/G7zmyGiBzYI4kLIlki7lERORNw0xpCxQoaKSUSKwoQ1gZRNTFq2eGxKTM7GpCFEuEHkLJPDBu7JdGwS2tMciXdX43rX7HD8xGpe/4IIfJgghQm3mK/OWdaJK4IIXJh0LD5gfasS2z1NLXnE/BklCdjEzH4RAyBHso4T0UliezEiU8kW71cYLEvL/rb4bgmrbpxHMYsla5Yl8W+JEKvK7Dos88TCYz2Oibx46WEg4wtKjHEG+96T9S6zfl0bgchX2V6TSJEcWX8T+WxargFMiEQaemJ7Y7LGUFkWmaNG0h1U0kp6fXnZRPbZlp2f2D6AtFEk1zmyjllWVzL3WosBWZZEgriYK1zC8eTIeGIuXksGwUQEaO9Df2kVQgghhBBCCLFadGgVQgghhBBCCLFadGgVQgghhBBCCLFadGgVQgghhBBCCLFaPihiGonEJ7YoNorEqOIdlgUithnGeRJuIlIHbyt8ZklELCcisPBE7GQw4T45vG4k+cHEVWD8ov65IYnrJMM7kUTtssQHeJKBPpBEbZYvXjKRBBFR+e8gj/EkmzsSAwtL0k9E4OM89uuaCOT3OiORUPQD1reosD0DEdOQNHosYVICcl1Bxg7xFFDZUSIXkiFsApkTltIAFteOSGgCqVdBYmdHkvc9qasl/RUHbOEtaYAdEQkMxaIPSb2YuMGSfvaZjC9mR1gZlswfzGzUExmfI/KpglQ5LvqNySVij+3H5BoliYGaCN8mJlSjcr/vNi5gGgxM/IEfo21ELhxJrDCRRkvmp5JI1rZknWmINKUv5p+1ZNswkrHtKyYZwz60TNi4IlpPJD4kFifSLgWZK6sa7zft9/OCiG0XCjK3NySGT0zMgvIgY3dQ5IjsKBN7ljNMnjQXD43mhNdMeyhL5P5FRWRSZAxnJugjMcb2Np4JwCzubfOi7UoSr5GsxYbETST7LvcnsAZ4IuOiLU+FlKTfiFBoKaSaEolj5qwitkgmhMrkfp7MvewveYmUZrJfWN6OiQID2bdE0kZMXMq2XoZJ4cj7BjJWLIlH4rI0dinLTbhvZ/2VCtKH5JnhDzgH6C+tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWywdFTHY4QFkkSbg2kmTysiFPJEKAhezIJpKk7TER3JME/liTZ0Z8piNiJ0eS8JlQJkeScGznlXBM4JRrLGSJ5bmEou/gvvn/n8FkSkSKw5LjidQjLNLtI8u0J21kiJjDkcR1JmxaEyNJLp/2RLBEJBmJxEAi/WMXWqTK4zOLggmcsO1qIjoz5DrWP+OI4ohAxh0TYizdAolJD0gdyonECflVWk/mhIK8WyJytoKILjLJ+yePMHFxu3jEawYS/k2Jz6zY1LRuB40xxphMGiY+kP6tsTID+lqMJcK7tJBJsFhhMjpvcG4vHZHgkXHnRiKLgxI+bzEjiA0LkQiTjBX4bpaMz5KIRJh4zFhcK7zDdawgdSjPyTPI77GXUpPYfzfJXiDrc0PGAB3wK8JNGGPDgBOIp5aUMyyqcE813s0/y+I6Rdw/BGJN6Tw2ciZjwk14nXVkv0dMc2zJD35ZBxInZO+U2SRI9mcFmdsTESAaIpwpChwnriJ71gnbs0zzdXEk9XJksU+JCKyI+Oc7bMP/6CQm1iRb+Uzm6MiCJZP2WswXjhokybw7sf0Ikdax/TLZB2XWR2Qus0wWuJhnHZN9ZbI+kTXWM10mERuxs0zOLZRZsgbYkvRrTwRTi/Vu7End2V6eNHoocSwOGfee72Pdq4UQQgghhBBCiO81OrQKIYQQQgghhFgtOrQKIYQQQgghhFgtH/xn+iHhJfR/o0kuHcsjSiQn0i1yWiL5H+1g8P+g4zIZ1hhTBHJdIrlv5H++B/IlvSx/1QfyZePD/LOZ/D+6b9iXEeN1LH+AfXl7ILkCkWRlFeRf43PAtvMk53aZj5AM+cJs9v/4jsQDy4NZeU7f1GJ9jw2JiYgxVpJ2GUlfVIu8B5Yzw3LcepbLwX4PReI6kDiZSB6In0gdWG6In9chs/cl+UeO5P0UJGc2kusmUgfyvdUmk1yo6RH7Ndck53yc14Okh5uJ5McOJJWlITk6rK5roz9hW93U+N4bslYUJKfFBZJf+R2+WL7GIWZakm83EodBJPOsY7m1JDfTkoDPZOwV9fw6FtskbdwEMndMZI2JA5k8AsuRJ3NAwPc93GHu69iQHPNxkacVSI6zJ34JMv85NheRflgTI8m3sgHrGxqSOz+doMyRyaGoFzndPe5jWAJ8T/YZBdnWdWQez5n0P5mjpoh7BWtJfC6KMsv9JHFNmtJkcv+S1DVv8X2reIHXMWHBhHtAR57RL/a72WN7jORzzpAJi+Vkkr3d2kgjxmxP5krP9hAst5nm7M/7nOWWFqRsZA4cliNJ5llL1l+2d8+GuGJYzq2bP5csE8YlMp9E4iwhc3aeyPgk5yAWUZbUoT+hdCKX7JyyPN+Q/SPpG5IyS69z9I05+kurEEIIIYQQQojVokOrEEIIIYQQQojVokOrEEIIIYQQQojVokOrEEIIIYQQQojV8kERU0EESz35clxHJCuxIrKTniRI+3mZnTCxOBd4f0uSkkkeOPuKXmPZF9AnkpRMEo4tSUpffsl3sBu4hrhFTCKChKLEpOQ0EfEB+bJ5t7QhGGMMEWe5gTyDCFLs4hmOmGh40jeBfAF3XHm+J1sAACAASURBVPn3ap9InzEBlmHSESI1sUQcY5axg2FoQo332rA+JGWhIMIu9n3uE8Z1R66LHY6oZlFXT8Iw9USYsCOSBhKvkQjATj2+XDniZzcFlo0nIkBj/prdUkBApGPkS9ZHInhoiWDtbN0OGmOMMY/kvT2Z70dy3eYcG/VIRBSb7bw/woHMRVT0gPcaSVxYi/0xsG+bJ9exeTZNRCSyiIMNMcwMZPhHImdhAkQmZxonIuIhAqhAxkBOROJE2s5t5vdLmYnoyPpP1s6BzJM1WQPXBNkCmWHEQd89oHSJbFFMO6BkaVpM+p7sgYoCX6QkgspE4skQoWQm1yUml6lQ2GTI+C8X95sMiu0mIoMJpJFiQeb7ERfGIhF5HhGFVTURQ07YdtliWbmw+6UB7xXIHMFkhCkdsYzsRdfGwNY9Jj0k65klcw/zYoXF/tORvZJ1eK+CzDOJ7dHIdZlcl4h4bCLjIo9kDSjmz7BExsXezBGpVSKHmSnjvJPJOlO2GKO+JgJZ9n5kbbPFfFxk8h6OjAFD1uyB2AjZtvh96C+tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWiw6tQgghhBBCCCFWywc1OPXmCZR5IooxFZGilJgg3JdExtMtBAYFJsOPRBLEFEtxwExwlzFZPxLhhnH4XEsSjlPGZhvcvCwSO4sfUc4UPL5vJnIRRyRGiRgibCDyC5Idn1q8X3UGRca73fzdWBY5SSJn1imSQ0/FTmtiWxPhDJNaEFFBQwQExQb7zC0kJpa0Z7PFWD+RceiJXKYl77sl47UgsgpDJCknIoUKC2nARKQH2waT/lFdYkxDZA7VJX62IZaogYynRKQh++WcY4w5e3IOZX7Rr0wG1FdQZEoiJbCkbzKTAa0MNkf1xH+SiNgnsXFRkbl3GWcB78XERrcDEUKQiaYjcq+CrgH4jEzm1I7EgV28XyJjINR4/3EkciIiO9qckc8SaUbPjH+RCLFabLvwlMwBCwkJE24kXDqNJb8Tt5msY0TgtSaa+hrKShInsSbtbnGSnhKJ2WG+zrqSiBdJHLK9wngieyAibIq5hbI0Ypn3WFdHhEVx0Y/RknrmLd6fCJtSeoSykchwPIkdaxsoYwKwKeIaUG2wv8Zxfr9E5peUSTyQPdtyLL3vs2ujJBIw5k/LbF5k+yCyhwYxHNkHebLnTUQgmS2WTWTzyvZLlqwVngjEmDsoL+JxIkK1zTnGpyWGPipjq4jwjkj7mDytPZG9y4BisIsXuA8yiz2qL9k6QfYJPTnvWZxjMhPAvQf9pVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGr5oIjJBUzeLSL7GIpSigIlAXkgibnl/LOJSCPiiEm+MWKysbWY6J9Jwn0msgpPhCMlSTjOAyYSm2kuHcgJ22Os8P42EsmBIbIKi21ZRSK/IrKSPu+h7ERENKH5FMqKan6/2JPseyI0sURUwcRZywTvtdEZ0u5MSpCwL1oiHXEtXrcJ81gZeoyTQ4/vcRpRdFF7jDtr8JlxxDEWCnxuVaNwI+CrGOfmn313RKHFhoiYNgXGxIaESU9kC8UW61UP2DdjcYCyV2+/hrLPPvlvoAxEHyReHZF8lERWxYRQSwnXGtm3OFcUBcokRiJF2ZP5ou6YtGzeXpa01f0Jx1PX4hpQbnENCGQMGCJAyYmNd4zbmsnnFvP2ocV51zcouQgVmrxKIi8xZC4yFdahHnHMDhnFNjePr6Hs4tmPoGw5BhwT+xErS0Fkb2QYGzMxpcl6KB32jy13eGHAdq9rnKPjhGXdQiiWiDzPkj1LirgXaUkbJ7KnMESAVJB1zDBZZIH3W8r3LBmb1hNBY0XaLWCbB4/P9BnXGTcQ8eb5WygbO7xfOaF41C32tkvhlDHGhIR1jWSv65mwbGR9sy4yERGxvashMj4mG0zkfn6xP3RkThnYGCBztnM4ZztPRIhEzpQSWe88EWgxFVOa7zX2LY7P4J9C2fYM55NiwGdORDBlyXxsOybB+xLK9g/3UHZ58VdQtpza+pYJy3Bey0Qmlcg4Hv+AJUB/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVo+KGIyEYUbmSTmOyLemVjSecSE42qR5DwwWQNJ/DdEzjRlTMCOmZzNSeK8mYiYw2Gidg6YNVwsnpuZdMnc4WsQ8QHx1ZhxJNdd4Pu2gbzvIyZIF0RO1ZSY0D6ZecJ1zHhNIpKTTcbQsh7LUiZSqxWxI9KlwWH/b4hkqthg3BUkZrf1XBDwdsQ22RMJjWeyKyKJMCWRCBTYZwNJhmf1GisiXVn8/quoyDNHTN4PJUozfElkICU+0xLBg9/hc8sjSj2un6Ik55yI49JyPJFnNkQ+UZJ2y4FIWZicbGVcEFnWicztT0oMoJrIss57MkctJGB3I8qzHl/inDX0t1B2dv0MyipLZDpEoGXJIPBkjh6ItCwv5Dk1EczkCeuVa4zZiggQeyKO2hZ43YGMgXRP5ugttue5J2NgsQZGIg+ryJzoiOTEOhwDI7luTXh3RgqxqKjJmkf2D0XC/nELGWU34P5kdDh/RrKFcwX2xdSTmCCiF7I8mUCe4RPuZWzezN+NCYuIhCo4FIIVZE8RHK4Vz883UHZTMQEiytnsHp97foGbr+NCJscEoJGsxUXGOccQMU0b2YZ3XcSJSNUM9qUle+1IpHWObFOW7jm8uzFmIjI6IkQaE346BSInIut5YUm9iIjJESmSXxiLzsg+OxQ3+EyD73G+wXodyd8ZG4fj8/WW7LVfkXE8HfFdKizrFoeyqSPrBJHHkeOCsYHMbTi1vRf9pVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGrRoVUIIYQQQgghxGr5oIipKomIiSTYW5JI7EmCsGuIPKmdJz6XAZPhB3K+7ifM3rVMzmQw+b2w+IzsiDzJEpGCw+T/UMzrUBAhUskkOfYCiqpzvK7smXQKnxH3mOTcovvKFBalBmcGy47dXDCQArZlJkn6wWMdoiWftev+vclZjXU7kFjcnrC+bmkWMMZcnWHcuYUQ40DEBTZhzOUNEWcRv5hNTKSB1xVENtD2WK9EnGgP9Tz+z4lMbOrwocMRyx6usKwyJP5JXR9qFGJ0AcUCf/GDH0PZ1qE45/A4r3/fYJtvPD4zkvG6Jw1nAxNLkAH7R+T8jEhGiMhrd4vxc/kcy85I9Q538z4af4uCJTLdG1+Qefwdvtt+QAFSs8P+dkQoNBKpXN9jTI0LidH5Fscsk5GVDyS2iUzJkbUtknExEGHPo8X6f3b9Eb5LxjWgX7yfI6Kbgo4BjO3eE0EKEamsiVBjjE0dxliKZN4mc+pkiMhnmreBy0yeyPZd2BeR7AucQ7mfJbFuSexkJiwjc6Uv53Gyc9hulhh4zsherCJtfk7Ehkeyz6pvsa6PW2zPzYtzKHt69hmUNfFh9vPbAsdSIhJP4n4zQyICGyoXWhehJHt5sg/yxKtZEmlfTcRVdpjHhrUYn5GMp0SkZTHh/Qeyb6mIBC+OWBYSGSuR7F3LueDxismfIvZ30xGJ2wXG7DWRdr09oezJHLBsX+Cadf3JcyjbDngmaW/m4rUh4b2YFC4T6Vx2KMHsBnI2eg/rPjEIIYQQQgghhPheo0OrEEIIIYQQQojVokOrEEIIIYQQQojVokOrEEIIIYQQQojV8kERU7PBLGpLkoEPLSYNo8LJmEDkNI+HedlA5AJ9i8nW/UQkBJZIA0asw+QwS943RCZF5Cl+h2WbRdb9ebyHa7Znl1hWYSulCrvlSCwkh9/dQVkyWK80ELHNFm0ozQXWP5zmdR0DijrchPfPnggHiPhhKlGksCbOSaL+dsJYb/dY5hIReOywXV6N83785hZjvU5EEvaAfRFI7AxHjImzHb7bPRFAXbbYZ6cS26RYjH87oQhgV2Cstz2aG3b3ZGxWGOvHl0T0Qp5xR57x+c9QNuA32IfVsGh3ItyYiEgjBLyXMXid8x+cgv/olAXOCxURCk0nFCy411soO11gnP3T7a9nPx/aR7gmjljm0jWUPd7hdaHDeJwu8bPpAuUsZULxytHieHxm531eZXymjRg/2wbbrYg4P7dE4NN+84BlFQqgOjIey09wPTIB32VayGN8j3NH74lkj5QlIo7KKxcxRSITGcl83BEJ4m5HhEpEyNj38/6JRBqTE85jPREbjSPOM3kiwhmyjxvJvshGrEMk0qminPdj4fCaqsT5oCY7RVvhZ7/pMDbrVzg2o/8UytL4Gt/lKY51S6Rz24WMc7jB9z2SukbSbrXDGOkSMRWtjC3Zp00YPqaPuC6EDmOvIe18v5DbdT2TjOH9eyJJGiYi9plwrZ0y9mVB9jcbMmZTjeMxLZ5bn7G1E+fPszMiT7O4BnwzoaDw8cuX+FmHa5slctTwHJ/RldjGRZjHbVrui4wxJyKPY4JeU5D9UsZ4eB/6S6sQQgghhBBCiNWiQ6sQQgghhBBCiNWiQ6sQQgghhBBCiNWiQ6sQQgghhBBCiNXyQQuIJQnI454JizC5dhiOWJZRHDGc5om/DxMmOLuEycGRCYAyyiVij+9bNkRiUzzHdyMJ6GXCpOFqIbGpnz7Dz3lMuM8tCmbuX6Jc4NsB6xB7lGscjyghGSy2XU5PoawvP8JnLJK3LekHS3714TOG1kh+R+LTukVM929voOzVDba76b6Got//A8ofih0msLcLEdNAhuW2R2HA5WcotSjTT/C6jMn25h7L9udELHCG48Q+Yv0f7+ax/Vi/hWs+3eC7NVuMifYR2+h+jxIOjHRjHl/9BspevsI5J9SfQNm/2f93UFYsRArjCec5WxEhFBGpNER0lQ1KGdbGN7/+Oyh7/TWRp5hvoez499i/l8+w7O523m8dmVQSmcc35RsoCzXG9lnA+TgmFEeEDufescH+jcRCcuMWz+0xQpsdCsC2JZGnEbHhuwHruq/wPR4ecd65fcDx6A22yaeXfwVlZTPvi+MR5wlbkID3GNueSE6sW/nvzjuUKvaPRGxkSDw53CtstsRgM833Sj0RZ+UCJUkuk/nIkW2dI0LJQIR6AZ8RPRHpEMlWVc3721bY/6UlQp8e63A6oLDoQPZdRxJ3tf8CPzvg/RIuC6Yhq0qu5/01EqFN7nHeyEQQFB2u2dS6tTLYK/ZHsifN2M6377BtDkTad1qIzI7kHJBGjO1QYEyljDK6SERMgZ1vMByNv8LrGiJU8sN8rCR0fZmeyGJrIqM7HXDvebvHl9tjVY0dcL6/3eP9EpHZdg3WKy4kS1PE92XCtkzWcTuS+Z6Js97DylcLIYQQQgghhBDfZ3RoFUIIIYQQQgixWnRoFUIIIYQQQgixWnRoFUIIIYQQQgixWj4oYmLn2mmDIgFHxDtuxKTzY8LE3+J8LvU4a/FehUPxx2FiGdPXUNS1mDRclChFOiPSpbDBOkw9JhxXZn6/dkAZxgOp1+t3KFzYDGgIuB9RahUtShP6ERPh44htbnuUety+wOR4vxAiZCKT8QHLLndnUOYcxk0y3z0B+4/BfYd9/RBR1jBZjLHLCiVGz55fQdmTj/5i9vPvH7CdCnsLZU8zxuZgUELzpsVxsjnDuHve4JjYkKT5rxxm/n90MY+xt/cYS29GlI51BmPHJRSfTMdXUHbKKJLpChTdPN2hwOPwm99B2cP/8NdQZvfzcec7EsMZ7+89ltkK61oZImVZGb+7wbZvTzjPbMm6EEqM9z97+jHe7/lcDHebMQZOexRz9Dc4Fm2Jc8/BY7zvKoz3JzURRwQcUw8R5+OLhWQwOZw79ncoJbkl8pKywvEeHnGO6ZbyJ2PMKeB1TYPXdS/fQdmbCteFYtmtCdexOmK8hw1Zn4mIx65cxnfX41zW9xjr1QbbYOpxDSjOsP0uL+fxvzEYJxNppyFhvJY1vsdwJMIisvZmspbnCceTNxj/wc/rVQaM67HHMXw8EUFjifF6zuoVicgv4jrj8FWMC/gu0eKFfiEEikRGWZD3dST8Y8DP1jhcV0cssO27gkjAAl6X9zjPTuR+xXZuLdpE3N/EnsyVFt+jtUTERMSlfUI5UU32PGmP+4pHEgfXizYZTti5PdlDvHuL46km60c14J6/KtH21Bucs848juM04hiYGlxTk53PH5HMO01AgZPzONfFButfn7Cu70N/aRVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVp0aBVCCCGEEEIIsVo+KGLKFpNrCyIsMkTE0JSYcJsdkUSU86TpS5LM/eT/be/MeiNJsuxs5uZrrGSuVVlVXa0eodXQAi3//yfoQYCAAVqj6equqlyYySQZq2+26EFP7uckst7aMX2+Nxoiwt3Mrl0zJ3m/2KCI6TMphP78CxZb+x0WG1uDxeFjg9etHBFHbV6Q100LiT//ioXFD/coZzIlyhDuDliUbQz2KxCRQmmxrU04X6PD+7t/QPlH9jj9vYYjY7TdYxidDMbNisi6IpNpLYiuw9/rVDnKvlYNFv7vEs7j9sfvoc2VU+HAf9ljzOUe11IggqG7n/CaKfwZ2mKH4oh8+we8NyIKurE4Jn2azncWUQ4wPGKB/4EYMoYHItLIUWhiDfZ/fYPzcClR2LRa4+f1A+aEIk6vcR4wXpsK11JWYZ4bA/l8nOrFMT5h/oxEHnJOKKtYZ5h7HnIch3XzfPLzd7e4xgZyHx/c36Dt8kBy9kDyJxFoPQWyVwSUWuQrjDObT/s/DhjbvsOBGxyu2ft77ENmibykwHxfO1x7dkWEZw7X2YhdNa6Yyj/yBnN7ST6fET3pPxFMLYk0MAkijsHYY14syBnoQvLMen8z+Xlnn8Nr8pqImDoUNp0+Y44a1tgH7/E+kiMSFvK3jbwmIstmGicpkOQWXkFTuSGCHOJn9GS/2w44Tu5M5DI30GRSwHEKZ2wrt9N+NeSMlWXYNhZ4PvXkzNqPyxaRGWNM8EQsmBHx2ID7qtvi6wI5C65203F+nhM5DxFvXdgauMM4HokIMidys1jhs8YloqBu1WBQ9bNzbziSfWfE8WAHgSeSJ7IGRUyZx7NclTCnuh3mrDTgWnEjXrecvdUTKVpO/gTqA95HImcoz8bkC+gvrUIIIYQQQgghFoseWoUQQgghhBBCLBY9tAohhBBCCCGEWCxfrWnd7PB/vrsB/ze6IjWiWcD/348Z/k92nqZ1H57VQ67x849n/H/pssL/0T4OB2h75fGLi/sC/5e9tPh5eYb/f96+/zT5+acPn+A1NwHrWw6GfJk3+T/wPsdxcwb/N7wjX3ruSN3oJsPfV6SffyX3Nx2nbUnqE+I/kWviGF0N9quq8X6XxGPAOuRhhTUEmxP24+mMNRT373DJ1atp3BUNxuafKqwV+fQBY/Mx/wXbOixUW5OiobvhHbTlpAbdky/zzg7Ta3xq7+E1e4Mx7B2u9Txh7HQrjNesxy8Gf/qEOSe1WPOyIuVW5/+J1/hsp3P98t9j32P/GtrqEuf5E6lxLDYs/rGe8+9Jl7AuJxYYo7bFsbkkjNGf32Id6mrzNP2se4yVN3tcd7Emtdm3mO/DBddPHHE+7g/Ylkh93S7iHOWzOmYf0RFgyJeyXwP21Vis57qSXzE7UuveXXHMNznOYVbinlqR1w3ZdF4j2TuOI8ZDRu6N1bQWbtk1rUNkNevEzxDwdX2L+agosS2vZ2snw/F8UeF8nUfMbb0l6zXi2nQR94V+xPcmh2eU0uFZqbDTOu+BnFm2W5zru44cQ49Yk5gbrCP3OcmfFb6uicTNgOWw5nrBPbuZnQtLR2o5R+xXHMn5t8La4uFh2WcgY4zJ96SG8Z7s3aQ2MbBc1uOcj+107E/kPH5TYe55uOL4nS+Ye6PH2B4T2RdIDbsrcH5zi31Is/rkSGqYb8jeeSY1vibgfYzEMdJHzPcmEj9PwvyRr1mOxs/LZl6DknhyDDkrWpI72fnRkBrcL6G/tAohhBBCCCGEWCx6aBVCCCGEEEIIsVj00CqEEEIIIYQQYrHooVUIIYQQQgghxGL5qojp4QmLgfOMFCATqcOKfPlwHbFIfjV7a1lggfdwRGlAl1BC8BRRAGOIIOFq0cSSRvwC4ZRjv8IFC6k/HH6e/Fx6HKPMobDlW+LgOK0+QFvfsS84Z5IHvLdoiNiGfEmzH5+grU/TtuGOfJHx/gW0PX7G/u/32NaZ3/6lwn8Pnh5+gjb/dgdt6Ra/NL1Zoenh++oNft5qZnqpcIyvLc5NWWDBfLjDAvkmwy/BThURpwwo4qoM9rUnAoKLn1637HDdFCvs+zdkfZ1fYl87koeyubzEGGM78mXhBQoY7o743vvwf6Dt5Kdz83j9j/CaH/47XjM/oOAgz3C+xm4LbUvDRcwfdiQmqzXmgdyR34sOt9DUXqZzVO8wLxwGNCJVDuPn8oivq9e4prKI0qW+xX0mEhNTZ/B1H/1sPyJ+lRURETZbXE9h/m3uxpjzRyK1qFE6ZQOu2bbBvWJb4F48DngNY6eSjBPZO+oVEbUEbCvmuc4YMzLT1YKwPcp5skQkNDvM91WFeaYgcdw/TOPf7jAmrpjGjGUxfCASGiLPWjmcf0MkLFXAvpoM1911dh7rzxib9f730HbjMDar13jNKxG99T2uw5ycO8s17nfP99i2IhKn/c10TAZyTnr8gPlwOOOE9Rn2IZDz2dL4/IBxxvaA9oL5gzl2VgU5k/tpjO4svvHa4lqsN0TsU2K8lwnjggrKSpKPEtnHEvahbafvzYh81DsSdw2eIbItiqiuV5yHocU4c0TEZG5JbO9wXwwnInFqpuM09Pjs1Q1kwyPy2YysAU/G6UvoL61CCCGEEEIIIRaLHlqFEEIIIYQQQiwWPbQKIYQQQgghhFgsemgVQgghhBBCCLFYvipi2pVEnkPkGhmRs+Q1KbAnUo/MTYuQr8cjvKYgxfq2xCLfcMQC4bbHa774Douh759QQnB49x7aVi/20LbJp7KbzbdYRH0p76DN+hra1lhrbU6k1rxvsf/OYEF3WaAgYvcSr7sicp7jbDxDwn6xgvRqgzEyeixwL+2yJRy7HKUm4cW/w9ftUHZUB4zZS0Sh0Mef/2XycyTipNWPeB+XO5RQPFoi/uiIdOwNBtn9P+PcVgOuRU8kCnUzvb/iGfZh95rIyYiEpWxRwpFGItwZUThiHH6e8yg7uvkB7+WHH/4ztD28nc7X5tUzvDeSRastCneGE95bRhwnS6NpsM9u9z20pZcoI8sPGO8liZ/UT+Um7oxxHHMUSZQ9rrENEWn0Fq95yVH+UFe4ztyI7+0zXFN5mt7fbY1xt32OceEtyjBGj/2/kNTbX3C/8+NnaKuvRETV4Jraf4N7W99P95REfBuZJ7//LnEvGiOOec6MVQti02Aus0R4WD3HCSo67FvpMW/7MJUWWSJXOQfcswsilFznRHJTkvMIye05SWYFERsVFmVE/jyN/5XBWM+f4/3eepIESTxZsl6HAePfEtHVMOBaPNXsLIMSzLu76T5zeCAyJbKPeU/yHDmz+m7ZZyBjjNmSuBhxeZuxwddVGxyHkPCsvbpO42wkua0kn++IxKescG5tT+SjGd5bNuIcRTKXI9kDmpnwr8pwkKpbIm4kgj4X8IwWByL8ckRSOeA1Ag6n8Rn21RY4nud2ei/tlTyQeHxfIs97iQi2YpCISQghhBBCCCHEvwH00CqEEEIIIYQQYrHooVUIIYQQQgghxGLRQ6sQQgghhBBCiMXyVRHTfo/F//mIRbgDMQW1ayzgzwwWDYeZdOP0y5/hNQ8blCG8+h0W16+PKNK4fYnCidviW2iz9l+h7emIhcqXHoVKx9n9/ZDwmi8+/wBtf738X2gbE45ld8ai7BCwgL8uiZiDSEhchf3/T3/6A7T98vnT5OfTGQurqwLDaOWxED5irbnJzLIlBM/e3EJbeyVCiAeUa3QlxnrD5BeHqZTAn1BS8L97lA69aIjs5z3O9WaNbY8/4e+rKvIrrJGIRJ6OKHoJxbT/r4iIyf2Ccf3ugONhiRygveJYuoBx1zh8b+dQ9LHd/Qdo+/33P0Lb7mY6FyciP8sjWYc9ESEQqV1ORAVLY/0Gc6q5EtHFPeaeVON4FZbIU2b5Mp4xxi49xkpf4ZimnNzbiOszv2JO9eTzipzMW08kK2kmq2C762ciybni2r4Qwc75CftgE1nHxGkxlCj2ae030PbGobDpGmZrj8iUgsV4d1Smg/cW3G+XcPw9WL1G6VLssM3dYcwaIo7xOeayop2elfoLio76Ase4JsK30RDJ4oC590TObDHHXGYdnuOOj/i6aKfx6Wq8X/sW3/eJiDfLAq/ZR4zhrMVrpID750jOrDmRyexr3O8vl+nnuYTXbD3ebx4w/kMg+4LBnLA06g3GWeywz9UF85Yn4zWQfXQzE/TYCucxM7iXe3IO2Pd4b/4ZkaeN2Da0mI99xLzI8uCln+4BocIYc5+w74eR5ECSF8fhAG3xSvKsR0FhR+Ru3qLc8LZ5AW2X62yNEguXJz49krLoQ+eV2f2+gP7SKoQQQgghhBBiseihVQghhBBCCCHEYtFDqxBCCCGEEEKIxaKHViGEEEIIIYQQi+WrIqaYsBi4iVg43qLTwtSfsSj7vsC2w69/m/x8PaFwYo01xGbIUBCy2pPi9wYLuu/f/Qptf+lQCFDVKJT5XN9DWzaT0xyI0GOzwmJ267EQfE/EB6uXKE4yZ5ybzhD5lcfi7ZIUUr87YPH2zk2lDtUOJQ/WkH4Rv5Jt8JpFuezfmzweiEypxflvTxjXL/YohOnrV9B2jT9Pfj5eMdj3D8+g7dMtjrsL+N73F1ycpf8EbU8R5WEZKdQfiCjMzKQ29xY/6zjiOuwS3u9uhdfczjs76QAAG+pJREFUPkdpTJkw1gciplmTfFV1eH/3TygIKodpLiozfJ/zuNZ7g2spb1AiEYmoYGmMPZFJtO+hrYsoU9gT+1oiY3iN03EOPcaKs0Q4QYQtKRBhEUlINsf5GDz2NSPb5GBw4jI/ywFkPXUdxlhPJDklMRaVGxQPZkR2ZIjTIiOii+GE+8fbRxw7MxsTS+YhJ2NpLV7TErlImeM4LYlAJF4Zmccu4jxuHO6XGYn/s5vG+7XAz1pVKIMZC5yLmuy9HRFl2Q7X2IUIXPqRnKkSEUP20zGxI/b9Ss4YLdmz1jXuAZ4I+jISO3nEfTcnsXi9x3l1exKfxXTdjStyPuvI+zzuO90G7+M5iYelURJhZm6xzzUJPj/Pi8aYzhDJUD89pwZm9rHkTEWOIz4j99ETSdSAY+/JmrU45eY0kD1llheOZIyeIq6BgayBFXmGGMk6NkTI6siatURkdr4nErAVObvMbsVt8Lml9GRPHHEOU4XzsMqIpfULLPuJQQghhBBCCCHEPzR6aBVCCCGEEEIIsVj00CqEEEIIIYQQYrHooVUIIYQQQgghxGL5qogp67BQ94lIZ2r0MJinJ2wbtigFeuimUo864rP0OWBxtBs+QNu1wC59H15A2+X6GdpWDRFuvMIC4d0JK79DNi1CzgKKCtqEfbh9/QY/v8b73b5BEc+HRyIr6VGwczo8x2vsccKKiNXmY5q+LiMCkqLB4vhEhDh5wQwhpIp+QVTjHbR115fQtiLF6/aK83jaPWLb07RtIPPwyeNi2j/hOrykj9DmLMaYJ6KgKmH8nyOuMWvxvcXMZ+BKlJq1HZlrVoD/bA9Nr1/8EdquRCSSXx+gzTsU2Bh0uJnrI/brmk/jPbZELlPjfJWBrJMa10RPPAhLY3x4C21tdwttWYnj4HPMPfmexMHDVBJBPBJU5JYMDuAQMC4yhxKbtELJmutQahGJUCVZ7EOcyUoiEX8MpM2SNZC/wHz/ssA4fjhgvi8Cyj9MgXtP3uAgxx77n2YWEkdEgUySZRKR/xABy3zcFscT5tRri/EUHZEYnXBvzF6SMWin+XIg8prsBvfsmszX4DH+Q8LY6Ru833SPspbRYNswkv0OwgLvbfTY90BsdOEWY+zF6z9B2/kD5iZLxi5aXMNZifeSyNoJs/2uJPtzR4Q+NVkT6YxtbUvkZ0uDpBTi94RzsDHGpBHHa91g2/t2mlOZxMdWeCM5Efu0PTlXdHjDYSBCMSL8GgLGz3zNGmNMNl+iGc73iZ2DHL7OVSgj29zi2fN4/wu0BXJd57BfucM12pOzi511LO9JTjCY6zZEiMXEbgMRwH0J/aVVCCGEEEIIIcRi0UOrEEIIIYQQQojFoodWIYQQQgghhBCLRQ+tQgghhBBCCCEWy1dFTP0VC5C7iMXLzmHxbl7jx//44kdo+85Mi4tHInpZ1ygSeGxRTsNkKoNdQ9vrP+LnPfuM7/3LI4ouzO33eH+vpvIY3xHhhsFi/c2KCELWeG9btByYrsTxdQXKf56vSfG/xzlMGRZ+5/n0ulnB+oWCiJRI5T6ROFlSCL4knlos3o9ETuZ2RGCyRYnR/yh/B213v5/G0/FCBA47nJvshG3dCd/b5di2/+N/w2t8QInR27dYvH802JaVM9FLj7KBmrgRrCWyhRL7tSXv3a5x7TxWJMaI/6sNGMeJiAoKP43tkkiE2G/+EhEh+IAxYv2yRWTGGHMNOC7Oo4TCVkTEZFHO8MfqFbQdfpjKwoaBSIwqlAmtAo7z9Yyys3Ek+ahC4VdJpDADyVHRk9w7Ew9VNfa9yPA+HIkVQ/a7TYV5NowobOpIfso8iUcinUo3RKBnp+POhFtk2ExJ+pU5jHeb4VwviY89EXa1RBzSEMFSgYnrv5Z/wNf9OD2jHC4oU/IV7tmrM9kDAkpSYomxeEPy57XF886RyOeuJcZONjtOph7vtyShToVgJeaIP9zimoi7b6Dt3Xvcx7ojEwThvNZEFvk4yzEZOTtlAePaO8wRieydDtPa8iACrZ6ceUoiYootxug332HurfLvJj8HssfsVhjH14ycZYnhMBq8ZmowVi6P99DWHTFW2Mbv0ryvKNnKyBjlObb5HJ9b3uzJs4FD2eHj0xlvjuwVtcE8U5WYsy6zvB0izv2KHLRyh888gewBnj4vcPSXViGEEEIIIYQQi0UPrUIIIYQQQgghFoseWoUQQgghhBBCLBY9tAohhBBCCCGEWCxfFTF1TB4yEuFGxI+6cVg0nTss8t29mQoB7ICF1T5gofL2ijKEOsei71BhH9oBi5z7CoUQ2xzFRtUOn/XrmXQjknsbE77PEqlFbvF1xxYLy03C9xpS0OwqnJu1IQKkjIRDMR33FHH+giFF1ETeMmZE/OGXLWLqjkRqMZC+/Q374RzOz5//CQvTL/3UxBBaLKLPCvysbSSigi2KKb5Z76Bt3GBBv8/wurtbsl6JTcM1088biRAtEilPSQQWgYguPjy+h7btix/w8zyOU/0S++DOKLWJLP5nhplEnDEkHZromdAH17Afly2hMcaYdMV4HyKZtwsRc0QUUfxyxdeNYboubIGfnxPbz0hiNttibi8HfN1IJBF5RSRoCe9lQyQrdvY74Drha3pL8l0kAp8e4+KxR2mGKTGO64ABmW3xur7HHGAKIgaM02tYsj9lDvtAtjsTiYgqsX1sQYQjzkUgW15GOtwEzGV/Pf+C7y2mkpSYY7wWLREeGiIKJGegyhLhW8KzTdoRQV1B9kCD/YpzMeQR77eIJCjIuSsLGOv/8uF/4Xs3r6EpJ0KY5hnJxx/x/nomikrTvawneSgjAieyrE1HcslIRIFL40z+vlUQOehgMeetMB0bS8RDu99N83E6kTxGhHpbIoEzG3LmX2O+t0fMd0d3B21MoFUVTKA3vT9P9rpIxi0k8rpHFHn+q0NR2kDElRuHa6rY4pnMkTXaZ0SOGaefF3KSx4nwsyX7XSLzlVmcmy+hv7QKIYQQQgghhFgsemgVQgghhBBCCLFY9NAqhBBCCCGEEGKx6KFVCCGEEEIIIcRi+aqI6UKkMJ4IZqzHAvNLgcXAwwcs4LUzOYM1+FkFKdQdA97bvBDaGGNaUvxfXy7QlhU4HPvn2NdIRBQ2TYuX8walMytSfN6Tav2CFOt78vuFzBKBASnyZnXqkciTIpGmhG46JlVOxFGOjEdB+krmNad3txyOJMSs/21F6B8Nvjn99BdoczN5UE4K/Nc9EamUKMO4R2eGcTkKgF48oejCFrgmXtygrCUlNCs02bQtfEMkCjtcX+0R7+PxEWUD/gnj5G74Z2jbG5SLbGoUMfkRr9taFFtlfrqOywrFBSVZS9eE0oOBxX+B97Y0osU+WyLZsxnGbUeETXcfH6HNZdNrVBXKw/Y7jIFEco/vMC+6/BW0FWyOiBjLE6lFHvbYNl+iDRE9kVjxROwWR1yLY4evY7KjyuLYmYH9fhrzvb9gX20x/byyxn4VJEaMJeIxEg8F2SuWxEAkkFmGY5ARqeDB4Bgc3mL8m5lUryRCrPUGhSuOfP7QY1zndQ1tmwY3C3JsMc/2eC/bHtusmd5fvCECyFvcT9rrA7Td37+DtuMd9svf/Qptt6sbaGuqW2jrS7zGmQjbvJ/ONVuHnqzXRKRjjpwT7EAGfWHsSyYzJftZh23Nc/y82GNsJDcd1z7i+aYucN0lIh+15NkjXDHfDUfc853BdbEhMZVXREp6nsWPxdfUJZ6fQob3O3qM9+5AYsph7EVyjSzhvbQF2QMiznUWpzHaXXGe2wzHLRFBX1NgLsq8RExCCCGEEEIIIf4NoIdWIYQQQgghhBCLRQ+tQgghhBBCCCEWix5ahRBCCCGEEEIslq+KmPrhM7SNAQtpmVAnEcFCnrCI3dnp62zCouSeSD6sxc/KciKEiEQKlGPXc4/vdQUpfB7Jvbhp8fJosYi6dFgcXRfkmqSw3AUsfE6kDzWRTkUiv7gS4UBBBFvGT4vhfSRykYpIKYisKmPyJ1KovSSIN8GknMhzdjh2ayaj6rDI3djp/FQ5GRNLxGErHPd4QanF5XKAtjCi1CNrMZ5GgzKEpsHXDatpX5PDMXrZo3Ci3ON99CfMOe0JxUnGE5nUFq9xOuBavJ5RwGDIOvFxOq9ZiXNarVB8Y5kcIsd81ROpzdLISFqwJN+7CvMbjeWA8hgTpmNTNhh3gQhL8oxIckiu9EQyl8heZD1pI2vWEQman+09NRFa1A7zfb/GtZJGErPhCdpGIu0LTMaX4+u6C86DCyjYScV0TFoiD0w1ikoKEjcZiYcQSE5cECTEjCF7WbbDMbAjkZNcSW6YnWUyIjUZrzigxQbntYw4r+OA8ZrO2Dbg0cv0JG/FiDFbbqYx6xPe7zce26rnb6BtdcUb+Xi+g7aDxT6YDMfk6Yxyv6cL5oRIxItx1v++JeIbj0FSgpnNmEgki5HssUujNRgDOdnjVuT87UYchxBxfstZvOQJzxklye2HhPNRYRoDqagxxvQ13tvjmeTjZ9j/4kQOh7MDo0u4v6+ITKrco0DSBjyj+AHvt8/I2Yhs2p7InsiRxxgi/RxnUrGxx7WTRTJf5PkpkLXSEUnll9BfWoUQQgghhBBCLBY9tAohhBBCCCGEWCx6aBVCCCGEEEIIsVj00CqEEEIIIYQQYrF8VcRkExbl5kSAlBwp1DZEikQEA3aYFuZeicTI9vh8XeVEYkKcDp60ZQGL323NpB6kkJjJg2ZF5MTBY4xFQUIiIqYsw0Jw12FhdU2K2XtHJCQDkx0RGQopkI4zWUlBxoNJuHLSh0AEW55IspZEJDIJR4RVTCiVjuS9OYmxfvredsA5zEdcXyP6B0w443vbhHNxzok4ypJYt7h4fPse2uJpKsl5naGU5+EbIm6IGP+tw3W9azB2QkAxTx3e4nUvaGXIAuYYJhzyZpqvsjPe7+2avRH7MCYUJgxE6LI0nCMCJLeHNrvFWLHEd2WJrKOcJWlPJEHWk4AfcAuzJN/1JN/FxOQpJB+TKRpHXGfjOH1hZjAGPJGz1AW2tYHsdyXOQ0EkRjFgnOWGyONqIiMsidzQTMVoZcL1tCbyGzaWVOK07C3AZGTsbPZ1YZUxxmQnzA1ZSdbJLGb7C56TBhI7IZB5JfHviUjncsE521Rkz2Lz41HEcrlOY3ZXYM7+hZwdX1rMJVciO6waFIBuT9jXqr2HtnOLsrzGowRwKPGeT3PhHzkn5SX5+08gc09kOKFd/h7wjJx57IpIgUic1eSs4cmZcZ6OHnvco3tyNhp7cuZpUT7ZEXFUTw7qdY5r2xNpV7PB/L52305+vp5RnmeIjG9VY1saUOyWEelSS9ZAuSZ7JVlniTyj9Tle4zyTuzkiXWpWuD4tEbvZjDx7DL99E9BfWoUQQgghhBBCLBY9tAohhBBCCCGEWCx6aBVCCCGEEEIIsVi+WtNaNvgST/4POvfki2XJF/ey75W3s7falnyZNandYfVgpSW1ZOTLtm3EeoZI6lyLFdZC+Ij/L1/OajjrHb6vSPg7gkC+yNeT2tdA6g1zMibXM/7vvSE1gqsc++9KrCPZzGsISI1jSWoAUsaKi0ltkMG6hSVR7PCei34Lba7FOXMl1lo49mui9fQa+YgvSh2pcbvBNr99hm091iiMAWsIBlLTvamxHmU8kficXeO+wPftr7iufz3gvfUj1iRZUlvcWKyh+PwRa8HGEvv15vX30Lbe4+ddPk6/lN43pC63voW2ROoU04B9KFkR/sIoX+AaKD2ugaYnddcNcQeQRVDH6djEkdQck3pIVkteNWR9snrYkdTwk3srSB26D9jXMKvP70nddEG+zf3oyWeRmtZE6ggDWQPlgGvAk5qp22f4hfbffIdt/n5al2Ud7vWbPdZfkZI+E6/Ea5AvfA94ifFfexynrMVYD2ucC1bDW+fTMe1Izp7HlzHG2Iixw84slqynmJH4L3Bua1Ln2V6J/8BPX3cg62ZzxDh8P36Ctjieoc0UuCaaW5KHiEvC5CQ3PX8Jbd+Seu1f3v5l8nNb4dxk5N42GZ67ukgOAMXya1pvyDkolhgruzM5uxP1TJwf+o0xaRbLHan1LXpyHrF4b4nUHY/E95KT+R4iihjSSGpOyXNFmnkHyh26PeqAOZuFRSLnx564FPIGB3juiDDGmEDWe9pjnevrDHObeXw3+bEl16zIOavOsWNMsZNIrvgS+kurEEIIIYQQQojFoodWIYQQQgghhBCLRQ+tQgghhBBCCCEWix5ahRBCCCGEEEIslq+LmEgh7arEQvRIJBwZ+Vb2SNrm0h5PJCYZ+4Zri0XJkXxxrSXmg2RQalGQwvnRPEDb4Mk35s6K6Z8ROZElXyqeE6kVU7NcO/KF4QFlBcT1YioicaprIpfYYDjYWYgkIi9hJdSBFN83Cfs6MGHTgqgcCqs2rzHWB3RwmJKMFfEDQHwOJQ5e7YhsgKzebsTxjBblTIHdcMK28wUDKgSMJzuTjBXkC8WfzCO0jQlFJcOZCE1GXHP9DUo42g0OMMsdm2coQ3j2GqUJu9X0ur3H+w0DkUpsSd4kecgSsdnSqC2Oy2aP991dcd5qIo5g0glTTccmNETaluE4u5zlbFyf3hI5jSGyGyJYGpjAguRZNxcDOiINIfdmibHIj+Q+iFwjIyK7lsiZdhWO0+0G5/Vmj/uWWU8/L7M4gS420HYl4pP5PP//tmX/7jyPuG9VNxg7vse52BDx5GiIkG0W2iHgNZuE8zqS08JIBJXBEvGkx5vriQApkFiMI7GpxGknPFk3VyLZi+SwkAYcX0NkUpcblODl6JYxAxGFfrcl+8IO+9/MpGjNCT/rYjD+mwY/P1+Rtg9fPYb/3dlvSJxZksuIyI445UxP1sVlnDa6De4BeYV7TN4Ssd8W42KT8H4fLH7epsS82ASco87hGs1moj2W7rKanOXJPmk83lsasa0rMS90ZO9hWfYlEUu6GsepnO1R2RnPY6PBdWwbHKOSCD/N4bfvAcveLYQQQgghhBBC/EOjh1YhhBBCCCGEEItFD61CCCGEEEIIIRaLHlqFEEIIIYQQQiyWr1aAhxMWzh9IZXVdEsnIQEQ0ESuww6zGm8maosdC5ZFITOqEbSmSeyuwaDhELHK2RKYTiDyonwlwjt0FXlMSWUVV42eRrpqWyBU8Ecx4j7KCSIRVuSPCqnADbfVMHDKQmxuImCMnhetU4LHw35u0j3fQ1l1RELDNUQrkiXRljEy8M53HusLPL4jUoq6x2J7JBto1vvdCZCARnWPGlSge6gtioUnT9dS2mDdCizGXLH4WcU4ZW6NswJb4eb7F+62JFOzgf8brnl9j22y+uhOOZetwrecJZQ42I7HOhCYLY3h8C20fTyh2WecoO+ojEfRZlDikWd5KRC5B/F+mIOsp5UQkQd475HgflkhnLJHdRCLnSbP17kguto6ICElebPaYP/Mrjq8xZE0RaVlDFkF1gwvNU9nT9POuTzhGXUXeR/IEcSyaSPa7JdHef4S26/kIbY3D/TOQvdcS4d38SJURGUyyGE+VI0c4tr7IehpIPiIeS1NnZL5rXP9pthYdEVYOCccjFSQASnxvIELJIqDc70zOcVWG/f/r395B2/YW10T3NO1rJLJDH1HgdCTnzowItkLEeV0ascc+Hwaco4K0nTqc33sicYr5dOzzEffVSKSHviJn3o5I+4j/Z7XBvLgrUEbXtXiNesTFMhcDRiKa9CThRSJdCuy8vCayI0PWWYf3W+ASMIfzB3zdSMa4m87/wOxaJK8NOY7R2BKxIYbDF1n2E4MQQgghhBBCiH9o9NAqhBBCCCGEEGKx6KFVCCGEEEIIIcRi0UOrEEIIIYQQQojF8lUR03nA4lri1jCtIVW+RGqRSDG0zbN5A3kNkSl5IifyRPyBte/GWvy8ssQXNhURjpAi8jSTTuVEnBQHLMqODgur+wE/f0PuLSPT529R9lQmfK8n8+oHLN4+z1waaSRF5ES4wWRaOZHpGCLiWRIdKTgviZTgaElRPjHHjCR2ypkQg/hbjBtbaDt3RIDVkQXWkHhtiEyNWFKKNQpBbsm6u/dTacL1SCQ35FdkzXztG2PGHNfczRrjutliPO0xrM11wPg/3eM4ueMJ3+ymgq0hsPgneSPHGykqFDxYInVbGmwNFEQyNrA+E/tFJAIYV0xfVxCBTTL4WT0Rw7mezK0jUiSyVByJx4xsWpnD+J57OSzZAwwRBVY5E7vgfTzfEBkZkV+5iNe1ZP+IJFdcA+axPE1zdE+kJJ7k+zrH/SmVKLpxRBy0JEYyF8WA/e1I/Ccyt4ZI9fJZHsjIPsviKTj8/JzF/wbPHo5svTX5vCrHvFWROD50UxlR32OOcOQsxq7ZEXHSbo3zYCscp+qA83BqUepzwSYTenJum1liElvX7M8/RLLHHIaRHYoXRmL7Xo+5oiMCzvGIsVfsMBDWaZoH+hLPARsifH0aiLApEeErEe81CedoIGe5krzXNeRcPbPKnsk5KJFgcY7tdxgsdYbjGypsq4mM7NQT2ZMn+b7Ds5bx02uMkeRscpYJA7Y5cuYfyOu+hP7SKoQQQgghhBBiseihVQghhBBCCCHEYtFDqxBCCCGEEEKIxaKHViGEEEIIIYQQi+WrIqYsYuHzSIwqGSnMtw6LnCsiLZmLObKEn8+K3y0Rc1is3TZZxHvLMiwkziMW/5dEmhBJMXSqptdwZIwsEQ4kIjRhkgM2UZYUgpeeSB5Iof9A5EwxkWL7WTeYWCIkIlwIeM3QEXkJs/MsiIo4EiIxuKSO2MlqjNlNhkXooZzFf8C5cSV+/nokIpUXKPYyLc5ZQ1wSVUPmrMd7eRoO0BZnooYtEZh5In+ypKC/MWS9kjXhLygRyIhdZJvj2DUR788SaVBaTSUPVYef78hacomIG4jBro/ETLIwSpJniWPMxJbIt2oiqSpITnXT2CC+MpM7nO8qYLwPW7IGiKCvJrmHvNMMRESVLMnRs66icsiYOH+RMYaEncks2e9IqmSen6okLySJrCR9cAHnMM5CviAdc0TOVhKjXCR78eiWvQfMRXnGGBMikUUGErQOX7ciMRBm8+MC5hkmBGtI/mifk5wykPeSs82KiMgMyW8ni/Kb+cetSiJoJCaisUPJYEbEN570v7+QeGrxHJORPaUk+yxxgJpkpyKhnMxfMWK/UkWkoGQsE9nbl0bvUeKTkXOfJbm9eo6DWhYk91bTsamIkLQkeTEn665aE0kSEQAVZE/uEt5vQdZjTOQsUE9jz5/ZmRrf54m01hG5XfLkDN2TM+WIryvI5laS850hTW2croGCnLOyHuc0krNXJM8GxNn3RZa9WwghhBBCCCGE+IdGD61CCCGEEEIIIRaLHlqFEEIIIYQQQiwWPbQKIYQQQgghhFgsNhHBiRBCCCGEEEIIsQT0l1YhhBBCCCGEEItFD61CCCGEEEIIIRaLHlqFEEIIIYQQQiwWPbQKIYQQQgghhFgsemgVQgghhBBCCLFY9NAqhBBCCCGEEGKx6KFVCCGEEEIIIcRi+X/O2CLazexXnQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA ASSEMBLING TO TRAIN AND EVALUATE"
      ],
      "metadata": {
        "id": "4s1Sdz-S2qTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare custom training set\n",
        "total_X_train_2classes=[]\n",
        "class1=custom_train_dataset(0)\n",
        "class2=custom_train_dataset(1)\n",
        "\n",
        "#append real images of class1\n",
        "total_X_train_2classes.append(class1)\n",
        "[total_X_train_2classes] = total_X_train_2classes\n",
        "\n",
        "#normalize and append synthetic data of class1\n",
        "X=X*255\n",
        "X=X.astype('uint8')\n",
        "total_X_train_2classes = np.concatenate((total_X_train_2classes,X), axis=0)\n",
        "\n",
        "\n",
        "#append real images of class2\n",
        "total_X_train_2classes = np.concatenate((total_X_train_2classes,class2), axis=0)\n",
        "\n",
        "#normalize and append synthetic data of class2\n",
        "X1=X1*255\n",
        "X1=X1.astype('uint8')\n",
        "total_X_train_2classes = np.concatenate((total_X_train_2classes,X1), axis=0)\n",
        "\n",
        "#prepare labels set\n",
        "y_train_2classes = np.zeros((2100, 1),dtype=int)\n",
        "\n",
        "for i in range(len(y_train_2classes)):\n",
        "  if i<1050:\n",
        "    y_train_2classes[i][0]=0\n",
        "  else:\n",
        "    y_train_2classes[i][0]=1\n"
      ],
      "metadata": {
        "id": "o8rR_Myr9XQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST AND EVALUATION ON ONLY 50 SAMPLES FROM EACH CLASS. TOTAL: 100 TRAINING SAMPLES, 700 EVAL SAMPLES"
      ],
      "metadata": {
        "id": "IhUvzwQt4AeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "accs = []\n",
        "for seed in range(50):\n",
        "  cnn = keras.Sequential([\n",
        "      layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      \n",
        "      layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      \n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  opt = SGD(lr=0.01, momentum=0.9)\n",
        "  cnn.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  cnn.fit(X_train_2classes_100, y_train_2classes_100, epochs=30)\n",
        "  res=cnn.evaluate(X_test_2classes_700,y_test_2classes_700)\n",
        "  accs.append(res[1])"
      ],
      "metadata": {
        "id": "taCydwvRXcSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb6434e-a50f-4fc5-e54f-7412f4879f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1727 - accuracy: 0.2400\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.3510 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7352 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.2431 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9322 - accuracy: 0.6000\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1378 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9670 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7462 - accuracy: 0.4800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7530 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.6300\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.6200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6747 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6954 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6429 - accuracy: 0.5900\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.6000\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6476 - accuracy: 0.6300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6305 - accuracy: 0.6700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6104 - accuracy: 0.7100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5982 - accuracy: 0.7500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5900\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5594 - accuracy: 0.7300\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5981 - accuracy: 0.6800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.6500\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.7200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5129 - accuracy: 0.7600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5015 - accuracy: 0.7500\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4803 - accuracy: 0.8100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.7800\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5572 - accuracy: 0.7300\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9486 - accuracy: 0.4100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8561 - accuracy: 0.4100\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2403 - accuracy: 0.4700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9799 - accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8867 - accuracy: 0.4700\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7781 - accuracy: 0.4300\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8017 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.6100\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7877 - accuracy: 0.4200\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6967 - accuracy: 0.6000\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7018 - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.5900\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6423 - accuracy: 0.6500\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6958 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6087 - accuracy: 0.7100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7096 - accuracy: 0.4500\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7082 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6769 - accuracy: 0.6800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.7000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6103 - accuracy: 0.7600\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5647 - accuracy: 0.7500\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5236 - accuracy: 0.7500\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5077 - accuracy: 0.7500\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8000\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6783 - accuracy: 0.7100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5029 - accuracy: 0.7500\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7900\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.7600\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7557\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.0294 - accuracy: 0.3800\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9169 - accuracy: 0.4600\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0263 - accuracy: 0.4500\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7318 - accuracy: 0.5100\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.9205 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2913 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2398 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2264 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9040 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8101 - accuracy: 0.5200\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7536 - accuracy: 0.4200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7196 - accuracy: 0.5200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6798 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6817 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7041 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7111 - accuracy: 0.4400\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.6000\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7486 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7712 - accuracy: 0.4200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.5400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6937 - accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6845 - accuracy: 0.5800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6961 - accuracy: 0.5600\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1664 - accuracy: 0.3000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1688 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8007 - accuracy: 0.4200\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6803 - accuracy: 0.5500\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.0813 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9609 - accuracy: 0.5900\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2621 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3323 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1747 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0320 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8719 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7428 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6501 - accuracy: 0.5900\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6550 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6402 - accuracy: 0.5900\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6273 - accuracy: 0.5900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6095 - accuracy: 0.6900\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6207 - accuracy: 0.6200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5111 - accuracy: 0.7600\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4924 - accuracy: 0.7700\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5277 - accuracy: 0.7300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.7600\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6159 - accuracy: 0.7400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4375 - accuracy: 0.8000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4625 - accuracy: 0.7800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.7800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4038 - accuracy: 0.8200\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.8400\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3580 - accuracy: 0.8500\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.7957\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 1s 7ms/step - loss: 2.2760 - accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 1.6244 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7643 - accuracy: 0.5100\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4777 - accuracy: 0.4000\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.9680 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3989 - accuracy: 0.6000\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.9530 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7563 - accuracy: 0.5200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8988 - accuracy: 0.3800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8978 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7441 - accuracy: 0.4100\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7174 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7180 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7159 - accuracy: 0.4700\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.6200\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7701 - accuracy: 0.4200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6600 - accuracy: 0.6000\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.7239 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7012 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6486 - accuracy: 0.6200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6418 - accuracy: 0.7000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 0.7100\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6004 - accuracy: 0.7100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5787 - accuracy: 0.7300\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5583 - accuracy: 0.7300\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7182 - accuracy: 0.5600\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6440 - accuracy: 0.6800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6526 - accuracy: 0.6100\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6187 - accuracy: 0.6814\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 2.1560 - accuracy: 0.1600\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0603 - accuracy: 0.4500\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8066 - accuracy: 0.5200\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.4672 - accuracy: 0.4200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4423 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4593 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4935 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2224 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1088 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8114 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6975 - accuracy: 0.5500\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6981 - accuracy: 0.5700\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7091 - accuracy: 0.5600\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6000\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5288 - accuracy: 0.7800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.8200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5658 - accuracy: 0.7100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4418 - accuracy: 0.7700\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4782 - accuracy: 0.7900\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4176 - accuracy: 0.7900\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5816 - accuracy: 0.7100\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6219 - accuracy: 0.7300\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.6800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5076 - accuracy: 0.7900\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7300\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8300\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3829 - accuracy: 0.8500\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3793 - accuracy: 0.8500\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.4530 - accuracy: 0.7971\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1500 - accuracy: 0.3700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0310 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7223 - accuracy: 0.5600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.3656 - accuracy: 0.4000\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3516 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3500 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2353 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0681 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8624 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7113 - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7711 - accuracy: 0.4300\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8180 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6746 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8316 - accuracy: 0.4200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8311 - accuracy: 0.4200\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6954 - accuracy: 0.5900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7317 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7151 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6801 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6762 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6835 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6778 - accuracy: 0.6500\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.4500\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6553 - accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6410 - accuracy: 0.6000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.7000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6240 - accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6556 - accuracy: 0.5800\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1154 - accuracy: 0.2500\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1000 - accuracy: 0.4700\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.8484 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2126 - accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.7554 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3296 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0777 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8462 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6783 - accuracy: 0.6300\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6900\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7283 - accuracy: 0.3900\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7215 - accuracy: 0.4900\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6525 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7648 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6337 - accuracy: 0.6600\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7059 - accuracy: 0.5300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6149 - accuracy: 0.6900\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6438 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5843 - accuracy: 0.7400\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5480 - accuracy: 0.7400\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5209 - accuracy: 0.7400\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5074 - accuracy: 0.7000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5125 - accuracy: 0.7600\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.7900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8487 - accuracy: 0.5600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6745 - accuracy: 0.5900\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6007 - accuracy: 0.6600\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5965 - accuracy: 0.7800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5172 - accuracy: 0.8000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.6600\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9574 - accuracy: 0.4400\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8249 - accuracy: 0.5700\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8444 - accuracy: 0.5600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.6623 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7000 - accuracy: 0.5500\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.5300\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7711 - accuracy: 0.5900\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8189 - accuracy: 0.6200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.6300\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6767 - accuracy: 0.6500\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8612 - accuracy: 0.5400\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7898 - accuracy: 0.6500\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6606 - accuracy: 0.6000\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6008 - accuracy: 0.7600\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5811 - accuracy: 0.6700\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5398 - accuracy: 0.7800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5336 - accuracy: 0.7600\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4923 - accuracy: 0.7700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.7600\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5353 - accuracy: 0.7700\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5301 - accuracy: 0.7400\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.7800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4508 - accuracy: 0.7900\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4505 - accuracy: 0.7900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5609 - accuracy: 0.7000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4253 - accuracy: 0.8100\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3825 - accuracy: 0.8300\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.7700\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3586 - accuracy: 0.8600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4052 - accuracy: 0.8200\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5626 - accuracy: 0.7314\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2697 - accuracy: 0.1600\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4839 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7273 - accuracy: 0.5700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4202 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6747 - accuracy: 0.4600\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7468 - accuracy: 0.4900\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8543 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7365 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6618 - accuracy: 0.6000\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6424 - accuracy: 0.6300\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6639 - accuracy: 0.6000\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5990 - accuracy: 0.6800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.6400\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7062 - accuracy: 0.6100\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5802 - accuracy: 0.7300\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6073 - accuracy: 0.6700\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7043 - accuracy: 0.5300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5790 - accuracy: 0.7400\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5895 - accuracy: 0.6800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6468 - accuracy: 0.6500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6117 - accuracy: 0.7000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6287 - accuracy: 0.6900\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5728 - accuracy: 0.7600\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6086 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5549 - accuracy: 0.7300\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6273 - accuracy: 0.6500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.7600\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5616 - accuracy: 0.7700\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4987 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5020 - accuracy: 0.7700\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.7357\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.1224 - accuracy: 0.3400\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0409 - accuracy: 0.4700\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9005 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2416 - accuracy: 0.5400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3075 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0842 - accuracy: 0.4900\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9850 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7827 - accuracy: 0.4900\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6602 - accuracy: 0.6100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7188 - accuracy: 0.4300\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6792 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.5600\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6572 - accuracy: 0.6300\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6516 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5790 - accuracy: 0.7200\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5772 - accuracy: 0.7100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5197 - accuracy: 0.7700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5143 - accuracy: 0.7700\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5695 - accuracy: 0.7200\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5207 - accuracy: 0.8000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4892 - accuracy: 0.8000\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4700 - accuracy: 0.7600\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5218 - accuracy: 0.7700\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4766 - accuracy: 0.8000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4574 - accuracy: 0.8200\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4316 - accuracy: 0.8200\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4709 - accuracy: 0.7400\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7839 - accuracy: 0.6300\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5457 - accuracy: 0.8200\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.6229\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1982 - accuracy: 0.2600\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6285 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7260 - accuracy: 0.6200\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2645 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3545 - accuracy: 0.5700\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2723 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0007 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7358 - accuracy: 0.5400\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9045 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7089 - accuracy: 0.5200\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8147 - accuracy: 0.4200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7666 - accuracy: 0.4400\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6952 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7388 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7101 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6672 - accuracy: 0.5900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7333 - accuracy: 0.4200\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7080 - accuracy: 0.4800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6683 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.6800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6549 - accuracy: 0.6600\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6435 - accuracy: 0.6700\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6367 - accuracy: 0.7100\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6311 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6622 - accuracy: 0.5900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6292 - accuracy: 0.6500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5695 - accuracy: 0.7200\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5715 - accuracy: 0.6900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5429 - accuracy: 0.7500\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5454 - accuracy: 0.7500\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5790 - accuracy: 0.7229\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2529 - accuracy: 0.1500\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5598 - accuracy: 0.5300\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6968 - accuracy: 0.5800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.2005 - accuracy: 0.6200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.0259 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1398 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4317 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2033 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0797 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9173 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7676 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7618 - accuracy: 0.4400\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7537 - accuracy: 0.4400\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7229 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6984 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.7300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7099 - accuracy: 0.4400\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6490 - accuracy: 0.6100\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6449 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6313 - accuracy: 0.6600\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.6000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6046 - accuracy: 0.6300\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5783 - accuracy: 0.6500\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5494 - accuracy: 0.7400\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5447 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.8000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5348 - accuracy: 0.7100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6010 - accuracy: 0.6400\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5313 - accuracy: 0.7557\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.1373 - accuracy: 0.2100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0579 - accuracy: 0.6600\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8133 - accuracy: 0.5500\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.4547 - accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0399 - accuracy: 0.5400\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2739 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3851 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1789 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9715 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8341 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6950 - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7223 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7094 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7244 - accuracy: 0.5100\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7219 - accuracy: 0.4700\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7876 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7023 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7036 - accuracy: 0.5600\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6596 - accuracy: 0.6700\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7073 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6815 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6272 - accuracy: 0.6300\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6330 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5970 - accuracy: 0.6800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5985 - accuracy: 0.7600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5603 - accuracy: 0.7900\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5354 - accuracy: 0.7500\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5620 - accuracy: 0.7100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5047 - accuracy: 0.7700\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5135 - accuracy: 0.7486\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.0301 - accuracy: 0.2300\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8041 - accuracy: 0.5300\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1745 - accuracy: 0.5800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0205 - accuracy: 0.5600\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9810 - accuracy: 0.5600\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8899 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9168 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7487 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7333 - accuracy: 0.4600\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6581 - accuracy: 0.6400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7915 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6832 - accuracy: 0.5900\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7105 - accuracy: 0.5500\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6768 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7069 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6337 - accuracy: 0.6600\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6485 - accuracy: 0.6400\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5909 - accuracy: 0.6700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5665 - accuracy: 0.7600\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.7200\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5172 - accuracy: 0.7300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5875 - accuracy: 0.6900\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5574 - accuracy: 0.7300\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5340 - accuracy: 0.7100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4541 - accuracy: 0.8300\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5974 - accuracy: 0.7000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4476 - accuracy: 0.8000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4821 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4398 - accuracy: 0.7800\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.6871\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.1075 - accuracy: 0.3100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0994 - accuracy: 0.5900\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9231 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.2081 - accuracy: 0.5300\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2534 - accuracy: 0.5900\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0635 - accuracy: 0.4400\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8805 - accuracy: 0.4100\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7240 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.6100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6449 - accuracy: 0.6400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6426 - accuracy: 0.6000\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5954 - accuracy: 0.7300\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5801 - accuracy: 0.7100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5964 - accuracy: 0.7100\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5475 - accuracy: 0.7600\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5762 - accuracy: 0.7300\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7105 - accuracy: 0.6100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5717 - accuracy: 0.6800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6285 - accuracy: 0.7000\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5343 - accuracy: 0.7600\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5529 - accuracy: 0.7300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6773 - accuracy: 0.7100\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5722 - accuracy: 0.7000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5938 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4823 - accuracy: 0.7600\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5113 - accuracy: 0.7600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4320 - accuracy: 0.8000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5111 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.6700\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7663 - accuracy: 0.5771\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.1312 - accuracy: 0.3400\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1012 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2479 - accuracy: 0.4000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6578 - accuracy: 0.6900\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3766 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1874 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0946 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7219 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7489 - accuracy: 0.4800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1369 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9374 - accuracy: 0.4200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8200 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7991 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7408 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7337 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6796 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6792 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6734 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6846 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6696 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7210 - accuracy: 0.4200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6591 - accuracy: 0.6400\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6512 - accuracy: 0.6400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.4300\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7090 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6385 - accuracy: 0.6600\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6104 - accuracy: 0.6900\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6743 - accuracy: 0.5800\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7365 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 2.1698 - accuracy: 0.3900\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0958 - accuracy: 0.4500\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7334 - accuracy: 0.4600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0706 - accuracy: 0.4800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9293 - accuracy: 0.4800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8038 - accuracy: 0.5300\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1883 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2495 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0330 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8859 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7296 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6644 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7240 - accuracy: 0.4500\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7156 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7854 - accuracy: 0.4000\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7254 - accuracy: 0.4700\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6728 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7605 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7186 - accuracy: 0.6300\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6619 - accuracy: 0.7200\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6337 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6223 - accuracy: 0.6500\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5968 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6007 - accuracy: 0.6600\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.6600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6778 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5828 - accuracy: 0.6500\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6103 - accuracy: 0.6200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5471 - accuracy: 0.7400\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 0.6229\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.0981 - accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.2120 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8944 - accuracy: 0.4600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0003 - accuracy: 0.4200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8501 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9147 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7743 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6761 - accuracy: 0.6300\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6595 - accuracy: 0.6800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9260 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7836 - accuracy: 0.4900\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8268 - accuracy: 0.4300\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7123 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6736 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6563 - accuracy: 0.6400\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6179 - accuracy: 0.6000\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6788 - accuracy: 0.5600\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5940 - accuracy: 0.6800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5511 - accuracy: 0.7500\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5984 - accuracy: 0.7100\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4979 - accuracy: 0.7800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4732 - accuracy: 0.8000\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4734 - accuracy: 0.7900\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4909 - accuracy: 0.7900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5216 - accuracy: 0.7100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6786 - accuracy: 0.6800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5367 - accuracy: 0.7700\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4758 - accuracy: 0.8100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4604 - accuracy: 0.8100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.8200\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.7229\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.2267 - accuracy: 0.2700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2580 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.6400\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4136 - accuracy: 0.3900\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8875 - accuracy: 0.5500\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6795 - accuracy: 0.5600\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8727 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7696 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6543 - accuracy: 0.6600\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7323 - accuracy: 0.5600\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6485 - accuracy: 0.6300\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6436 - accuracy: 0.6000\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5936 - accuracy: 0.6700\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6118 - accuracy: 0.6600\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5831 - accuracy: 0.7300\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5775 - accuracy: 0.7100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5931 - accuracy: 0.7000\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6372 - accuracy: 0.6600\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5172 - accuracy: 0.7500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5066 - accuracy: 0.7800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5453 - accuracy: 0.6900\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5728 - accuracy: 0.7100\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5572 - accuracy: 0.7400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5067 - accuracy: 0.7500\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5186 - accuracy: 0.7300\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4751 - accuracy: 0.7800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4628 - accuracy: 0.8000\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.7900\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5456 - accuracy: 0.7357\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.2708 - accuracy: 0.1500\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4779 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8704 - accuracy: 0.4700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6975 - accuracy: 0.6800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6304 - accuracy: 0.6500\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8609 - accuracy: 0.4800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9442 - accuracy: 0.6000\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9168 - accuracy: 0.4200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8343 - accuracy: 0.4200\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6057 - accuracy: 0.7700\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7302 - accuracy: 0.6300\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6565 - accuracy: 0.6200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8298 - accuracy: 0.4100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6866 - accuracy: 0.6000\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7458 - accuracy: 0.5900\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6180 - accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5507 - accuracy: 0.7300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6681 - accuracy: 0.6400\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.6500\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7158 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5486 - accuracy: 0.8000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5639 - accuracy: 0.7100\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5139 - accuracy: 0.7800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5124 - accuracy: 0.7900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.7300\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4624 - accuracy: 0.8200\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4452 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4343 - accuracy: 0.7900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4445 - accuracy: 0.7800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5967 - accuracy: 0.7071\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.0697 - accuracy: 0.3300\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8242 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0905 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2052 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1138 - accuracy: 0.5600\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3421 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4104 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2706 - accuracy: 0.4100\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0189 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8756 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.6200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9715 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7866 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8455 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7883 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7525 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7317 - accuracy: 0.5200\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7624 - accuracy: 0.4200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7329 - accuracy: 0.4200\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7022 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7122 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7126 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7343 - accuracy: 0.4200\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6834 - accuracy: 0.5200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7391 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7433 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6764 - accuracy: 0.5800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7479 - accuracy: 0.4200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7638 - accuracy: 0.4200\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7083 - accuracy: 0.5500\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1663 - accuracy: 0.2700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2011 - accuracy: 0.5900\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9200 - accuracy: 0.5300\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7083 - accuracy: 0.6100\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6347 - accuracy: 0.6700\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9056 - accuracy: 0.5500\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7101 - accuracy: 0.6200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7632 - accuracy: 0.6100\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6465 - accuracy: 0.7300\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5567 - accuracy: 0.7000\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5434 - accuracy: 0.7600\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1080 - accuracy: 0.6200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7393 - accuracy: 0.5100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7222 - accuracy: 0.7300\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5570 - accuracy: 0.7100\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5147 - accuracy: 0.7500\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5378 - accuracy: 0.7300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0187 - accuracy: 0.5900\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6980 - accuracy: 0.6100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6483 - accuracy: 0.7100\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5431 - accuracy: 0.7400\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4808 - accuracy: 0.7600\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4912 - accuracy: 0.7600\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.6700\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6875 - accuracy: 0.6600\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5507 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4771 - accuracy: 0.7800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5525 - accuracy: 0.7300\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4746 - accuracy: 0.8000\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5217 - accuracy: 0.7600\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6752 - accuracy: 0.6543\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 1s 7ms/step - loss: 2.1509 - accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0028 - accuracy: 0.4500\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.6500\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9825 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7727 - accuracy: 0.6200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1367 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9868 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7922 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8243 - accuracy: 0.4200\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6983 - accuracy: 0.5400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8175 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.6300\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8836 - accuracy: 0.4200\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8667 - accuracy: 0.4200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7405 - accuracy: 0.5500\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6844 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6547 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6417 - accuracy: 0.6400\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6295 - accuracy: 0.7300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5945 - accuracy: 0.7400\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5713 - accuracy: 0.7700\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5538 - accuracy: 0.7300\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5531 - accuracy: 0.6900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6876 - accuracy: 0.6300\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5911 - accuracy: 0.6800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5295 - accuracy: 0.7400\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.7700\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.6814\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 2.1226 - accuracy: 0.2900\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0205 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7329 - accuracy: 0.4600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3393 - accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.9460 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2826 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0861 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7023 - accuracy: 0.5500\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8390 - accuracy: 0.4400\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6732 - accuracy: 0.5400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8179 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8190 - accuracy: 0.4500\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8359 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8745 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8712 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7192 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6847 - accuracy: 0.5300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.6600\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7177 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5700\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6342 - accuracy: 0.5900\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5567 - accuracy: 0.7800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6460 - accuracy: 0.6100\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5438 - accuracy: 0.7200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5243 - accuracy: 0.7400\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4912 - accuracy: 0.7800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4870 - accuracy: 0.7800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4913 - accuracy: 0.7400\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4808 - accuracy: 0.7800\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.6757\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1379 - accuracy: 0.2800\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1333 - accuracy: 0.4200\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7668 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2509 - accuracy: 0.4200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9862 - accuracy: 0.5600\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3048 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1579 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7738 - accuracy: 0.4400\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8201 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7793 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7205 - accuracy: 0.5600\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7247 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6900 - accuracy: 0.5100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7478 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7695 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5700\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.6000\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6793 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5100\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7007 - accuracy: 0.4300\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.6200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6712 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.6200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6777 - accuracy: 0.5400\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6536 - accuracy: 0.7100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6566 - accuracy: 0.5900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6428 - accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6191 - accuracy: 0.7400\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6406 - accuracy: 0.5743\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.0284 - accuracy: 0.5700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1558 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8150 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9986 - accuracy: 0.3800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7951 - accuracy: 0.5400\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9138 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7731 - accuracy: 0.6200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8127 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7045 - accuracy: 0.5400\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6487 - accuracy: 0.5700\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6687 - accuracy: 0.5900\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6813 - accuracy: 0.5200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5881 - accuracy: 0.7100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7456 - accuracy: 0.5900\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6549 - accuracy: 0.6600\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6216 - accuracy: 0.7100\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5862 - accuracy: 0.6900\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5323 - accuracy: 0.7700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5428 - accuracy: 0.7400\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5748 - accuracy: 0.6900\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4788 - accuracy: 0.7600\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4790 - accuracy: 0.7900\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5943 - accuracy: 0.6700\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.7900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4585 - accuracy: 0.8100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.8100\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5251 - accuracy: 0.7700\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.6700\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5057 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4654 - accuracy: 0.8300\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.7343\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.1722 - accuracy: 0.1900\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0836 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9902 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7220 - accuracy: 0.5400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1299 - accuracy: 0.4800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9840 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3954 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7414 - accuracy: 0.5700\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6879 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7852 - accuracy: 0.4200\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0206 - accuracy: 0.4800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4662 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8460 - accuracy: 0.4200\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9636 - accuracy: 0.4200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8071 - accuracy: 0.4100\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7320 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.5400\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8548 - accuracy: 0.4200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6494 - accuracy: 0.6300\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8269 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7162 - accuracy: 0.5300\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7923 - accuracy: 0.4200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7286 - accuracy: 0.5100\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7172 - accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6980 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6710 - accuracy: 0.5900\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6709 - accuracy: 0.6900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6643 - accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.5700\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.7200\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.3493 - accuracy: 0.2100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6407 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9196 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7902 - accuracy: 0.6100\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8928 - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3116 - accuracy: 0.5600\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0539 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9709 - accuracy: 0.4200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6874 - accuracy: 0.6200\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8442 - accuracy: 0.4200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7507 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8023 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7265 - accuracy: 0.6900\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6376 - accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6519 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6775 - accuracy: 0.5300\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6227 - accuracy: 0.6400\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5731 - accuracy: 0.7400\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5782 - accuracy: 0.6800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5209 - accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5554 - accuracy: 0.6900\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5911 - accuracy: 0.6800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7141 - accuracy: 0.6200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5637 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5722 - accuracy: 0.7300\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5175 - accuracy: 0.8100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5063 - accuracy: 0.7800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4772 - accuracy: 0.7900\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5369 - accuracy: 0.7343\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1533 - accuracy: 0.3000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9165 - accuracy: 0.5700\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1365 - accuracy: 0.5400\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0024 - accuracy: 0.5800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8534 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7887 - accuracy: 0.5300\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9007 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7658 - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8239 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6710 - accuracy: 0.6300\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6809 - accuracy: 0.5500\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6458 - accuracy: 0.7000\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6764 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6497 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6109 - accuracy: 0.6800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6358 - accuracy: 0.6100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8220 - accuracy: 0.4500\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7616 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7364 - accuracy: 0.6000\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.6500\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5652 - accuracy: 0.7700\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7922 - accuracy: 0.5800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7560 - accuracy: 0.5400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6077 - accuracy: 0.6900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6430 - accuracy: 0.6700\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5479 - accuracy: 0.7100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5486 - accuracy: 0.7400\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4891 - accuracy: 0.7500\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.7200\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7414\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.0133 - accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0487 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0033 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7295 - accuracy: 0.5900\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6284 - accuracy: 0.6300\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6300 - accuracy: 0.6800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6402 - accuracy: 0.6700\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5886 - accuracy: 0.7100\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5495 - accuracy: 0.7300\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5344 - accuracy: 0.7700\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5883 - accuracy: 0.6600\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5407 - accuracy: 0.7100\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4972 - accuracy: 0.7600\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6034 - accuracy: 0.7200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5103 - accuracy: 0.7100\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5113 - accuracy: 0.7500\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8200\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4339 - accuracy: 0.8000\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.7700\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6142 - accuracy: 0.6900\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4509 - accuracy: 0.8300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4127 - accuracy: 0.8200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4362 - accuracy: 0.7800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8300\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.7900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7613 - accuracy: 0.6600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6978 - accuracy: 0.6700\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7438 - accuracy: 0.7300\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6433 - accuracy: 0.6200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5790 - accuracy: 0.7000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5086 - accuracy: 0.7571\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1815 - accuracy: 0.1700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3009 - accuracy: 0.5500\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6703 - accuracy: 0.6700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1475 - accuracy: 0.4800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2290 - accuracy: 0.4800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0534 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0468 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8159 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7227 - accuracy: 0.4600\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7341 - accuracy: 0.4100\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.6500\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.4700\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6590 - accuracy: 0.6400\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6548 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6411 - accuracy: 0.7700\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6362 - accuracy: 0.6900\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6167 - accuracy: 0.6700\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6037 - accuracy: 0.6500\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5685 - accuracy: 0.7500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5426 - accuracy: 0.7100\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5809 - accuracy: 0.6600\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5608 - accuracy: 0.7400\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5910 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5414 - accuracy: 0.7500\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5013 - accuracy: 0.7300\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5186 - accuracy: 0.7300\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4727 - accuracy: 0.8100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4878 - accuracy: 0.7700\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5657 - accuracy: 0.7300\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.6814\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.9526 - accuracy: 0.4400\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8754 - accuracy: 0.5500\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7681 - accuracy: 0.5800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6476 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.8953 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5910 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9145 - accuracy: 0.4800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7390 - accuracy: 0.5900\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7458 - accuracy: 0.5100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6603 - accuracy: 0.5700\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9139 - accuracy: 0.5600\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7630 - accuracy: 0.4700\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8202 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7171 - accuracy: 0.6000\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.7200\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.7500\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6160 - accuracy: 0.7400\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6006 - accuracy: 0.6900\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6045 - accuracy: 0.6600\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5443 - accuracy: 0.7800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6839 - accuracy: 0.6200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8328 - accuracy: 0.6300\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6384 - accuracy: 0.6400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5448 - accuracy: 0.7500\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5125 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5857 - accuracy: 0.6900\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5213 - accuracy: 0.7500\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5063 - accuracy: 0.7900\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.7900\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5631 - accuracy: 0.7157\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 2.3040 - accuracy: 0.1800\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4434 - accuracy: 0.5700\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7081 - accuracy: 0.6700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1432 - accuracy: 0.5400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9227 - accuracy: 0.5300\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0984 - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9167 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7570 - accuracy: 0.6000\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6208 - accuracy: 0.6700\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6602 - accuracy: 0.6600\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.2474 - accuracy: 0.5200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8522 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8876 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7305 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.6700\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5784 - accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5453 - accuracy: 0.7400\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.6100\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5640 - accuracy: 0.6900\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5138 - accuracy: 0.7800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.7000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5630 - accuracy: 0.7100\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6188 - accuracy: 0.6700\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5861 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.7700\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4941 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4890 - accuracy: 0.8000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4405 - accuracy: 0.7900\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4459 - accuracy: 0.7600\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.7300\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.2490 - accuracy: 0.3700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5011 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8979 - accuracy: 0.6300\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8383 - accuracy: 0.5600\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6537 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8699 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7835 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8339 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7102 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6630 - accuracy: 0.6200\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6888 - accuracy: 0.6200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6435 - accuracy: 0.6100\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6834 - accuracy: 0.6100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6329 - accuracy: 0.6000\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6496 - accuracy: 0.6500\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6561 - accuracy: 0.6000\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5608 - accuracy: 0.7100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5099 - accuracy: 0.7400\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5204 - accuracy: 0.7700\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5851 - accuracy: 0.6600\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5126 - accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5434 - accuracy: 0.7700\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4901 - accuracy: 0.7800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5016 - accuracy: 0.7400\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.8100\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.8100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4752 - accuracy: 0.7800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5478 - accuracy: 0.7200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.6800\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5416 - accuracy: 0.7257\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.0950 - accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1176 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9120 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2917 - accuracy: 0.6200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2299 - accuracy: 0.4200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0650 - accuracy: 0.5500\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6977 - accuracy: 0.4900\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8664 - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8018 - accuracy: 0.4600\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7719 - accuracy: 0.4800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6742 - accuracy: 0.5400\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6746 - accuracy: 0.6400\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6496 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6456 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6705 - accuracy: 0.5900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.5900\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.7000\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7354 - accuracy: 0.5100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6133 - accuracy: 0.6600\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5857 - accuracy: 0.6900\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5989 - accuracy: 0.7200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.7500\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.8000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4816 - accuracy: 0.7200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4923 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4730 - accuracy: 0.7700\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5024 - accuracy: 0.7400\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4909 - accuracy: 0.8000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.6957\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.2471 - accuracy: 0.3200\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3700 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8043 - accuracy: 0.4000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1140 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0921 - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4645 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3503 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9067 - accuracy: 0.3800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8287 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7715 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7310 - accuracy: 0.4500\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7460 - accuracy: 0.4400\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7282 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8319 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7644 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7109 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.6400\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6879 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6608 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.5900\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 0.6200\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6396 - accuracy: 0.6000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6192 - accuracy: 0.7500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6717 - accuracy: 0.5500\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6041 - accuracy: 0.7400\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6517 - accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6478 - accuracy: 0.6100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6103 - accuracy: 0.7114\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 2.2253 - accuracy: 0.1800\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9494 - accuracy: 0.4600\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8781 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.3336 - accuracy: 0.4800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2070 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2592 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0098 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8407 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7299 - accuracy: 0.4400\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5900\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7101 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7466 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6702 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6586 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6394 - accuracy: 0.6200\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6480 - accuracy: 0.7300\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6047 - accuracy: 0.7800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6526 - accuracy: 0.5900\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5587 - accuracy: 0.7600\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5948 - accuracy: 0.6500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5981 - accuracy: 0.6600\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5709 - accuracy: 0.6700\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4902 - accuracy: 0.8000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.7700\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.7000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.7800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.8200\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4095 - accuracy: 0.7900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5030 - accuracy: 0.7700\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.6900\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.8057\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.0862 - accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1046 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6668 - accuracy: 0.5600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.8248 - accuracy: 0.4400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4948 - accuracy: 0.5600\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6889 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2758 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9737 - accuracy: 0.4400\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8572 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6786 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8476 - accuracy: 0.4200\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7445 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7605 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7095 - accuracy: 0.6500\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6989 - accuracy: 0.7100\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6679 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7114 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6631 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6798 - accuracy: 0.5100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6308 - accuracy: 0.5900\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6073 - accuracy: 0.6700\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6396 - accuracy: 0.6400\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5616 - accuracy: 0.7800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5448 - accuracy: 0.7400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5433 - accuracy: 0.7400\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5118 - accuracy: 0.7600\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5600 - accuracy: 0.7100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.7800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5685 - accuracy: 0.7200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7500\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.7114\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.2442 - accuracy: 0.1900\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4473 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7993 - accuracy: 0.5700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.3302 - accuracy: 0.4200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 3.0683 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.5831 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8910 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8805 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6970 - accuracy: 0.5500\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7848 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6991 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7256 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7269 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6889 - accuracy: 0.5900\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7258 - accuracy: 0.4400\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6742 - accuracy: 0.6100\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7334 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7050 - accuracy: 0.5100\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6842 - accuracy: 0.5200\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6973 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7040 - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6633 - accuracy: 0.6000\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7050 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6382 - accuracy: 0.6600\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6271 - accuracy: 0.6500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6419 - accuracy: 0.5900\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7021 - accuracy: 0.5100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6851 - accuracy: 0.6000\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6838 - accuracy: 0.5900\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.6714\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.1411 - accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1054 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0814 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0268 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9773 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9829 - accuracy: 0.5900\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6777 - accuracy: 0.6400\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7628 - accuracy: 0.5600\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6092 - accuracy: 0.6100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6155 - accuracy: 0.7100\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5872 - accuracy: 0.6800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6603 - accuracy: 0.6500\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5622 - accuracy: 0.7200\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5780 - accuracy: 0.7100\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6214 - accuracy: 0.6800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9127 - accuracy: 0.5300\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8781 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8035 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6531 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6866 - accuracy: 0.5500\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6045 - accuracy: 0.6400\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6107 - accuracy: 0.6500\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5953 - accuracy: 0.6800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4990 - accuracy: 0.7800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4897 - accuracy: 0.8200\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4905 - accuracy: 0.8000\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.8000\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.8100\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5092 - accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5216 - accuracy: 0.7500\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7686\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.3315 - accuracy: 0.1800\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6743 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9022 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7074 - accuracy: 0.5400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.7447 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6827 - accuracy: 0.5900\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9346 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8113 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6714 - accuracy: 0.6000\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7474 - accuracy: 0.4700\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6563 - accuracy: 0.6000\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6619 - accuracy: 0.6000\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6312 - accuracy: 0.5900\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6102 - accuracy: 0.6500\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5719 - accuracy: 0.7700\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6946 - accuracy: 0.6100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.6100\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6097 - accuracy: 0.7000\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5597 - accuracy: 0.7600\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6623 - accuracy: 0.6300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5581 - accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5473 - accuracy: 0.7200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4953 - accuracy: 0.7900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4790 - accuracy: 0.8000\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5200 - accuracy: 0.7100\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5795 - accuracy: 0.7100\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.7700\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.8000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5400 - accuracy: 0.7543\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.1497 - accuracy: 0.4100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1419 - accuracy: 0.6400\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8199 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.4243 - accuracy: 0.4800\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.2665 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.3813 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7774 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7315 - accuracy: 0.4500\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8005 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7915 - accuracy: 0.4400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9427 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6770 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6951 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6877 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6839 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6685 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7678 - accuracy: 0.4800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5300\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7598 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7397 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7160 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.7000\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6279 - accuracy: 0.7200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6331 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5905 - accuracy: 0.7400\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5657 - accuracy: 0.7200\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9287 - accuracy: 0.5900\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7255 - accuracy: 0.5100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5915 - accuracy: 0.7400\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.5729\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 2.0344 - accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9431 - accuracy: 0.4000\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9633 - accuracy: 0.5400\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7393 - accuracy: 0.5700\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2427 - accuracy: 0.5800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7219 - accuracy: 0.5600\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9473 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7594 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6734 - accuracy: 0.5200\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7823 - accuracy: 0.6000\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5700\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6636 - accuracy: 0.6000\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5947 - accuracy: 0.6800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6311 - accuracy: 0.6300\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5665 - accuracy: 0.7400\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5950 - accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5549 - accuracy: 0.7000\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.6900\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5823 - accuracy: 0.7100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5550 - accuracy: 0.7200\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4871 - accuracy: 0.7700\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5170 - accuracy: 0.7200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5219 - accuracy: 0.7400\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5602 - accuracy: 0.7100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4505 - accuracy: 0.7800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.8200\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5576 - accuracy: 0.7000\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6062 - accuracy: 0.7100\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4398 - accuracy: 0.8100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5472 - accuracy: 0.7514\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.2128 - accuracy: 0.1700\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0862 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5800\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9193 - accuracy: 0.6400\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9476 - accuracy: 0.4500\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1400 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1170 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9288 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7871 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7148 - accuracy: 0.4400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7472 - accuracy: 0.4000\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6782 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7073 - accuracy: 0.5800\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7234 - accuracy: 0.4400\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6800 - accuracy: 0.6000\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7258 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7130 - accuracy: 0.4100\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7782 - accuracy: 0.4200\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6974 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6791 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7249 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6614 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6671 - accuracy: 0.7200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6449 - accuracy: 0.7500\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6324 - accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6505 - accuracy: 0.5900\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5894 - accuracy: 0.6800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6054 - accuracy: 0.5800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6601 - accuracy: 0.5700\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5439 - accuracy: 0.7800\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6405 - accuracy: 0.6171\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2047 - accuracy: 0.2900\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2890 - accuracy: 0.5100\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8602 - accuracy: 0.4900\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2431 - accuracy: 0.4100\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8515 - accuracy: 0.6000\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9508 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9168 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.4600\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6651 - accuracy: 0.6100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6332 - accuracy: 0.6000\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7833 - accuracy: 0.4700\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7999 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6849 - accuracy: 0.6700\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7158 - accuracy: 0.5200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6646 - accuracy: 0.6600\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5925 - accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5538 - accuracy: 0.7200\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6458 - accuracy: 0.6500\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5517 - accuracy: 0.7100\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5785 - accuracy: 0.7600\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7710 - accuracy: 0.6000\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5527 - accuracy: 0.7200\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6044 - accuracy: 0.7300\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6087 - accuracy: 0.6900\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5350 - accuracy: 0.7300\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5960 - accuracy: 0.7100\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5422 - accuracy: 0.7700\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.7300\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4700 - accuracy: 0.8200\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4556 - accuracy: 0.7700\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6125 - accuracy: 0.7043\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.3091 - accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5792 - accuracy: 0.5900\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6970 - accuracy: 0.5600\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3437 - accuracy: 0.5700\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9265 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1108 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2962 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8739 - accuracy: 0.5800\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8131 - accuracy: 0.5800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6789 - accuracy: 0.5400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6971 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5800\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7323 - accuracy: 0.4200\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6858 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6599 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7416 - accuracy: 0.4200\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6889 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6704 - accuracy: 0.6400\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6721 - accuracy: 0.7300\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6663 - accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6274 - accuracy: 0.6000\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6189 - accuracy: 0.7900\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6982 - accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.5700\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6873 - accuracy: 0.5500\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6318 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5909 - accuracy: 0.6800\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5368 - accuracy: 0.8000\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.7500\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5393 - accuracy: 0.7343\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1705 - accuracy: 0.2600\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3016 - accuracy: 0.6300\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6948 - accuracy: 0.5300\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2018 - accuracy: 0.5500\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.0408 - accuracy: 0.6200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2314 - accuracy: 0.4200\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3703 - accuracy: 0.4200\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0082 - accuracy: 0.4200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7409 - accuracy: 0.4800\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9079 - accuracy: 0.5800\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7584 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7429 - accuracy: 0.4000\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.5200\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7246 - accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7135 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6765 - accuracy: 0.5800\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6940 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6970 - accuracy: 0.5800\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6699 - accuracy: 0.5800\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.6300\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6767 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.5800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6575 - accuracy: 0.7000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6829 - accuracy: 0.5100\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6461 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6369 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6301 - accuracy: 0.7500\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6167 - accuracy: 0.7000\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6076 - accuracy: 0.6800\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7131 - accuracy: 0.4886\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.9786 - accuracy: 0.4300\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9696 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7759 - accuracy: 0.4700\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7332 - accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9014 - accuracy: 0.4800\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9556 - accuracy: 0.5600\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7870 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7839 - accuracy: 0.5700\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6523 - accuracy: 0.6100\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.6300\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3319 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9842 - accuracy: 0.4200\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1386 - accuracy: 0.3900\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7986 - accuracy: 0.5200\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6774 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.5800\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6429 - accuracy: 0.6400\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9316 - accuracy: 0.5500\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7111 - accuracy: 0.5200\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6026 - accuracy: 0.7000\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8553 - accuracy: 0.4700\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8705 - accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8469 - accuracy: 0.4800\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6779 - accuracy: 0.5600\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7680 - accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7604 - accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7137 - accuracy: 0.4600\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7084 - accuracy: 0.4300\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6399 - accuracy: 0.6400\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8261 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.1865 - accuracy: 0.3100\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3349 - accuracy: 0.5800\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7267 - accuracy: 0.5200\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2913 - accuracy: 0.5500\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3628 - accuracy: 0.5200\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2554 - accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9250 - accuracy: 0.5800\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6471 - accuracy: 0.6200\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7461 - accuracy: 0.5400\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0060 - accuracy: 0.5400\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9975 - accuracy: 0.5800\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7017 - accuracy: 0.6900\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7336 - accuracy: 0.7100\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.7700\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6517 - accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6583 - accuracy: 0.6200\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6240 - accuracy: 0.7300\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6664 - accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.4900\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5795 - accuracy: 0.7700\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5663 - accuracy: 0.7200\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5394 - accuracy: 0.7700\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.7200\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5028 - accuracy: 0.8000\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4967 - accuracy: 0.7900\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.7700\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5880 - accuracy: 0.7500\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4638 - accuracy: 0.7700\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6103 - accuracy: 0.7300\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7154 - accuracy: 0.6500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7297 - accuracy: 0.6086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accs = np.array(accs)\n",
        "print('Acc over 50 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZINQFwXZPfNB",
        "outputId": "00d4d3c2-649e-41b6-cdc7-01f1a9616156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc over 50 instances: 0.68 +- 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "accs = []\n",
        "for seed in range(50):\n",
        "  cnn = keras.Sequential([\n",
        "      layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      \n",
        "      layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      \n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  opt = SGD(lr=0.01, momentum=0.9)\n",
        "  cnn.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  cnn.fit(total_X_train_2classes, y_train_2classes, epochs=30)\n",
        "  res=cnn.evaluate(X_test_2classes_700,y_test_2classes_700)\n",
        "  accs.append(res[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyzOeKVqc4tQ",
        "outputId": "994e421f-39e4-4239-ffb0-380a8f31d365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.4957\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4981\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.4886                              \n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4933\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.4976\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 12905956352.0000 - accuracy: 0.5043\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.7213 - accuracy: 0.4962\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.4971\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5067\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7005 - accuracy: 0.5143\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.5019\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6979 - accuracy: 0.5038\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.5048\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7003 - accuracy: 0.4724\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6987 - accuracy: 0.4781\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6963 - accuracy: 0.4848\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6973 - accuracy: 0.5067\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.5057\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6972 - accuracy: 0.4990\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6955 - accuracy: 0.5114\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6961 - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4924\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.5124\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.4952\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6963 - accuracy: 0.4781\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.4981\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4876\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.4962\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6965 - accuracy: 0.4971\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6989 - accuracy: 0.5057\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6960 - accuracy: 0.4876\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6951 - accuracy: 0.4867\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6971 - accuracy: 0.4981\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6962 - accuracy: 0.4990\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.4952\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.4929\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.5105\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4890\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 3ms/step - loss: 27978737056446515863617536.0000 - accuracy: 0.4995\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.0395 - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8283 - accuracy: 0.4876\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7657 - accuracy: 0.5143\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7378 - accuracy: 0.4924\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7239 - accuracy: 0.4848\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.4933\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.7084 - accuracy: 0.5124\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7067 - accuracy: 0.5067\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4962\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.4952\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4790\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7013 - accuracy: 0.4990\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6993 - accuracy: 0.4981\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6991 - accuracy: 0.4943\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6984 - accuracy: 0.4790\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6987 - accuracy: 0.4971\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6982 - accuracy: 0.4829\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4876\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.5057\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.4790\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.4819\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.4895\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6976 - accuracy: 0.5057\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.4981\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6960 - accuracy: 0.5057\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6967 - accuracy: 0.4990\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6958 - accuracy: 0.4943\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.4981\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7008 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 9094104064.0000 - accuracy: 0.5100\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.9764 - accuracy: 0.4876\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.8274 - accuracy: 0.4971\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7808 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7579 - accuracy: 0.4867\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7445 - accuracy: 0.4943\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7358 - accuracy: 0.5038\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7298 - accuracy: 0.5029\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7250 - accuracy: 0.4905\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.4962\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7189 - accuracy: 0.4857\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7164 - accuracy: 0.4924\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.5019\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7135 - accuracy: 0.4848\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7121 - accuracy: 0.4857\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7104 - accuracy: 0.4829\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.7099 - accuracy: 0.4857\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7079 - accuracy: 0.5057\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.4914\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.4743\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.4876\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7051 - accuracy: 0.4962\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4857\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4810\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7041 - accuracy: 0.4990\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7028 - accuracy: 0.5057\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7028 - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4971\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: 0.7021 - accuracy: 0.4819\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7008 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.4967\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4910\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 15393635.0000 - accuracy: 0.4914\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9542 - accuracy: 0.5048\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8216 - accuracy: 0.5114\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7788 - accuracy: 0.5057\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7566 - accuracy: 0.5010\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7440 - accuracy: 0.4924\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7355 - accuracy: 0.4857\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.4886\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7250 - accuracy: 0.4971\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.4771\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7183 - accuracy: 0.4829\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7165 - accuracy: 0.4933\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7148 - accuracy: 0.5057\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7129 - accuracy: 0.4895\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7118 - accuracy: 0.4810\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7101 - accuracy: 0.4981\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7088 - accuracy: 0.4943\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7080 - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7072 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7067 - accuracy: 0.4924\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.4962\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7053 - accuracy: 0.4962\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7050 - accuracy: 0.4886\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.4952\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7041 - accuracy: 0.5010\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.5029\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.4924\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.5038\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7018 - accuracy: 0.5114\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7037 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4743                                 \n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 993624971411319650666846093312.0000 - accuracy: 0.4895\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9618 - accuracy: 0.4943\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8243 - accuracy: 0.4876\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7790 - accuracy: 0.4981\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7575 - accuracy: 0.4714\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7439 - accuracy: 0.4990\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7356 - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7295 - accuracy: 0.4819\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7246 - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7215 - accuracy: 0.4990\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7184 - accuracy: 0.4829\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7159 - accuracy: 0.4933\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7150 - accuracy: 0.4848\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7125 - accuracy: 0.4990\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7111 - accuracy: 0.4971\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7101 - accuracy: 0.4943\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4838\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.4924\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7072 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.4895\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.5019\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7051 - accuracy: 0.4990\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.5019\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.5048\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.5010\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.5048\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4838\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4924\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7021 - accuracy: 0.4924\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7017 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 10321455399026294784.0000 - accuracy: 0.4852\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9664 - accuracy: 0.5038\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8249 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7796 - accuracy: 0.4867\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7575 - accuracy: 0.4771\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7442 - accuracy: 0.4857\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7354 - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7293 - accuracy: 0.4981\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7249 - accuracy: 0.4952\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7216 - accuracy: 0.4895\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7185 - accuracy: 0.4933\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7163 - accuracy: 0.4914\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7145 - accuracy: 0.5010\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7125 - accuracy: 0.5067\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7117 - accuracy: 0.4914\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7100 - accuracy: 0.4990\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4971\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7085 - accuracy: 0.4971\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7069 - accuracy: 0.5114\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.5010\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.5019\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.4990\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7048 - accuracy: 0.4752\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4895\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.4990\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7037 - accuracy: 0.4876\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4876\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5057\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.4829\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7011 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 16378921.0000 - accuracy: 0.4824\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9488 - accuracy: 0.4933\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8201 - accuracy: 0.5019\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7772 - accuracy: 0.4986\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7560 - accuracy: 0.4914\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7435 - accuracy: 0.4862\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7350 - accuracy: 0.4929\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7289 - accuracy: 0.4862\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7245 - accuracy: 0.4829\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7207 - accuracy: 0.4848\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7182 - accuracy: 0.4871\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7160 - accuracy: 0.4824\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7139 - accuracy: 0.4910\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7121 - accuracy: 0.4981\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7111 - accuracy: 0.4929\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7098 - accuracy: 0.5010\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.5005\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.5062\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.4914\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.4943\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7056 - accuracy: 0.5010\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.4895\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.5005\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.4933\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4771\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4881\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4895\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7029 - accuracy: 0.4952\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7018 - accuracy: 0.4914\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4795\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2101293219840.0000 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 2492.5901 - accuracy: 0.4938\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.0094 - accuracy: 0.4971\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8348 - accuracy: 0.4819\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7836 - accuracy: 0.4924\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7603 - accuracy: 0.4924\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7455 - accuracy: 0.5076\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7367 - accuracy: 0.4990\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7301 - accuracy: 0.4943\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7254 - accuracy: 0.4895\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7217 - accuracy: 0.4848\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7190 - accuracy: 0.4876\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7166 - accuracy: 0.4952\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7144 - accuracy: 0.5019\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7131 - accuracy: 0.4952\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7115 - accuracy: 0.4819\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7104 - accuracy: 0.4848\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7094 - accuracy: 0.4943\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.5057\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7073 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7067 - accuracy: 0.5019\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7060 - accuracy: 0.5029\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.4990\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7048 - accuracy: 0.4771\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.4895\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7041 - accuracy: 0.4867\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7034 - accuracy: 0.4971\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4933\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4943\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4867\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7021 - accuracy: 0.4886\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4971\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4957\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4886\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4886\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 19.0709 - accuracy: 0.4905\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7009 - accuracy: 0.5114\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.5048\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7017 - accuracy: 0.5095\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4943\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4924\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.5114\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7065 - accuracy: 0.4838\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.5105\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4895\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7114 - accuracy: 0.5133\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.5095\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6990 - accuracy: 0.5057\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.5038\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4952\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4895\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6993 - accuracy: 0.4867\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6985 - accuracy: 0.4924\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7005 - accuracy: 0.5057\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6987 - accuracy: 0.4924\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6978 - accuracy: 0.5095\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.5019\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7001 - accuracy: 0.5048\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4952\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6991 - accuracy: 0.5143\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6984 - accuracy: 0.4933\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4990\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6995 - accuracy: 0.5029\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7012 - accuracy: 0.4876\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4919\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 241725053477979395457024.0000 - accuracy: 0.4738\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9525 - accuracy: 0.4990\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8214 - accuracy: 0.4876\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7791 - accuracy: 0.4905\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7566 - accuracy: 0.4924\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7436 - accuracy: 0.4943\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7352 - accuracy: 0.4819\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.4924\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7246 - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7212 - accuracy: 0.4914\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7186 - accuracy: 0.4905\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7159 - accuracy: 0.5048\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.5029\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.4876\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7120 - accuracy: 0.4743\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7099 - accuracy: 0.4962\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4848\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7082 - accuracy: 0.5057\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7078 - accuracy: 0.4800\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.4895\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.4962\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7052 - accuracy: 0.4952\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4781\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4971\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.4914\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.4914\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7032 - accuracy: 0.5067\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4981\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4886\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4800\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7011 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 145759713873625088.0000 - accuracy: 0.4819\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7379 - accuracy: 0.5095\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7078 - accuracy: 0.4981\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7013 - accuracy: 0.4981\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6989 - accuracy: 0.4933\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4838\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.4981\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7011 - accuracy: 0.5019\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6978 - accuracy: 0.4848\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7001 - accuracy: 0.4895\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.4924\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.4943\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6976 - accuracy: 0.4914\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5010\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6952 - accuracy: 0.4981\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.4733\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.4895\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6977 - accuracy: 0.5067\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6955 - accuracy: 0.4876\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.4933\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6954 - accuracy: 0.4981\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.4981\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4952\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.4933\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.5019\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.4905\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6963 - accuracy: 0.4876\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.5010\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.4857\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.4895\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 337297228399576292338935820976128.0000 - accuracy: 0.4914\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9624 - accuracy: 0.4933\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8238 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 0.7792 - accuracy: 0.4743\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7573 - accuracy: 0.4981\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7443 - accuracy: 0.4905\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7354 - accuracy: 0.4924\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7295 - accuracy: 0.4867\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 0.7249 - accuracy: 0.4962\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7215 - accuracy: 0.4886\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7188 - accuracy: 0.4810\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 0.7162 - accuracy: 0.4914\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.4895\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7124 - accuracy: 0.4924\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7115 - accuracy: 0.4905\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7101 - accuracy: 0.5057\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4952\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7085 - accuracy: 0.4943\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7072 - accuracy: 0.4952\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.4971\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.4781\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7053 - accuracy: 0.4838\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.4943\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.4924\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7043 - accuracy: 0.4886\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.4905\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.4829\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4914\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7027 - accuracy: 0.4838\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4924\n",
            "22/22 [==============================] - 1s 4ms/step - loss: 0.6998 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4948\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4881\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4890                                \n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 1536079872.0000 - accuracy: 0.4981\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8832 - accuracy: 0.4876\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7458 - accuracy: 0.4981\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7172 - accuracy: 0.5010\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7097 - accuracy: 0.4971\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4971\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4971\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7004 - accuracy: 0.4933\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6990 - accuracy: 0.5038\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7008 - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.4924\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.4867\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.5019\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6977 - accuracy: 0.5019\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6977 - accuracy: 0.4838\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.4752\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7004 - accuracy: 0.5076\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.4990\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6961 - accuracy: 0.5038\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6977 - accuracy: 0.4943\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6985 - accuracy: 0.4857\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6962 - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5067\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6968 - accuracy: 0.5048\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6980 - accuracy: 0.4943\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.5019\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.5038\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.5162\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6982 - accuracy: 0.4971\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6954 - accuracy: 0.5029\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 2217.0044 - accuracy: 0.4857\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9695 - accuracy: 0.4857\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8254 - accuracy: 0.5019\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7797 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7577 - accuracy: 0.4886\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7444 - accuracy: 0.4990\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7363 - accuracy: 0.5048\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7302 - accuracy: 0.4886\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7251 - accuracy: 0.4962\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.4914\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7184 - accuracy: 0.5086\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7164 - accuracy: 0.4857\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7128 - accuracy: 0.4752\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7113 - accuracy: 0.4895\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7104 - accuracy: 0.4857\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7090 - accuracy: 0.4781\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7080 - accuracy: 0.5067\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.4943\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7060 - accuracy: 0.4952\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7051 - accuracy: 0.5019\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7048 - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7044 - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4981\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.4952\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.5048\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.4933\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.5019\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4857\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7016 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 20580150674339135488.0000 - accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9659 - accuracy: 0.5038\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8248 - accuracy: 0.4905\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7797 - accuracy: 0.4971\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7574 - accuracy: 0.4867\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7445 - accuracy: 0.4876\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7356 - accuracy: 0.5029\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7303 - accuracy: 0.4829\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7254 - accuracy: 0.4952\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7215 - accuracy: 0.5029\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7187 - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7162 - accuracy: 0.5038\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7146 - accuracy: 0.4981\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7131 - accuracy: 0.4819\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7113 - accuracy: 0.4943\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7102 - accuracy: 0.4914\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7090 - accuracy: 0.4886\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7080 - accuracy: 0.4943\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7078 - accuracy: 0.4905\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7064 - accuracy: 0.4933\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7072 - accuracy: 0.4876\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7055 - accuracy: 0.4924\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4990\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4876\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4933\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.5076\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.5095\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7023 - accuracy: 0.4971\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4924\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 1592309.5000 - accuracy: 0.4990\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9598 - accuracy: 0.4905\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8233 - accuracy: 0.4943\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7791 - accuracy: 0.4838\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7570 - accuracy: 0.4924\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7439 - accuracy: 0.5029\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7354 - accuracy: 0.5019\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.4876\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7248 - accuracy: 0.4800\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7212 - accuracy: 0.4867\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7184 - accuracy: 0.4857\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7162 - accuracy: 0.4962\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7140 - accuracy: 0.4981\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7128 - accuracy: 0.4762\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7111 - accuracy: 0.5010\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7104 - accuracy: 0.4952\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4886\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.4914\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.4838\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7065 - accuracy: 0.4905\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.4895\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7066 - accuracy: 0.4667\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7049 - accuracy: 0.4990\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.4800\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.4981\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4838\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7029 - accuracy: 0.4952\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7028 - accuracy: 0.5048\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7026 - accuracy: 0.4933\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7021 - accuracy: 0.4962\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7014 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4876\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 10346811852400948674560.0000 - accuracy: 0.4919\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9667 - accuracy: 0.4971\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8253 - accuracy: 0.4971\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7798 - accuracy: 0.4952\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7576 - accuracy: 0.4990\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7445 - accuracy: 0.4971\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7355 - accuracy: 0.5029\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7293 - accuracy: 0.4971\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7247 - accuracy: 0.4829\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.4914\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7185 - accuracy: 0.4952\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7161 - accuracy: 0.4886\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7144 - accuracy: 0.4762\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7124 - accuracy: 0.5010\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7112 - accuracy: 0.4933\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7102 - accuracy: 0.5133\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7095 - accuracy: 0.4895\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7085 - accuracy: 0.4876\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7073 - accuracy: 0.4962\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7063 - accuracy: 0.4886\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.4781\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7051 - accuracy: 0.4924\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.4962\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7048 - accuracy: 0.4981\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.5133\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7029 - accuracy: 0.5010\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4933\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7026 - accuracy: 0.4857\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7019 - accuracy: 0.4810\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.7014 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4848                               \n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 127.6198 - accuracy: 0.5086\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7370 - accuracy: 0.5067\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7099 - accuracy: 0.4905\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.5067\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.5086\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6982 - accuracy: 0.5133\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.5086\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.5076\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7004 - accuracy: 0.4981\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6985 - accuracy: 0.5048\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6959 - accuracy: 0.5152\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7008 - accuracy: 0.4943\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.4819\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7014 - accuracy: 0.5143\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.4924\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6982 - accuracy: 0.4990\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6981 - accuracy: 0.4914\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.5029\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.4990\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.4933\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6976 - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.4943\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.4848\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4867\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7004 - accuracy: 0.4905\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7003 - accuracy: 0.4962\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.5171\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5010\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6952 - accuracy: 0.4962\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6985 - accuracy: 0.5124\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 6595869653134836218782115954688.0000 - accuracy: 0.4933\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9539 - accuracy: 0.5038\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8220 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7789 - accuracy: 0.4971\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7569 - accuracy: 0.4781\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7439 - accuracy: 0.4971\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7353 - accuracy: 0.4971\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7295 - accuracy: 0.4905\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7252 - accuracy: 0.4981\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7218 - accuracy: 0.5029\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7185 - accuracy: 0.4990\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7161 - accuracy: 0.5010\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7141 - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.4905\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7114 - accuracy: 0.4924\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7100 - accuracy: 0.4752\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.4905\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7080 - accuracy: 0.4895\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7070 - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7064 - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7057 - accuracy: 0.5048\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.4895\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7046 - accuracy: 0.5010\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.5067\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.4990\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.4810\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7030 - accuracy: 0.4800\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.4943\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.5105\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 0.7021 - accuracy: 0.4867\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7014 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4971\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 5ms/step - loss: 212706769619312967680.0000 - accuracy: 0.4929\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8402 - accuracy: 0.4990\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7047 - accuracy: 0.5038\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7053 - accuracy: 0.4962\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7162 - accuracy: 0.5105\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7130 - accuracy: 0.4810\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.5171\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7148 - accuracy: 0.5067\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7010 - accuracy: 0.5171\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.4990\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6993 - accuracy: 0.5029\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7073 - accuracy: 0.4981\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7020 - accuracy: 0.5210\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6984 - accuracy: 0.4933\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7046 - accuracy: 0.4743\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7077 - accuracy: 0.5133\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7004 - accuracy: 0.4857\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.4895\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.5152\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6952 - accuracy: 0.5143\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.4943\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.4857\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7010 - accuracy: 0.4962\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6997 - accuracy: 0.4886\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6963 - accuracy: 0.4857\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7072 - accuracy: 0.4781\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6963 - accuracy: 0.5095\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6965 - accuracy: 0.5143\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.6980 - accuracy: 0.4981\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.4910\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 5ms/step - loss: 84331108087256580096.0000 - accuracy: 0.4890\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9581 - accuracy: 0.4952\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8230 - accuracy: 0.5019\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7789 - accuracy: 0.4924\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7569 - accuracy: 0.4962\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7443 - accuracy: 0.4905\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7354 - accuracy: 0.4952\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7293 - accuracy: 0.4952\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7249 - accuracy: 0.4971\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.4705\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7183 - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7162 - accuracy: 0.4962\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7143 - accuracy: 0.5076\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7128 - accuracy: 0.4886\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7114 - accuracy: 0.5010\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7105 - accuracy: 0.4952\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7094 - accuracy: 0.4857\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7091 - accuracy: 0.4857\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7076 - accuracy: 0.4981\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7065 - accuracy: 0.5010\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.4924\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7052 - accuracy: 0.4895\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.5019\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7045 - accuracy: 0.4838\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7041 - accuracy: 0.4867\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7037 - accuracy: 0.4743\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.4810\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7029 - accuracy: 0.4867\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.4914\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.4943\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7037 - accuracy: 0.4671\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 907591483392.0000 - accuracy: 0.4852\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9955 - accuracy: 0.4886\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8319 - accuracy: 0.4981\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7827 - accuracy: 0.4924\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7592 - accuracy: 0.4838\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7453 - accuracy: 0.4895\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7363 - accuracy: 0.4895\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7301 - accuracy: 0.4876\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.4981\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7218 - accuracy: 0.4952\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7187 - accuracy: 0.4905\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7169 - accuracy: 0.4848\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7147 - accuracy: 0.4962\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.4914\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.4857\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7108 - accuracy: 0.4952\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7094 - accuracy: 0.4867\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7084 - accuracy: 0.4952\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7074 - accuracy: 0.4943\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7069 - accuracy: 0.4924\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7061 - accuracy: 0.4914\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.5019\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7050 - accuracy: 0.4895\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7040 - accuracy: 0.5086\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7040 - accuracy: 0.4876\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4895\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7032 - accuracy: 0.5076\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7020 - accuracy: 0.5010\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.4867\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7008 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 557610.5625 - accuracy: 0.4724\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9632 - accuracy: 0.4905\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8242 - accuracy: 0.4990\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7796 - accuracy: 0.4771\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7572 - accuracy: 0.4848\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7443 - accuracy: 0.4848\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7355 - accuracy: 0.4962\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7293 - accuracy: 0.4981\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7247 - accuracy: 0.4971\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7215 - accuracy: 0.4876\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7186 - accuracy: 0.4886\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7167 - accuracy: 0.4848\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7142 - accuracy: 0.4971\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7129 - accuracy: 0.4714\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7112 - accuracy: 0.4990\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7107 - accuracy: 0.4829\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7090 - accuracy: 0.4819\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7082 - accuracy: 0.4790\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7073 - accuracy: 0.5038\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7067 - accuracy: 0.5010\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.4762\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7054 - accuracy: 0.4943\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7046 - accuracy: 0.4971\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4924\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.4752\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7034 - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4895\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4876\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7026 - accuracy: 0.5048\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5048\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.4943\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5000\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 158295834755072.0000 - accuracy: 0.4895\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9722 - accuracy: 0.4971\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.8270 - accuracy: 0.4800\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7806 - accuracy: 0.5010\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7579 - accuracy: 0.4800\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7448 - accuracy: 0.4771\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7359 - accuracy: 0.4895\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7294 - accuracy: 0.5048\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7251 - accuracy: 0.4962\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7213 - accuracy: 0.4924\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7189 - accuracy: 0.4933\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7163 - accuracy: 0.4933\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7144 - accuracy: 0.4810\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.4886\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7111 - accuracy: 0.4905\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7101 - accuracy: 0.4971\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7095 - accuracy: 0.4781\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7087 - accuracy: 0.4981\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7073 - accuracy: 0.4914\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7065 - accuracy: 0.4962\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.4962\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7054 - accuracy: 0.4848\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.5019\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7044 - accuracy: 0.4905\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7036 - accuracy: 0.4943\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7034 - accuracy: 0.4781\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7034 - accuracy: 0.4838\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7029 - accuracy: 0.4819\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.5143\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7020 - accuracy: 0.5067\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.5329\n",
            "Epoch 1/30\n",
            "66/66 [==============================] - 1s 4ms/step - loss: 2835656960.0000 - accuracy: 0.4981\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.9543 - accuracy: 0.4867\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8217 - accuracy: 0.4990\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7783 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7568 - accuracy: 0.4971\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7443 - accuracy: 0.4743\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7354 - accuracy: 0.4829\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7293 - accuracy: 0.4914\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7250 - accuracy: 0.4924\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7214 - accuracy: 0.4924\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7182 - accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.4752\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7141 - accuracy: 0.5067\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7125 - accuracy: 0.5010\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7115 - accuracy: 0.5010\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7099 - accuracy: 0.5048\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7099 - accuracy: 0.4886\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7079 - accuracy: 0.4771\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7074 - accuracy: 0.4943\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7065 - accuracy: 0.4933\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.5000\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7055 - accuracy: 0.5019\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7052 - accuracy: 0.5038\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7044 - accuracy: 0.4886\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7037 - accuracy: 0.4943\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7033 - accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7032 - accuracy: 0.5038\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.5067\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.4914\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5048\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.4671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM3_k_XLMlFD",
        "outputId": "8b3ff0f4-5db2-4d7c-e93a-d425a0685814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc over 10 instances: 0.52 +- 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE OF GOOD SYNTHETIC DATA GENERATING (USING ALL 50 000 CIFAR IMAGES)"
      ],
      "metadata": {
        "id": "jomrxeRj07us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# plot the generated images\n",
        "def create_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :])\n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = load_model('generator_model_100.h5')\n",
        "# generate images\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "# generate images\n",
        "X = model.predict(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "create_plot(X, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "p9Auex1-20jY",
        "outputId": "b8291f15-1e74-4f39-ae4e-66f22a8402da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 100 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNDfKwjm0_Jc"
      },
      "source": [
        "### Method 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yFmXFgOOhf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHYMycueOquR"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # MODEL WITH MODIFIED KERNEL SIZE + BATCHNORM\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=5, stride=2), \n",
        "                      nn.BatchNorm2d(16),\n",
        "                      nn.ReLU()]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=5), \n",
        "                      nn.BatchNorm2d(32),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
        "                        nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*4*4, 32)\n",
        "        \n",
        "        self.layerout=nn.Linear(32,10)\n",
        "\n",
        "\n",
        "\n",
        "        # add one hidden layer with non linearity in the Fully Connected layers\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*4*4)\n",
        "        rel=torch.nn.ReLU()\n",
        "        x = self.fc(x)\n",
        "        x=rel(x)\n",
        "        x=self.layerout(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Net2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "\n",
        "        # MODEL WITH BASELINE KERNEL SIZE + BATCHNORM\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
        "                      nn.BatchNorm2d(16),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
        "                      nn.BatchNorm2d(32),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*5*5, 32)\n",
        "        \n",
        "        self.layerout=nn.Linear(32,10)\n",
        "\n",
        "\n",
        "        # add one hidden layer with non linearity in the Fully Connected layers\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*5*5)\n",
        "        rel=torch.nn.ReLU()\n",
        "        x = self.fc(x)\n",
        "        x=rel(x)\n",
        "        x=self.layerout(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        " Learning Rate Search (Among lr=[0.1,0.01,0.008,0.001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7xU1HMelJ3",
        "outputId": "79bb4abd-12e3-4d06-a95c-4e4f7f08acc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.374567\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 22.781084\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.429636\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.883973\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.710661\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.597462\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.462121\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.334390\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.231649\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.131401\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.051381\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.009787\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002027\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000956\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000542\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000274\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000184\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000137\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000114\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000096\n",
            "\n",
            "Test set: Average loss: 4.0027, Accuracy: 445/700 (63.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.345267\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 11.334431\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.231470\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.642717\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.426818\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.249616\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.085547\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.017797\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.002922\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.000395\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.000279\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.000104\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000050\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000036\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000032\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000029\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000026\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000024\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000024\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000024\n",
            "\n",
            "Test set: Average loss: 2.4254, Accuracy: 537/700 (76.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.289375\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 12.053628\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.195035\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.493074\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.380740\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.160955\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.031479\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.008583\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.002703\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.001121\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.000519\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.000378\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000320\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000286\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000263\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000246\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000234\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000226\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000220\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000217\n",
            "\n",
            "Test set: Average loss: 4.0720, Accuracy: 457/700 (65.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.238388\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 24.695141\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.282435\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.852013\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.663605\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.526553\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.403217\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.241721\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.147739\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.207990\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.716066\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.476291\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.418699\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.298042\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.224542\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.164119\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.126030\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.092175\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.068730\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.052521\n",
            "\n",
            "Test set: Average loss: 4.2809, Accuracy: 418/700 (59.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.295067\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.175858\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.352694\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.922014\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.684865\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.716909\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.653434\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.536479\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.525829\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.412813\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.774624\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.413593\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.410036\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.319488\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.256966\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.212189\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.167045\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.138907\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 1.855834\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.653169\n",
            "\n",
            "Test set: Average loss: 1.8524, Accuracy: 352/700 (50.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.232424\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.113955\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.362199\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.828173\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 1.406227\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.942639\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.900456\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.025146\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.598781\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.332671\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.005808\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.890545\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.821124\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.765643\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.634365\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.613294\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.601313\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.583865\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.571051\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.563815\n",
            "\n",
            "Test set: Average loss: 1.4619, Accuracy: 340/700 (48.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.335132\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.217401\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.127455\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.570532\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.351124\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.520331\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.218290\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.180793\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.104275\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.071947\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.054436\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.041773\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.031739\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.027473\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.023689\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.020376\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.017007\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.013944\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.011112\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.008603\n",
            "\n",
            "Test set: Average loss: 2.0078, Accuracy: 559/700 (79.86%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.338259\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.282173\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.075320\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.208901\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.617502\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.335290\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.107720\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.028225\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.007359\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.003009\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.001004\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.000703\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000430\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000265\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000189\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000153\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000135\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000124\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000117\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000113\n",
            "\n",
            "Test set: Average loss: 3.7931, Accuracy: 466/700 (66.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.279930\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.510645\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.399781\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.850769\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.602839\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.471056\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.398606\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.312246\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.256745\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.226825\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.197628\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.161634\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.135738\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.116687\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.097687\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.079481\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.066984\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.054289\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.041053\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.031719\n",
            "\n",
            "Test set: Average loss: 4.6219, Accuracy: 511/700 (73.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.250774\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 8.193418\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.037483\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.699591\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.587121\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.459191\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.343239\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.195836\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.095376\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.024788\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008507\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001849\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000890\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000501\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000301\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000218\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000189\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000173\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000162\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000155\n",
            "\n",
            "Test set: Average loss: 4.3022, Accuracy: 443/700 (63.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.344762\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.130537\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.255203\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.610410\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.361493\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.225340\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.086343\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.026540\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.008346\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.003716\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.001353\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.000902\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000559\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000423\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000345\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000279\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000242\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000224\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000212\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000201\n",
            "\n",
            "Test set: Average loss: 0.6487, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.317359\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.844172\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.658257\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.460880\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.306915\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.137273\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.084361\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.044966\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.009669\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.004154\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.002030\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001528\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000981\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000604\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000465\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000397\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000342\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000307\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000283\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000266\n",
            "\n",
            "Test set: Average loss: 0.5872, Accuracy: 613/700 (87.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.291745\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.099831\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.404239\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.545370\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.461575\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.361382\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.290637\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.189854\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.060217\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.017621\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007051\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003946\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002064\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001086\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000832\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000718\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000625\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000544\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000483\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000440\n",
            "\n",
            "Test set: Average loss: 1.1417, Accuracy: 513/700 (73.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.245551\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.036628\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.122280\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.647898\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.608331\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.936281\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.454565\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.345599\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.198139\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.080727\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.029803\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.009885\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003982\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002196\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001398\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000981\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000798\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000668\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000595\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000544\n",
            "\n",
            "Test set: Average loss: 0.7828, Accuracy: 516/700 (73.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.299825\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.909324\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.658975\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.363248\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.263047\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.172193\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.091868\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.197768\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.102016\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.041536\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.022449\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.010734\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.004701\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002665\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001723\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001245\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000998\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000788\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000679\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000593\n",
            "\n",
            "Test set: Average loss: 0.5167, Accuracy: 617/700 (88.14%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.308374\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.177532\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.878735\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.942989\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.575653\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.462671\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.326121\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.180657\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.064350\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.018180\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005848\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002400\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001285\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000798\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000588\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000481\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000410\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000368\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000341\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000322\n",
            "\n",
            "Test set: Average loss: 1.1088, Accuracy: 475/700 (67.86%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.203081\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.869618\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.925446\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.404861\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.288360\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.149824\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.075246\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.026435\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.013902\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.005538\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003194\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001679\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001081\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000823\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000656\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000518\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000437\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000401\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000371\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000345\n",
            "\n",
            "Test set: Average loss: 0.9948, Accuracy: 606/700 (86.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.208416\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.004415\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.160094\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.656078\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.604918\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.171047\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.369147\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.138302\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.051667\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.013887\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.004966\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002414\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001350\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000956\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000727\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000597\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000511\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000457\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000419\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000391\n",
            "\n",
            "Test set: Average loss: 1.3286, Accuracy: 509/700 (72.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.236337\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.104288\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.688077\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.659330\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.326053\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.176604\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.099159\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.039400\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.019558\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.008515\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.004110\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002319\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001407\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000979\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000713\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000572\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000493\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000440\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000401\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000371\n",
            "\n",
            "Test set: Average loss: 1.1130, Accuracy: 579/700 (82.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.267848\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.886315\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.746650\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.642860\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.811952\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.432096\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.272740\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.140700\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.053866\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.018463\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.006907\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003061\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001626\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001078\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000819\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000637\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000543\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000481\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000437\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000406\n",
            "\n",
            "Test set: Average loss: 1.2540, Accuracy: 497/700 (71.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.217592\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.059175\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.440163\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.657374\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.585997\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.463614\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.288566\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.145897\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.051086\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.015425\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005154\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002331\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001247\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000814\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000613\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000504\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000434\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000389\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000358\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000337\n",
            "\n",
            "Test set: Average loss: 0.6092, Accuracy: 592/700 (84.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.325717\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.207866\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.786741\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.687990\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.430583\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.257908\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.149706\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.079255\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.036645\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.016233\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008183\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004617\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002861\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001962\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001476\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001183\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000999\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000874\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000785\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000718\n",
            "\n",
            "Test set: Average loss: 0.5832, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.353656\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.149757\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.627656\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.682229\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.601375\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.530007\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.408526\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.288804\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.153774\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.059842\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.020655\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007423\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003388\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001834\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001198\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000886\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000720\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000622\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000557\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000516\n",
            "\n",
            "Test set: Average loss: 1.0448, Accuracy: 519/700 (74.14%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.362391\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.187748\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.659243\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.693381\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.620010\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.560606\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.497448\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.750920\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.238507\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.101373\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.041175\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.014205\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006733\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003308\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002240\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001504\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001208\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001026\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000893\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000803\n",
            "\n",
            "Test set: Average loss: 0.6961, Accuracy: 524/700 (74.86%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.295505\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.065841\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.240927\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.590063\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.452250\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.332411\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.257647\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.155010\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.084853\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.034678\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.014975\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006250\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003271\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001947\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001280\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000965\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000792\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000672\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000586\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000530\n",
            "\n",
            "Test set: Average loss: 0.5583, Accuracy: 621/700 (88.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.374489\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.211406\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.728929\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.733267\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.658722\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.495949\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.391277\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.276660\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.165156\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.075119\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.029658\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.011909\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.005511\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002965\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001837\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001343\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001057\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000893\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000782\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000709\n",
            "\n",
            "Test set: Average loss: 1.0665, Accuracy: 488/700 (69.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.403324\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.179954\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.320361\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.482874\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.293359\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.209004\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.103296\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.055732\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.027267\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.012086\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.006005\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003422\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002341\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001683\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001290\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001070\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000928\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000828\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000754\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000695\n",
            "\n",
            "Test set: Average loss: 0.8488, Accuracy: 595/700 (85.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.265392\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.118295\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.552566\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.694600\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.616206\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.550344\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.454508\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.327927\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.168766\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.069317\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.025144\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.009806\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.004638\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002652\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001749\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001304\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001052\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000906\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000807\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000741\n",
            "\n",
            "Test set: Average loss: 1.2062, Accuracy: 504/700 (72.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.277850\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.122589\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.681035\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.795882\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.347558\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.226396\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.161707\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.102801\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.054173\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.030980\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.018614\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.009929\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.005630\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003468\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002412\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001776\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001424\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001165\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001004\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000887\n",
            "\n",
            "Test set: Average loss: 1.0636, Accuracy: 562/700 (80.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.250026\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.077587\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.493097\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.710543\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.711880\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.623880\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.667068\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.513712\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.420706\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.289511\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.164770\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.075880\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.029033\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.011408\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.005162\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.002843\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001841\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001348\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001086\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000917\n",
            "\n",
            "Test set: Average loss: 1.3401, Accuracy: 489/700 (69.86%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.231760\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.209778\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.163835\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.103013\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.025117\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.924836\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.793609\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.621610\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.407958\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.176571\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.961844\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.789222\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.698388\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.661634\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.624315\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.608130\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.587481\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.570087\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.550645\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.530735\n",
            "\n",
            "Test set: Average loss: 0.6100, Accuracy: 508/700 (72.57%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.203245\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.182857\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.143239\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.099240\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.050752\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.991045\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.912402\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.804710\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.656195\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.454441\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.205995\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.960528\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.770338\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.636923\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.557555\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.498886\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.445305\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.400047\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.354652\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.314937\n",
            "\n",
            "Test set: Average loss: 0.4007, Accuracy: 576/700 (82.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.195328\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.170144\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.123611\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.074975\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.025874\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.970154\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.900761\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.809984\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.689525\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.530228\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.328650\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 1.102897\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.899396\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.754236\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.666191\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.619829\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.586843\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.561984\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.538748\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.516463\n",
            "\n",
            "Test set: Average loss: 0.6215, Accuracy: 476/700 (68.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.304621\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.286907\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.250079\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.203054\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.148595\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.085591\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 2.007526\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.905496\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.766911\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.577087\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.331250\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 1.063467\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.848037\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.726749\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.671941\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.644725\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.626042\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.611225\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.597219\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.583879\n",
            "\n",
            "Test set: Average loss: 0.6521, Accuracy: 448/700 (64.00%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.367997\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.349883\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.310311\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.254130\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.179284\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.077868\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.932674\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.724745\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.458489\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.195547\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.990057\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.828209\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.704313\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.644993\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.602977\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.563988\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.531877\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.493928\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.459644\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.425697\n",
            "\n",
            "Test set: Average loss: 0.4304, Accuracy: 600/700 (85.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.347811\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.327449\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.283355\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.221746\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.139472\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.026840\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.868079\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.645027\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.349989\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.026916\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.779144\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.657181\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.601399\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.568273\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.543768\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.519478\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.495815\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.472731\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.449027\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.425191\n",
            "\n",
            "Test set: Average loss: 0.6507, Accuracy: 442/700 (63.14%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.278564\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.258865\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.218651\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.165734\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.102392\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 2.023021\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.914056\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.757126\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.539720\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.271044\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.006432\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.815920\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.710933\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.662703\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.634538\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.611165\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.589657\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.566753\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.542757\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.516573\n",
            "\n",
            "Test set: Average loss: 0.5914, Accuracy: 516/700 (73.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.317222\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.276901\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.182613\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.034829\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 1.821476\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.526778\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.186716\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.921536\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.763282\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.685801\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.669293\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.640440\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.632101\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.616456\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.605665\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.591637\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.578867\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.563829\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.548515\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.531613\n",
            "\n",
            "Test set: Average loss: 0.6495, Accuracy: 457/700 (65.29%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.318735\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.297644\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.250233\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.181230\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.089429\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.968450\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.805987\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.586358\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.308536\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.015497\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.781294\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.644712\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.580363\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.523594\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.478189\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.431715\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.388415\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.347433\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.311408\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.280438\n",
            "\n",
            "Test set: Average loss: 0.5055, Accuracy: 537/700 (76.71%)\n",
            "\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.227492\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.212025\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.178111\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.129422\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 2.063756\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.974274\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.852028\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.687754\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.473403\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.215630\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.957536\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.771962\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.684510\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.645546\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.621735\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.605270\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.588864\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.573463\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.557399\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.541212\n",
            "\n",
            "Test set: Average loss: 0.6492, Accuracy: 465/700 (66.43%)\n",
            "\n",
            "Learning Rate (0.1): [64.68571428571428, 9.710041089469378]\n",
            "\n",
            "\n",
            "Learning Rate (0.01): [78.77142857142857, 7.364614791040203]\n",
            "\n",
            "\n",
            "Learning Rate (0.008): [78.45714285714287, 6.789788250733296]\n",
            "\n",
            "\n",
            "Learning Rate (0.001): [71.78571428571429, 7.439620903223056]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "lrs,lr_accs=([0.1,0.01,0.008,0.001],[])\n",
        "\n",
        "for tlr in lrs:\n",
        "  accs = []\n",
        "  for seed in range(30,40):\n",
        "    prng = RandomState(seed)\n",
        "    random_permute = prng.permutation(np.arange(0, 1000))\n",
        "    print()\n",
        "    classes =  prng.permutation(np.arange(0,10))\n",
        "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "    indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "    train_data = Subset(cifar_data, indx_train)\n",
        "    val_data = Subset(cifar_data_val, indx_val)\n",
        "    print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                              batch_size=128, \n",
        "                                              shuffle=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                            batch_size=128, \n",
        "                                            shuffle=False)\n",
        "    \n",
        "\n",
        "    model = Net()\n",
        "    model.to(device)\n",
        "    \n",
        "    # slightly reduce learning rate\n",
        "    # lrs=[0.1,0.01,0.008,0.005]\n",
        "    # lr_acs=[]\n",
        "    \n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                                lr=tlr, momentum=0.9,\n",
        "                                weight_decay=0.0005)\n",
        "                                \n",
        "    for epoch in range(100):\n",
        "      train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "      \n",
        "    accs.append(test(model, device, val_loader))\n",
        "    \n",
        "  \n",
        "  adata=  np.array(accs)\n",
        "  res=([adata.mean(),adata.std()])\n",
        "  lr_accs.append(res)\n",
        "  \n",
        "  # print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "c=0\n",
        "for r in lr_accs:\n",
        "  print('Learning Rate ('+str(lrs[c])+'): '+str(r))\n",
        "  print('\\n')\n",
        "  c+=1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91SotTaYPEoX"
      },
      "source": [
        "Testing accuracy using TestBed after modifying model, learning rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "b3c39bca-4eae-4746-c614-b6c85a00cc2e",
        "id": "ga5GPZZyPEoX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.379151\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.160531\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.180120\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.636040\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.515437\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.351547\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.226638\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.134627\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.059152\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.027568\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.011901\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006052\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003523\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002278\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001704\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001339\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001110\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000984\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000886\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000811\n",
            "\n",
            "Test set: Average loss: 0.5574, Accuracy: 613/700 (87.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.180326\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.894687\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.935267\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.706748\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.641163\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.594182\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.543970\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.416654\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.253625\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.133644\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.055014\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.020159\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.008007\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003872\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002291\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001539\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001161\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000959\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000827\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000739\n",
            "\n",
            "Test set: Average loss: 1.5512, Accuracy: 438/700 (62.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.361175\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.227566\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.869651\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.912122\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.629127\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.554658\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.466320\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.378407\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.214390\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.101717\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.036958\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.014070\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006047\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003056\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002023\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001422\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001145\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000978\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000861\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000787\n",
            "\n",
            "Test set: Average loss: 1.0208, Accuracy: 455/700 (65.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.220855\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.959301\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.989553\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.598461\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.485403\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.330940\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.175418\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.065030\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.021375\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.007796\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003304\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001724\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001109\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000825\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000676\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000579\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000515\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000471\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000438\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000413\n",
            "\n",
            "Test set: Average loss: 0.6022, Accuracy: 601/700 (85.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.354546\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.161112\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.795356\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.828480\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.669947\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.647650\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.609480\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.557871\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.752646\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.356026\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.222494\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.099565\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.038546\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.013182\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.005699\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.002984\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001822\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001297\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001027\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000855\n",
            "\n",
            "Test set: Average loss: 1.3661, Accuracy: 484/700 (69.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.282489\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.119721\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.508426\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.672454\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.637254\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.571548\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.403374\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.272766\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.147980\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.070374\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.026197\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.011012\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.005301\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003091\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002043\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001519\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001229\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001046\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000935\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000850\n",
            "\n",
            "Test set: Average loss: 1.0832, Accuracy: 494/700 (70.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.323973\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.088423\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.251647\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.621042\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.472504\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.377763\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.149408\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.059121\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.014584\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.005366\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.002462\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001222\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000843\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000657\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000509\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000421\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000376\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000348\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000327\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000310\n",
            "\n",
            "Test set: Average loss: 0.5453, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.252819\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.956284\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.058842\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.639265\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.536678\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.343153\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.188005\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.097310\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.042420\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.015096\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005884\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002889\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001628\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001091\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000814\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000650\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000545\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000477\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000433\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000400\n",
            "\n",
            "Test set: Average loss: 0.4798, Accuracy: 625/700 (89.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.243426\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.063754\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.355836\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.596448\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.451919\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.265457\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.148616\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.080667\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.037154\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.017146\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007835\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004382\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002844\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001994\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001507\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001238\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001060\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000946\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000860\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000795\n",
            "\n",
            "Test set: Average loss: 0.7168, Accuracy: 582/700 (83.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.279946\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.067703\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.404604\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.630959\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.523520\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.409399\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.295932\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.167319\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.069214\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.023356\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008773\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003804\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002003\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001309\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000923\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000742\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000625\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000548\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000498\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000462\n",
            "\n",
            "Test set: Average loss: 1.1021, Accuracy: 513/700 (73.29%)\n",
            "\n",
            "Acc over 10 instances: 77.36 +- 9.74\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.008, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xALHwSNAL18a"
      },
      "source": [
        "Testing accuracy using TestBed after modifying model, learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PePXEYKL18b"
      },
      "source": [
        " TEST ON OLD VS NEW CONV LAYERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toxDCp1CL18b",
        "outputId": "d0f14508-07f0-4e01-e9e1-07c75308c229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.236798\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.409307\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.110986\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.046768\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.738859\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.765150\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.807651\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.561233\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.506503\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.440669\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.386448\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.047794\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.234300\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.249730\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.117565\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.191163\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.058993\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.142328\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.024927\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.061998\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.010368\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.037231\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.005020\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.019224\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002769\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.009437\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001815\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.006473\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001343\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.004245\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001064\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.003263\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000887\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.002673\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000771\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.002137\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000692\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001862\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000635\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LAP\\Anaconda\\envs\\GPUSTUFF\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.4159, Accuracy: 625/700 (89.29%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6261, Accuracy: 605/700 (86.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.209604\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.498983\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.033279\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.197569\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.387478\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.567745\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.610108\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.669258\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.503843\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.638097\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.402594\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.622127\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.267989\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.513728\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.152269\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.381179\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.065150\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.275414\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.027072\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.191963\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.011279\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.125265\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.005190\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.073893\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002859\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.040605\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001873\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.022657\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001339\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.013206\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001068\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.008381\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000912\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.005798\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000802\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.004372\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000730\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.003486\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000677\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.002918\n",
            "\n",
            "Test set: Average loss: 1.3994, Accuracy: 457/700 (65.29%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.6505, Accuracy: 455/700 (65.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.458567\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.299184\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.272200\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.985406\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.586373\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.806244\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.647088\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.673253\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.607628\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.652830\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.548924\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.939318\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.599990\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.771020\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.322283\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.610860\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.170155\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.487188\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.071098\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.423482\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.028694\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.352434\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.011327\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.267865\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.005659\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.179374\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003217\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.105367\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002078\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.058009\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001553\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.030581\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001258\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.017187\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001068\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.010444\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000947\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.007027\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000861\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.005186\n",
            "\n",
            "Test set: Average loss: 1.0893, Accuracy: 482/700 (68.86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.1320, Accuracy: 463/700 (66.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.369527\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.413413\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.944079\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.198073\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.780750\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.307661\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.605016\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.664974\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.535945\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.603986\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.377377\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.195280\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.240732\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.656565\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.119339\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.196686\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.052015\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.101958\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.021729\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.048012\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.009045\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.023061\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004191\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.011764\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002363\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006852\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001581\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003730\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001170\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002342\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000945\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001617\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000812\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001309\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000719\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001142\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000652\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000955\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000600\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000855\n",
            "\n",
            "Test set: Average loss: 0.6700, Accuracy: 582/700 (83.14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5216, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.396236\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.212791\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.261969\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.717368\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.996074\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.674395\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.203214\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.595637\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.640132\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.527779\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.590159\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.920289\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.521268\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.945116\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.424785\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.415642\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.462848\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.231645\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.255681\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.153976\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.105622\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.084550\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.042807\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.048549\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.015658\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.024404\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.007257\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.013353\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.004094\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.008188\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.002437\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.005719\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001771\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.004183\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001384\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.003234\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001134\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.002721\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001001\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.002329\n",
            "\n",
            "Test set: Average loss: 1.1102, Accuracy: 478/700 (68.29%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.0428, Accuracy: 460/700 (65.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.351630\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.293942\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.255307\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.119311\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.053454\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.364361\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.528384\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.668650\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.634993\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.629825\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.488725\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.724439\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.320708\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.558104\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.169495\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.535273\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.071653\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.329596\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.026080\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.256530\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.009501\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.169770\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004189\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.100647\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002200\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.060747\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001407\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.033916\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001021\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.019026\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000813\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.011587\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000695\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.007631\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000623\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.005471\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000567\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.004230\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000528\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.003410\n",
            "\n",
            "Test set: Average loss: 1.1679, Accuracy: 508/700 (72.57%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.2662, Accuracy: 450/700 (64.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.241005\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.385554\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.055639\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.996248\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.414620\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.786311\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.631487\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.641412\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.489166\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.558134\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.336566\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.629532\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.162541\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.283491\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.069544\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.289290\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.020196\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.102730\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.006592\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.062035\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003218\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.027063\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001670\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.011708\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001170\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006417\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000829\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003587\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000643\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002567\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000545\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001925\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000483\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001428\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000438\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001261\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000405\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001095\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000379\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000960\n",
            "\n",
            "Test set: Average loss: 0.9958, Accuracy: 569/700 (81.29%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8326, Accuracy: 578/700 (82.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.262295\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.351073\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.084780\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.202750\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.333130\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.715017\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.611753\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.768615\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.420995\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.625703\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.227152\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.432692\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.110827\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.227791\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.039049\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.161937\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.012341\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.089426\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.004102\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.046671\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.001603\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.025931\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.000982\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.013905\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000634\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.008437\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000455\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.005550\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000377\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.003891\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000332\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.002913\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000302\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.002287\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000278\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001917\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000262\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001652\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000251\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001478\n",
            "\n",
            "Test set: Average loss: 0.4699, Accuracy: 630/700 (90.00%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.9017, Accuracy: 558/700 (79.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.369838\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.387673\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.271132\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.128143\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.065737\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.183028\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.493718\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.627042\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.640541\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.482689\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.560760\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.318651\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.421614\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.150511\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.250206\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.074150\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.153963\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.022176\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.072420\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.011847\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.027320\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.006283\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.010734\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003556\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.004767\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002211\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002560\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001411\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001560\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001153\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001123\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000942\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000871\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000811\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000741\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000722\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000647\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000665\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000578\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000622\n",
            "\n",
            "Test set: Average loss: 0.4803, Accuracy: 608/700 (86.86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.9754, Accuracy: 591/700 (84.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.419106\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.404593\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.238227\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.133719\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.910509\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.337271\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.116089\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.642725\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.729014\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.565099\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.559271\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.635541\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.463212\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.405262\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.388059\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.325684\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.320305\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.198351\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.253715\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.113465\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.167878\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.060753\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.096351\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.031069\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.048748\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.016852\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.023080\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.009755\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.011524\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.005876\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.006200\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.004239\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.003820\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.003224\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.002658\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.002543\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.002043\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.002170\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001663\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001872\n",
            "\n",
            "Test set: Average loss: 0.8514, Accuracy: 510/700 (72.86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.0698, Accuracy: 518/700 (74.00%)\n",
            "\n",
            "Acc over 10 instances with modified Conv kernels: 77.84 +- 8.85\n",
            "Acc over 10 instances with baseline kernels: 75.60 +- 9.15\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 50 samples per class\n",
        "\n",
        "accs = []\n",
        "a2=[]\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  m2 = Net2()\n",
        "  m2.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.008, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  o2 = torch.optim.SGD(m2.parameters(), \n",
        "  lr=0.008, momentum=0.9,\n",
        "  weight_decay=0.0005)\n",
        "\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    train(m2, device, train_loader, o2, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "  a2.append(test(m2, device, val_loader))\n",
        "accs = np.array(accs)\n",
        "a2 = np.array(a2)\n",
        "print('Acc over 10 instances with modified Conv kernels: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "print('Acc over 10 instances with baseline kernels: %.2f +- %.2f'%(a2.mean(),a2.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcR5arX1C9k"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1"
      ],
      "metadata": {
        "id": "wmNPzb0s2Yr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "An3T2J-tb9hK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1SFRctlcD9X"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        # print(' grad ::',model.conv1.weight.grad)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfVlbqs7_s_"
      },
      "source": [
        "Using the ResNet18 Model with finetuning the Fully Connected layers, without BatchNorm\n",
        "\n",
        "This model was tested on CIFAR-10 in the paper below. Initially wanted to use the 101-layer version but ran out of CUDA memory. \n",
        "Source: https://arxiv.org/pdf/1512.03385.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKgxEZSEbts7",
        "outputId": "9b5f5bae-2e40-4fc8-800d-907ac198c0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.144550\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.796397\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.575010\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.268290\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.159066\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 1.067854\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.981698\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.934756\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.897287\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.882161\n",
            "\n",
            "Test set: Average loss: 0.8670, Accuracy: 350/700 (50.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.040960\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.702590\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.316683\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.133116\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.986784\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.904906\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.851504\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.827385\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.807518\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.792507\n",
            "\n",
            "Test set: Average loss: 0.7912, Accuracy: 549/700 (78.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.703543\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.324178\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.078638\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.907855\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.816136\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.792129\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.586964\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.671024\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.616363\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.830805\n",
            "\n",
            "Test set: Average loss: 0.5875, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.572083\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 2.193778\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.732687\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.328326\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.099003\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.913894\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.832383\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.737020\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.681305\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.651147\n",
            "\n",
            "Test set: Average loss: 0.6391, Accuracy: 663/700 (94.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.523216\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 2.186726\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.787046\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.472840\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.272548\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 1.125171\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 1.025828\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.969123\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.918659\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.877027\n",
            "\n",
            "Test set: Average loss: 0.8777, Accuracy: 520/700 (74.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.209886\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.826594\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.532461\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.328194\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.162123\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 1.056976\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.984833\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.934596\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.899875\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.879265\n",
            "\n",
            "Test set: Average loss: 0.8698, Accuracy: 350/700 (50.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.102304\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.942674\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.709383\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.464091\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.269719\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 1.128968\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 1.037004\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.975867\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.925349\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.891967\n",
            "\n",
            "Test set: Average loss: 0.8891, Accuracy: 412/700 (58.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.237269\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.812551\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.456779\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.154108\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.003756\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.921844\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.869042\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.847635\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.815503\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.799793\n",
            "\n",
            "Test set: Average loss: 0.7986, Accuracy: 350/700 (50.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.289509\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.697676\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.232235\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.999056\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.906472\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.835875\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.808431\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.787287\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.779997\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.764741\n",
            "\n",
            "Test set: Average loss: 0.7639, Accuracy: 350/700 (50.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.864264\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.856308\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.454321\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 1.251884\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 1.296066\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 1.343565\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 1.042205\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.973855\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.810674\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.783905\n",
            "\n",
            "Test set: Average loss: 0.8469, Accuracy: 554/700 (79.14%)\n",
            "\n",
            "Acc over 10 instances: 67.09 +- 16.31\n",
            "TIME TAKEN (FINETUNE RESNET FC LAYERS W/OUT BATCHNORM):  78.22159886360168\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "import time as timer\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(256) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "start=timer.time()\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=10, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=10, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "\n",
        "  model.fc = nn.Sequential(nn.Linear(512,3),nn.Sigmoid(),nn.Linear(3,  10))\n",
        "  optimizer = torch.optim.SGD(model.fc.parameters(), momentum=0.9,\n",
        "                              lr=0.006, weight_decay=0.0005)\n",
        "  model.to(device)\n",
        "\n",
        "  # print(model)\n",
        "  # print(modelll)\n",
        "  for epoch in range(10):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "print('TIME TAKEN (FINETUNE RESNET FC LAYERS W/OUT BATCHNORM): ',str(timer.time()-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjrdmqNHbts8"
      },
      "source": [
        "SAME FULLY CONNECTED MODIFICATIONS WITH BATCHNORM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "4c73cb77-c455-4891-de29-fed4f745e280",
        "id": "887XbnlbcD9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.003268\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.301448\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.157399\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.176280\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.561086\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.091566\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.010576\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.150219\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.020494\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.023850\n",
            "\n",
            "Test set: Average loss: 0.1059, Accuracy: 672/700 (96.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.876523\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.569810\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.342263\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.210819\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.096001\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.335787\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.087299\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.075204\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.074891\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.031308\n",
            "\n",
            "Test set: Average loss: 0.6963, Accuracy: 528/700 (75.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.898803\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.503158\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.606937\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.851628\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.216460\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.145952\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.065333\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.042893\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.056802\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.017662\n",
            "\n",
            "Test set: Average loss: 0.5087, Accuracy: 569/700 (81.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.935741\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.440900\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.307922\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.711379\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.081251\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.029911\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.036659\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.017745\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.052278\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.016830\n",
            "\n",
            "Test set: Average loss: 0.3062, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.859037\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.437978\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.282167\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.077694\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.107428\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.199576\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.043608\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.266576\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.070101\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.550081\n",
            "\n",
            "Test set: Average loss: 0.4350, Accuracy: 575/700 (82.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.911935\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.573625\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.310662\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.250242\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.253773\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.363595\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.099082\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.172694\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.527511\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.049703\n",
            "\n",
            "Test set: Average loss: 0.2993, Accuracy: 620/700 (88.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.092905\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.390760\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.247336\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.064870\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.037003\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.034661\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.015924\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.015656\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.009833\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.012941\n",
            "\n",
            "Test set: Average loss: 0.1134, Accuracy: 671/700 (95.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.700302\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.267234\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.331383\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.029180\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.070307\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.025107\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.009042\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.011259\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.019614\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.010823\n",
            "\n",
            "Test set: Average loss: 0.0873, Accuracy: 675/700 (96.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.941274\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.320131\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.175008\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.053952\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.036827\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.033067\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.056606\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.010832\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.012237\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.023735\n",
            "\n",
            "Test set: Average loss: 0.2108, Accuracy: 652/700 (93.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.802484\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.359245\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.550113\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.311238\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.043708\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.033060\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.183425\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.315478\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.329228\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.642193\n",
            "\n",
            "Test set: Average loss: 0.4270, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "Acc over 10 instances: 88.07 +- 6.90\n",
            "TIME TAKEN (FINETUNE RESNET FC LAYERS W/BATCHNORM):  62.81531023979187\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "import time as timer\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(256) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "start=timer.time()\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=10, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=10, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "  model.fc = nn.Sequential(nn.Linear(512,3),nn.BatchNorm1d(3),nn.Sigmoid(),nn.Linear(3,  10))\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.fc.parameters(), momentum=0.9,\n",
        "                              lr=0.1, weight_decay=0.0005)\n",
        "  model.to(device)\n",
        "\n",
        " \n",
        "  for epoch in range(10):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "print('TIME TAKEN (FINETUNE RESNET FC LAYERS W/BATCHNORM): ',str(timer.time()-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0bUsgTRbts9"
      },
      "source": [
        "Testing effect of increasing hidden layer width on FC Layer (width=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgSuil8Mbts9",
        "outputId": "58d312be-54c1-4d4e-d561-5fd3e9d8ff2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
            "c:\\Users\\LAP\\Anaconda\\envs\\GPUSTUFF\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\LAP\\Anaconda\\envs\\GPUSTUFF\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.604210\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.699631\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.370300\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.264012\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.327038\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.141643\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.177719\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.156865\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.091687\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.082793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LAP\\Anaconda\\envs\\GPUSTUFF\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1789, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.708047\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.805295\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.575287\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.412759\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.361689\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.307594\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.200509\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.131071\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.172052\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.126587\n",
            "\n",
            "Test set: Average loss: 0.3878, Accuracy: 588/700 (84.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.762090\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.831219\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.662098\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.610618\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.374841\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.287196\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.261806\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.206715\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.266270\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.144605\n",
            "\n",
            "Test set: Average loss: 0.3826, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.645744\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.757283\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.546775\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.333590\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.250109\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.492446\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.171146\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.122086\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.134396\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.110888\n",
            "\n",
            "Test set: Average loss: 0.1814, Accuracy: 679/700 (97.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.411545\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.833404\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.642112\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.415419\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.378712\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.312770\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.265900\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.298152\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.153673\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.178125\n",
            "\n",
            "Test set: Average loss: 0.3380, Accuracy: 620/700 (88.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.538451\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.946996\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.563599\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.459713\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.396848\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.263850\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.572512\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.182600\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.155142\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.115936\n",
            "\n",
            "Test set: Average loss: 0.3468, Accuracy: 612/700 (87.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.612925\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.835695\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.531692\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.337214\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.356624\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.194899\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.169566\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.189954\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.124325\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.092133\n",
            "\n",
            "Test set: Average loss: 0.1793, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.564522\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.753628\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.458285\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.401032\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.310340\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.262492\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.155237\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.158469\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.129973\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.126291\n",
            "\n",
            "Test set: Average loss: 0.1837, Accuracy: 670/700 (95.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.481258\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.681407\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.416610\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.396739\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.398522\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.171792\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.193589\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.192140\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.157991\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.097844\n",
            "\n",
            "Test set: Average loss: 0.2137, Accuracy: 661/700 (94.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\LAP/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.626879\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.780932\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.469288\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.334297\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.354087\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.210007\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.176890\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.166621\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.243879\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.128499\n",
            "\n",
            "Test set: Average loss: 0.3266, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Acc over 10 instances: 91.41 +- 5.30\n",
            "TIME TAKEN (FINETUNE RESNET FC LAYERS W/WIDE HIDDEN & W/BATCHNORM):  63.80422830581665\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "import time as timer\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(256) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "start=timer.time()\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=10, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=10, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "  \n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "\n",
        "  model.fc = nn.Sequential(nn.Linear(512,18),nn.BatchNorm1d(18),nn.Sigmoid(),nn.Linear(18,  10))\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.fc.parameters(), momentum=0.9,\n",
        "                              lr=0.01, weight_decay=0.0005)\n",
        "  model.to(device)\n",
        "\n",
        "  # print(model)\n",
        "  # print(modelll)\n",
        "  for epoch in range(10):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "print('TIME TAKEN (FINETUNE RESNET FC LAYERS W/WIDE HIDDEN & W/BATCHNORM): ',str(timer.time()-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2\n",
        "\n",
        "Experimenting with Different Optimization Schedules on the baseline AlexNet.  \n",
        "\n",
        "We run the training many times, pairing different learning rates with different batch sizes to get a baseline. Then we will use learning rate scheduler's to implement an adaptive learning rate on the best performer."
      ],
      "metadata": {
        "id": "b7dn20jgFWJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QynPt6eA1EaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c6cea2-3164-4c7e-f49c-a90d60a09015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.116779\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000115\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000183\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.001575\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000015\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000211\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000019\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000457\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000180\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0439, Accuracy: 688/700 (98.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 2.661740\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.029912\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.179871\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.133870\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.004859\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.011382\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000185\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000584\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000150\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000351\n",
            "\n",
            "Test set: Average loss: 0.3966, Accuracy: 619/700 (88.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.499165\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.034342\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.006945\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.002206\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.001950\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.002518\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.001667\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.001708\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.001165\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000865\n",
            "\n",
            "Test set: Average loss: 0.3511, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.206401\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.005817\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000959\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000353\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000096\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000186\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000002\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000166\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000310\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000124\n",
            "\n",
            "Test set: Average loss: 0.0855, Accuracy: 677/700 (96.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.153732\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.129869\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.020421\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.009865\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.003460\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000426\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.001475\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.001605\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.002659\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.001917\n",
            "\n",
            "Test set: Average loss: 0.3567, Accuracy: 621/700 (88.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.493462\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.453810\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.002120\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.001800\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000178\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000392\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000076\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000036\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000019\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000136\n",
            "\n",
            "Test set: Average loss: 0.5885, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.550908\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000039\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.001067\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.016910\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000226\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000038\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000001\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 690/700 (98.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.025498\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.012354\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.001231\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.001096\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.001052\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000049\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000186\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000561\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000240\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000242\n",
            "\n",
            "Test set: Average loss: 0.0750, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.052081\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000627\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000005\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000064\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000070\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000358\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000814\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000857\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000229\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000561\n",
            "\n",
            "Test set: Average loss: 0.0600, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.357677\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.293684\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.038616\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.001373\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.001826\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.003058\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.001422\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.001207\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000606\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000499\n",
            "\n",
            "Test set: Average loss: 0.3613, Accuracy: 611/700 (87.29%)\n",
            "\n",
            "For LR=0.001 and Batch Size=10 Acc: 92.61 +- 5.19\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.297943\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.258763\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.005599\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.005746\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.023248\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000045\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.012735\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.007636\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.002139\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000024\n",
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 690/700 (98.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.900497\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.277699\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.465371\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.942231\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 1.003414\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.064057\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.007665\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.212086\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.008757\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.004687\n",
            "\n",
            "Test set: Average loss: 0.7965, Accuracy: 559/700 (79.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.930230\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.021924\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.005862\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.012982\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.011281\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000680\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000394\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.072425\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000077\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.001829\n",
            "\n",
            "Test set: Average loss: 0.3464, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.804073\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.025480\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.005753\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000021\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000088\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.003641\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000009\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000026\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000287\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000751\n",
            "\n",
            "Test set: Average loss: 0.0737, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.841726\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.156606\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.190863\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.017043\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.007038\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.034947\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.005287\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.002550\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.008568\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000306\n",
            "\n",
            "Test set: Average loss: 0.3278, Accuracy: 618/700 (88.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.620408\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.361542\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.005891\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.008529\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.008756\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.001381\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000203\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000083\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000348\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.001083\n",
            "\n",
            "Test set: Average loss: 0.5038, Accuracy: 590/700 (84.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.179986\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.266074\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.001619\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.002514\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000550\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.001585\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000148\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000047\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000803\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000694\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 693/700 (99.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.040130\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.018190\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.001991\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.002108\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.003687\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.006308\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000126\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000352\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.002256\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.001054\n",
            "\n",
            "Test set: Average loss: 0.0723, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.269684\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.046525\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.012627\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000628\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.135684\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000489\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000909\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.018473\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000026\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.002598\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.220523\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.255354\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.018297\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.118939\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.001943\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.004559\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.071294\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.002038\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.004269\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.005876\n",
            "\n",
            "Test set: Average loss: 0.4583, Accuracy: 597/700 (85.29%)\n",
            "\n",
            "For LR=0.001 and Batch Size=32 Acc: 91.53 +- 6.89\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.036778\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.301106\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.126563\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.037789\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.021664\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.016238\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.023335\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.009894\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.006172\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.003690\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 688/700 (98.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.308073\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.645176\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.321464\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.166885\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.158772\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.125966\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.066276\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.078796\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.060415\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.029160\n",
            "\n",
            "Test set: Average loss: 0.3078, Accuracy: 608/700 (86.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.321165\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.454417\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.255883\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.164017\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.118290\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.086999\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.022275\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.027258\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.032923\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.018252\n",
            "\n",
            "Test set: Average loss: 0.3018, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.201218\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.373150\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.162182\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.057640\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.059892\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.018417\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.011564\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.005316\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.004586\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.004029\n",
            "\n",
            "Test set: Average loss: 0.0985, Accuracy: 674/700 (96.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.379203\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.368079\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.366052\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.156744\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.222494\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.099365\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.087281\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.065959\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.042423\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.030124\n",
            "\n",
            "Test set: Average loss: 0.3228, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.426634\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.569860\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.378993\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.222892\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.110157\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.077718\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.070072\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.091754\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.045339\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.059919\n",
            "\n",
            "Test set: Average loss: 0.3774, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.313387\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.439986\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.156587\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.090874\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.024740\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.015201\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.011312\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.011061\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.004554\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.003096\n",
            "\n",
            "Test set: Average loss: 0.0475, Accuracy: 689/700 (98.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.385624\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.333897\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.111249\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.068520\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.025604\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.005901\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.018942\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.006576\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.006984\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.004846\n",
            "\n",
            "Test set: Average loss: 0.0831, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.964106\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.274306\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.095797\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.075381\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.042485\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.011387\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.024116\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.009226\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.007406\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.001968\n",
            "\n",
            "Test set: Average loss: 0.0576, Accuracy: 687/700 (98.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.513976\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.457740\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.304999\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.197775\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.110738\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.060932\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.054438\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.023496\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.020157\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.013720\n",
            "\n",
            "Test set: Average loss: 0.3647, Accuracy: 603/700 (86.14%)\n",
            "\n",
            "For LR=0.001 and Batch Size=64 Acc: 92.20 +- 5.47\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.400629\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.934652\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.416752\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.249164\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.163911\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.110811\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.076543\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.054954\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.040898\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.031165\n",
            "\n",
            "Test set: Average loss: 0.0774, Accuracy: 683/700 (97.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.406849\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.019090\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.636214\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.493069\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.394915\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.300173\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.235501\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.184408\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.143242\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.116649\n",
            "\n",
            "Test set: Average loss: 0.3373, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.298138\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.204678\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.637566\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.430349\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.308174\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.226611\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.171037\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.134923\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.109823\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.090662\n",
            "\n",
            "Test set: Average loss: 0.2985, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.542763\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.171236\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.601992\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.348670\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.200318\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.168995\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.108191\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.059646\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.039936\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.033728\n",
            "\n",
            "Test set: Average loss: 0.0973, Accuracy: 682/700 (97.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.569609\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.215680\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.598995\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.443572\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.357414\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.295985\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.252363\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.215998\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.184209\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.157203\n",
            "\n",
            "Test set: Average loss: 0.3189, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.703780\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.366902\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.642154\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.432355\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.356061\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.277908\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.218835\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.189730\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.162520\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.134635\n",
            "\n",
            "Test set: Average loss: 0.3678, Accuracy: 593/700 (84.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.327515\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.979674\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.445497\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.238046\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.145724\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.093209\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.057154\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.037142\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.026675\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.020503\n",
            "\n",
            "Test set: Average loss: 0.0708, Accuracy: 691/700 (98.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.405840\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.107007\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.459587\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.236689\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.173326\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.118687\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.071363\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.046829\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.036055\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.029951\n",
            "\n",
            "Test set: Average loss: 0.1165, Accuracy: 672/700 (96.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.513821\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.099749\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.456987\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.281156\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.166565\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.105551\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.076928\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.056928\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.041209\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.030042\n",
            "\n",
            "Test set: Average loss: 0.0800, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.460343\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.367495\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.654855\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.436418\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.306244\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.236456\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.180626\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.136733\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.110381\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.093030\n",
            "\n",
            "Test set: Average loss: 0.3742, Accuracy: 583/700 (83.29%)\n",
            "\n",
            "For LR=0.001 and Batch Size=128 Acc: 91.63 +- 6.04\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000223\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.3432, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 3.116477\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000011\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.001274\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.1114, Accuracy: 593/700 (84.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.312710\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.447543\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000041\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.8217, Accuracy: 604/700 (86.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 8.074660\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000005\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.6128, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.836028\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 1.914117\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.004529\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000128\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.0227, Accuracy: 617/700 (88.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 8.959224\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.0681, Accuracy: 597/700 (85.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.4968, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 2.754678\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000018\n",
            "\n",
            "Test set: Average loss: 2.3736, Accuracy: 646/700 (92.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 5.034041\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000003\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.2879, Accuracy: 667/700 (95.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 1.034914\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000271\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.1074, Accuracy: 605/700 (86.43%)\n",
            "\n",
            "For LR=0.01 and Batch Size=10 Acc: 91.07 +- 5.18\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.942294\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.018075\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.2015, Accuracy: 690/700 (98.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.389073\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.741417\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 14.811361\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 6.5867, Accuracy: 588/700 (84.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.222698\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000011\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000008\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000243\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.3811, Accuracy: 594/700 (84.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.001744\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000003\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.9329, Accuracy: 657/700 (93.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 3.945540\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 5.643228\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 12.071638\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 5.204062\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 15.566216\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.6266, Accuracy: 608/700 (86.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 4.214048\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.353363\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.006411\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.0306, Accuracy: 572/700 (81.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 1.707572\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.028799\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 8.496475\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.421238\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.3420, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.581169\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.001689\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.3464, Accuracy: 682/700 (97.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 2.089730\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 2.186086\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.176893\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.4730, Accuracy: 682/700 (97.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.116789\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 1.475001\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.838370\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.139282\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000063\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.1620, Accuracy: 577/700 (82.43%)\n",
            "\n",
            "For LR=0.01 and Batch Size=32 Acc: 90.51 +- 6.77\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.194526\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.026055\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.002171\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000373\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000079\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000127\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000008\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000047\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000036\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 0.0641, Accuracy: 691/700 (98.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.535932\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 10.641270\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 11.696033\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.916291\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 1.642431\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 1.942839\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 1.525135\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 1.831486\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 2.275941\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.6147, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.440111\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 7.250974\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 3.373363\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.310088\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 1.712592\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.014308\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.436860\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.426319\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.237195\n",
            "\n",
            "Test set: Average loss: 2.8309, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 5.856812\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.006380\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.268977\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.621519\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000008\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000002\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.8689, Accuracy: 660/700 (94.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 2.360329\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 15.487064\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 4.969060\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.661645\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.703197\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 3.338154\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.814511\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 1.092681\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000042\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.017618\n",
            "\n",
            "Test set: Average loss: 2.5003, Accuracy: 618/700 (88.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.032730\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.329107\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.223285\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 1.588764\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.030750\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.357522\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000173\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000020\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.036047\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000058\n",
            "\n",
            "Test set: Average loss: 1.9087, Accuracy: 600/700 (85.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.167781\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.002398\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.044751\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000370\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000010\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.0271, Accuracy: 694/700 (99.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.369744\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.022851\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.534130\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.005736\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.025958\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 0.2770, Accuracy: 683/700 (97.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 2.874758\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 6.239948\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 9.442183\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 1.237567\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 2.870127\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 1.147427\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.5293, Accuracy: 677/700 (96.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 3.248724\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 4.645586\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 6.338107\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.371328\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 1.621647\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.775757\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.066359\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.644215\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.010013\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.586999\n",
            "\n",
            "Test set: Average loss: 4.2400, Accuracy: 568/700 (81.14%)\n",
            "\n",
            "For LR=0.01 and Batch Size=64 Acc: 91.29 +- 6.37\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.168711\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.157434\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.271758\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.032005\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.143418\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.015179\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.000394\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000021\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000013\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000030\n",
            "\n",
            "Test set: Average loss: 0.0498, Accuracy: 690/700 (98.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.590892\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.519858\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 7.861777\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.838246\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.447953\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.230317\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.046930\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.143111\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.163497\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.044223\n",
            "\n",
            "Test set: Average loss: 0.8626, Accuracy: 605/700 (86.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.221025\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.281635\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.814049\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.539576\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.511554\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.292133\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.073327\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.220522\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.560225\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.300502\n",
            "\n",
            "Test set: Average loss: 0.9844, Accuracy: 605/700 (86.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.446093\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.549030\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 7.066801\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.005705\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.883479\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.625019\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.088847\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000196\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000583\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.043512\n",
            "\n",
            "Test set: Average loss: 0.7028, Accuracy: 652/700 (93.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.306059\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.501595\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 4.091440\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 3.908566\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.554331\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.035312\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.521094\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.578604\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.253744\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.583848\n",
            "\n",
            "Test set: Average loss: 1.8808, Accuracy: 563/700 (80.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.619272\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.738631\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 6.208803\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.210272\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.649813\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.154989\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.997514\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.034181\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.316627\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.028434\n",
            "\n",
            "Test set: Average loss: 2.5656, Accuracy: 547/700 (78.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.391506\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.713928\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 5.170696\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.313198\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.540767\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.144130\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.000005\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.003436\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.109382\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.245276\n",
            "\n",
            "Test set: Average loss: 1.3150, Accuracy: 621/700 (88.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.632391\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.025022\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 4.385868\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.018884\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.734087\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.577271\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.002186\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000002\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.002097\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.046060\n",
            "\n",
            "Test set: Average loss: 0.3385, Accuracy: 674/700 (96.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.026278\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.253352\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 7.385629\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.020412\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.745514\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.166574\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.386611\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.072860\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000265\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.012766\n",
            "\n",
            "Test set: Average loss: 0.2124, Accuracy: 677/700 (96.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.555567\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.682625\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 5.179694\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.252866\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.682915\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.046740\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.272127\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.587935\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.278234\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.070112\n",
            "\n",
            "Test set: Average loss: 1.0457, Accuracy: 611/700 (87.29%)\n",
            "\n",
            "For LR=0.01 and Batch Size=128 Acc: 89.21 +- 6.55\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.1453, Accuracy: 682/700 (97.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 28.834270\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 40.176891\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 7.310266\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 38.1028, Accuracy: 569/700 (81.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 65.371849\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000050\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 31.1197, Accuracy: 606/700 (86.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.0026, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 7.847039\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 51.968239\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 14.266266\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 25.635675\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 31.968594\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 24.8851, Accuracy: 602/700 (86.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 5.787652\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 8.938528\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 22.3651, Accuracy: 588/700 (84.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 28.328970\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.0651, Accuracy: 661/700 (94.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.5475, Accuracy: 676/700 (96.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.969678\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.7229, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 13.360922\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 18.911163\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 18.1865, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "For LR=0.05 and Batch Size=10 Acc: 90.61 +- 6.16\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 8.566532\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 74.692101\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 200.241806\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.3387, Accuracy: 661/700 (94.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 2.574454\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 14.377647\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 2.307001\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 15.9979, Accuracy: 604/700 (86.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 9.938286\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 29.602371\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 13.6108, Accuracy: 604/700 (86.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 59.571640\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 6.845016\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 93.313057\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.4235, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 30.540655\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 19.171341\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 11.5160, Accuracy: 596/700 (85.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 67.637787\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 12.014626\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 10.945038\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 19.2599, Accuracy: 565/700 (80.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.6364, Accuracy: 668/700 (95.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000702\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.1662, Accuracy: 648/700 (92.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 4.528961\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 2.587136\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 9.523422\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.6676, Accuracy: 687/700 (98.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 8.766178\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000331\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 15.9263, Accuracy: 562/700 (80.29%)\n",
            "\n",
            "For LR=0.05 and Batch Size=32 Acc: 89.71 +- 6.44\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 14.741599\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 6.839412\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 21.274681\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 1.909737\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 1.151320\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 2.976305\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.8871, Accuracy: 690/700 (98.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 9.137119\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 71.635635\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 38.818878\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 1.004663\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 9.118191\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 22.011106\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 10.459808\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 5.653475\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.553484\n",
            "\n",
            "Test set: Average loss: 13.7988, Accuracy: 595/700 (85.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.375415\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 27.049099\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 6.097352\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000003\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000002\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 1.446699\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 1.186966\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.2059, Accuracy: 614/700 (87.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.120024\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.339317\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.034310\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.4464, Accuracy: 683/700 (97.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 7.984194\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 44.912235\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 40.588989\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 2.332128\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 47.706566\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 3.220138\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 13.155346\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 11.416537\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 1.057843\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 1.479190\n",
            "\n",
            "Test set: Average loss: 24.0513, Accuracy: 583/700 (83.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 6.530668\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 11.355052\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 35.152077\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 5.539566\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 11.635741\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 1.301503\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 10.721087\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 2.221590\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 1.052248\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 3.015288\n",
            "\n",
            "Test set: Average loss: 14.1578, Accuracy: 586/700 (83.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 1.126928\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 34.072731\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.179806\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.269567\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.0111, Accuracy: 676/700 (96.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 0.030268\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 0.020337\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.6912, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 17.349566\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 13.255219\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 23.678385\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000001\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 2.335002\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.696323\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 8.215235\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.2296, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 4.097269\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 6.528823\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 45.714867\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 2.166791\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 4.467503\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.918752\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 12.416050\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 1.465558\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.357107\n",
            "\n",
            "Test set: Average loss: 13.7660, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "For LR=0.05 and Batch Size=64 Acc: 91.40 +- 6.25\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.526810\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 4.086785\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 37.330605\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.000166\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.842621\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 4.627349\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.676059\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000054\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.4682, Accuracy: 691/700 (98.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.110722\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 13.427238\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 78.969948\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 44.470837\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 55.948677\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 54.255924\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.888845\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 44.425793\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 28.946016\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.112560\n",
            "\n",
            "Test set: Average loss: 25.9717, Accuracy: 498/700 (71.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.054233\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.309435\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 36.672348\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.449369\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 19.374231\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.256142\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 5.379708\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 5.947192\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.274353\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.029285\n",
            "\n",
            "Test set: Average loss: 7.8473, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.453263\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.458515\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 28.521118\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.832463\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 18.433636\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.091141\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.339952\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 3.371206\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 4.740905\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.941392\n",
            "\n",
            "Test set: Average loss: 4.3335, Accuracy: 646/700 (92.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.215712\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.808230\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 14.801992\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 33.673977\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 6.694702\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 12.695734\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 17.011086\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 3.469040\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 3.460033\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 9.147669\n",
            "\n",
            "Test set: Average loss: 20.4816, Accuracy: 508/700 (72.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.950682\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.807651\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 39.507145\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.288206\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 20.790108\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.585158\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 3.596215\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 7.711154\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 3.547544\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.024517\n",
            "\n",
            "Test set: Average loss: 9.6711, Accuracy: 572/700 (81.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.260146\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.028042\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.000428\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.001229\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.001239\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.000439\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.000125\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000039\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000015\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 0.5339, Accuracy: 667/700 (95.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.547116\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 5.076419\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 31.065407\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.000068\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.614991\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 3.593341\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.919708\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.036485\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.2742, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.464984\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 4.281915\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 50.275005\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 16.408115\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.688275\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 29.622444\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 5.582328\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.057691\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.030382\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 3.269729\n",
            "\n",
            "Test set: Average loss: 16.3879, Accuracy: 574/700 (82.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.491765\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.170607\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.559863\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 25.777992\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.392728\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 9.144440\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.492238\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.073495\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.350472\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.799184\n",
            "\n",
            "Test set: Average loss: 11.2473, Accuracy: 547/700 (78.14%)\n",
            "\n",
            "For LR=0.05 and Batch Size=128 Acc: 85.33 +- 9.54\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.001316\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.658659\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 8.096777\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.8365, Accuracy: 674/700 (96.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 5.296240\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 84.678772\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 40.0218, Accuracy: 616/700 (88.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 36.942085\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 61.5811, Accuracy: 592/700 (84.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 50.210682\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.087868\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 8.346814\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.1757, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 226.656174\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 27.784826\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 47.3468, Accuracy: 605/700 (86.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 19.447887\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 2.082251\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 22.576031\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 70.4394, Accuracy: 569/700 (81.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.6627, Accuracy: 659/700 (94.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.2596, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 20.163275\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 10.7113, Accuracy: 669/700 (95.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [90/100 (90%)]\tLoss: 16.149330\n",
            "Train Epoch: 1 [90/100 (90%)]\tLoss: 5.235461\n",
            "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 28.6106, Accuracy: 595/700 (85.00%)\n",
            "\n",
            "For LR=0.1 and Batch Size=10 Acc: 90.64 +- 5.88\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 1.723040\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 41.065170\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.1032, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 96.274498\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 85.964073\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 111.942734\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 31.6324, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 31.705193\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 1.551408\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 13.613434\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 56.3917, Accuracy: 583/700 (83.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 55.679367\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 48.188721\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 8.5080, Accuracy: 675/700 (96.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.283182\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 90.125671\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 13.686157\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 23.8066, Accuracy: 597/700 (85.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 30.234997\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 59.644714\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 43.8145, Accuracy: 559/700 (79.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.9007, Accuracy: 688/700 (98.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.7131, Accuracy: 683/700 (97.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 96.754227\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.9806, Accuracy: 692/700 (98.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [12/100 (75%)]\tLoss: 49.294853\n",
            "Train Epoch: 1 [12/100 (75%)]\tLoss: 31.547897\n",
            "Train Epoch: 2 [12/100 (75%)]\tLoss: 56.977692\n",
            "Train Epoch: 3 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [12/100 (75%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [12/100 (75%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 54.9365, Accuracy: 606/700 (86.57%)\n",
            "\n",
            "For LR=0.1 and Batch Size=32 Acc: 90.97 +- 6.90\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 2.827648\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 5.049790\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.005767\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 11.796062\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 1.073137\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 1.064928\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.6416, Accuracy: 675/700 (96.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 17.881905\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 241.308105\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 132.568298\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 29.857592\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 29.821831\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 40.713917\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 5.284312\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 9.858693\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 20.411974\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.567554\n",
            "\n",
            "Test set: Average loss: 28.2412, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 8.668880\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 12.253056\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 4.505694\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 2.991404\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 21.254427\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.431773\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 3.108266\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 1.010139\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 15.9005, Accuracy: 620/700 (88.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 26.233309\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 3.272076\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 6.052462\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 2.467487\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 9.230815\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.3379, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 8.729971\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 137.802551\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 64.432869\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 10.660403\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 27.657751\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 3.704497\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 9.015469\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 5.266605\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 3.995751\n",
            "\n",
            "Test set: Average loss: 31.9905, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 6.180704\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 49.423401\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 141.190338\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 10.158245\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 51.479782\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 14.666395\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 5.830566\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 19.467827\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.753235\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.045415\n",
            "\n",
            "Test set: Average loss: 34.3829, Accuracy: 588/700 (84.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 33.842831\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 57.092094\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 75.941833\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 2.895405\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 8.286640\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 12.368137\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.233561\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 6.7145, Accuracy: 666/700 (95.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 29.366276\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 1.084596\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 12.588697\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 6.9847, Accuracy: 667/700 (95.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 2.839435\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 35.831596\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 0.000001\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 1.070241\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 3.6795, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [36/100 (50%)]\tLoss: 5.763139\n",
            "Train Epoch: 1 [36/100 (50%)]\tLoss: 124.505424\n",
            "Train Epoch: 2 [36/100 (50%)]\tLoss: 60.038147\n",
            "Train Epoch: 3 [36/100 (50%)]\tLoss: 5.515368\n",
            "Train Epoch: 4 [36/100 (50%)]\tLoss: 13.326221\n",
            "Train Epoch: 5 [36/100 (50%)]\tLoss: 8.942547\n",
            "Train Epoch: 6 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [36/100 (50%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [36/100 (50%)]\tLoss: 5.689321\n",
            "\n",
            "Test set: Average loss: 50.6907, Accuracy: 546/700 (78.00%)\n",
            "\n",
            "For LR=0.1 and Batch Size=64 Acc: 90.30 +- 6.51\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.884612\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 3.328781\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 41.050758\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 6.291836\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 29.801678\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.271338\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.006593\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 4.955527\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 8.051270\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 4.697268\n",
            "\n",
            "Test set: Average loss: 3.8725, Accuracy: 664/700 (94.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.202967\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.846819\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 133.962250\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 24.494390\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 224.051605\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 250.315125\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 35.060982\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 118.377419\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 170.066986\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 19.743864\n",
            "\n",
            "Test set: Average loss: 43.0440, Accuracy: 537/700 (76.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.568438\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 13.604449\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 105.641205\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 34.374638\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 34.591793\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 36.238926\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 2.644755\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 8.232455\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 18.493008\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 17.149403\n",
            "\n",
            "Test set: Average loss: 32.9413, Accuracy: 594/700 (84.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.266259\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 7.291146\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 97.807999\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.897921\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 3.113795\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 18.080145\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 3.150250\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.417042\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.3741, Accuracy: 667/700 (95.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.072380\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 11.774513\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 103.947754\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 19.429565\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 58.094128\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 46.527611\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 6.650384\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 24.014780\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 35.958790\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 18.495220\n",
            "\n",
            "Test set: Average loss: 20.4151, Accuracy: 604/700 (86.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.193276\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.932889\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 49.607250\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 10.470219\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.169010\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 3.007890\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 3.848052\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.035702\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.197242\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.754672\n",
            "\n",
            "Test set: Average loss: 22.8951, Accuracy: 541/700 (77.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.416220\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 7.130583\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 81.707390\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 5.991200\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 8.365640\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.727949\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.079380\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.8309, Accuracy: 679/700 (97.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.607781\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 17.770786\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 43.033237\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 6.275957\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 7.724148\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.137745\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 2.1022, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.377256\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.942380\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.450715\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 3.627751\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.281423\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.022288\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.062857\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000187\n",
            "\n",
            "Test set: Average loss: 1.0768, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.227553\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.538180\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 20.514812\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 56.226933\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 16.695276\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 9.877398\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 27.242912\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.062236\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.536264\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 4.057698\n",
            "\n",
            "Test set: Average loss: 48.0579, Accuracy: 486/700 (69.43%)\n",
            "\n",
            "For LR=0.1 and Batch Size=128 Acc: 87.69 +- 9.82\n",
            "For LR=0.001 and Batch Size=10 Acc: 92.61 +- 5.19 \n",
            "\n",
            "For LR=0.001 and Batch Size=32 Acc: 91.53 +- 6.89 \n",
            "\n",
            "For LR=0.001 and Batch Size=64 Acc: 92.20 +- 5.47 \n",
            "\n",
            "For LR=0.001 and Batch Size=128 Acc: 91.63 +- 6.04 \n",
            "\n",
            "For LR=0.01 and Batch Size=10 Acc: 91.07 +- 5.18 \n",
            "\n",
            "For LR=0.01 and Batch Size=32 Acc: 90.51 +- 6.77 \n",
            "\n",
            "For LR=0.01 and Batch Size=64 Acc: 91.29 +- 6.37 \n",
            "\n",
            "For LR=0.01 and Batch Size=128 Acc: 89.21 +- 6.55 \n",
            "\n",
            "For LR=0.05 and Batch Size=10 Acc: 90.61 +- 6.16 \n",
            "\n",
            "For LR=0.05 and Batch Size=32 Acc: 89.71 +- 6.44 \n",
            "\n",
            "For LR=0.05 and Batch Size=64 Acc: 91.40 +- 6.25 \n",
            "\n",
            "For LR=0.05 and Batch Size=128 Acc: 85.33 +- 9.54 \n",
            "\n",
            "For LR=0.1 and Batch Size=10 Acc: 90.64 +- 5.88 \n",
            "\n",
            "For LR=0.1 and Batch Size=32 Acc: 90.97 +- 6.90 \n",
            "\n",
            "For LR=0.1 and Batch Size=64 Acc: 90.30 +- 6.51 \n",
            "\n",
            "For LR=0.1 and Batch Size=128 Acc: 87.69 +- 9.82 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "lr_list = [0.001, 0.01, 0.05, 0.1]\n",
        "batch_size_list = [10, 32, 64, 128]\n",
        "outputs = []\n",
        "\n",
        "for i in lr_list:\n",
        "  for j in batch_size_list:\n",
        "    accs = []\n",
        "\n",
        "    for seed in range(10):\n",
        "      prng = RandomState(seed)\n",
        "      random_permute = prng.permutation(np.arange(0, 1000))\n",
        "      classes =  prng.permutation(np.arange(0,10))\n",
        "      indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "      indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "      train_data = Subset(cifar_data, indx_train)\n",
        "      val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "      print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "      \n",
        "      train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                                batch_size=j, \n",
        "                                                shuffle=True)\n",
        "\n",
        "      val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                              batch_size=j, \n",
        "                                              shuffle=False)\n",
        "      \n",
        "      model = models.alexnet(pretrained=True)\n",
        "      model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "      optimizer = torch.optim.SGD(model.classifier.parameters(), \n",
        "                                  lr=i, momentum=0.9,\n",
        "                                  weight_decay=0.0005)\n",
        "      model.to(device)\n",
        "      for epoch in range(10):\n",
        "        train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "        \n",
        "      accs.append(test(model, device, val_loader))\n",
        "\n",
        "    accs = np.array(accs)\n",
        "    resOut = 'For LR=' +  str(i) + ' and Batch Size=' +  str(j) + str(' Acc: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "    print(resOut)\n",
        "    outputs.append(resOut)\n",
        "\n",
        "for i in outputs:\n",
        "  print(i,'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing with Learning Rate Schedulers"
      ],
      "metadata": {
        "id": "5XbYwBRXh3_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing using LambdaLR with SGD Optimizer, LR=0.01 and BatchSize=128 "
      ],
      "metadata": {
        "id": "IpVyIDwsjEsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = models.alexnet(pretrained=True)\n",
        "  model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  optimizer = torch.optim.SGD(model.classifier.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  \n",
        "  #LAMBDALR \n",
        "  lambda1 = lambda epoch: epoch // 2\n",
        "  scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "  model.to(device)\n",
        "  for epoch in range(20):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    scheduler.step()\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "id": "JI9zaN3aKAmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e483598-2466-41cf-b543-0121f702d5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.129553\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.129553\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.129553\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.283324\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.016265\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.011078\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.054504\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000326\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000002\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.4879, Accuracy: 691/700 (98.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.841288\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.841288\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.841288\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.171253\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.185900\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.444350\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.286888\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.018808\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000021\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000190\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.002299\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.005793\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000776\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000170\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000048\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000012\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000001\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.5871, Accuracy: 587/700 (83.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.993299\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.993299\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.993299\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.138056\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.128913\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.055850\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.031342\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.005989\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000574\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000043\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.000005\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.6457, Accuracy: 609/700 (87.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.454918\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.454918\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.454918\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.560724\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.320460\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 3.251641\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.013946\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.094553\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.476364\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.757871\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.429187\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.080717\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 1.0997, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.670979\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.670979\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.670979\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.656248\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.048312\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.412457\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.581396\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.205396\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.124447\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.021194\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.000065\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 5.8108, Accuracy: 606/700 (86.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.387043\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.387043\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.387044\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.102857\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.495373\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.382250\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.784318\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.638801\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.222535\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.428725\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.249065\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.002868\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000014\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.003922\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.017278\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000114\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000031\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000023\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000018\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000014\n",
            "\n",
            "Test set: Average loss: 14.2505, Accuracy: 558/700 (79.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.357047\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.357046\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.357046\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.049407\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.005257\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.000134\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.000027\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000003\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000001\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.8968, Accuracy: 674/700 (96.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.324843\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.324843\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.324843\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.215032\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.019047\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.594768\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.175076\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.014956\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.074350\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.069672\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.003451\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 4.9853, Accuracy: 647/700 (92.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.536283\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.536283\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.536283\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.445308\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.021505\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.126686\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.177910\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.072130\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.003669\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 0.5505, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.497439\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.497438\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.497438\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.772413\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.113822\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.131058\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.166691\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.305644\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.112780\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.015593\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.022428\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.002042\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.000034\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 7.0683, Accuracy: 609/700 (87.00%)\n",
            "\n",
            "Acc over 10 instances: 90.73 +- 6.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing using LambdaLR scheduler with Adagrad Optimizer, LR=0.001 and BatchSize=128. Requires 20 epochs to converge."
      ],
      "metadata": {
        "id": "4k1EtXVmsvgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = models.alexnet(pretrained=True)\n",
        "  model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adagrad(model.classifier.parameters(), \n",
        "                              lr=0.001)\n",
        "  \n",
        "  #LAMBDALR \n",
        "  lambda1 = lambda epoch: epoch // 2\n",
        "  scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        " \n",
        "  for epoch in range(20):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    scheduler.step()\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkrpYsFFlNaW",
        "outputId": "0210d696-34fd-48da-91ea-166418996fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.521478\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.521478\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.521479\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.767196\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.463297\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.505100\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.533771\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.674837\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.319889\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.066403\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.030149\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.025427\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.022821\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.020483\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.018665\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.016960\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.015581\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014280\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013203\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012183\n",
            "\n",
            "Test set: Average loss: 0.0676, Accuracy: 692/700 (98.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 3.076972\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 3.076972\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 3.076972\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.030903\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.856980\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.526479\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.393298\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.447708\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.740783\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.599650\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.238424\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 1.695440\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.844865\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.610839\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.275280\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.050539\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.024969\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.021803\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.020310\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.019040\n",
            "\n",
            "Test set: Average loss: 0.2731, Accuracy: 622/700 (88.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.407825\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.407825\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.407825\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.768513\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.557285\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.520114\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.873987\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.031207\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.668399\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.710955\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.238032\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.102376\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.039286\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.034144\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.030544\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.027365\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.024932\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.022732\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.020971\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.019344\n",
            "\n",
            "Test set: Average loss: 0.3095, Accuracy: 597/700 (85.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.245054\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.245054\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.245054\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.721049\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.445453\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.442745\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.655184\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.180964\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.694975\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.205198\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.021422\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.019267\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.017625\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.016055\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.014796\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.013592\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.012603\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.011659\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.010868\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.010114\n",
            "\n",
            "Test set: Average loss: 0.0874, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.187176\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.187176\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.187176\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.830483\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.626882\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.123391\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.456168\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.738578\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.750427\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.540578\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.238479\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.124881\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.075740\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.060357\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.052010\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.045642\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.041006\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.036926\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.033715\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.030783\n",
            "\n",
            "Test set: Average loss: 0.3443, Accuracy: 602/700 (86.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.739332\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.739332\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.739333\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.918128\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.665847\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.525348\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.734771\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.634170\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.403587\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.767062\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.373568\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.192068\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.069762\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.057478\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.051005\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.045416\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.041120\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.037229\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.034118\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.031254\n",
            "\n",
            "Test set: Average loss: 0.3401, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.315102\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.315102\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.315101\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.710441\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.411149\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.225596\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.166639\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.147233\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.158489\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.205364\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.307076\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.402539\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.471371\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.069447\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.004923\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.004178\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.003674\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.003260\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.002955\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.002692\n",
            "\n",
            "Test set: Average loss: 0.0693, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.179883\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.179883\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.179883\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.691171\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.414087\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.268044\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.213377\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.285258\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.170360\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.150483\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.031179\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.022907\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.020554\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.018365\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.016641\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.015017\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.013703\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.012465\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.011445\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.010484\n",
            "\n",
            "Test set: Average loss: 0.0829, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.468246\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.468246\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.468246\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.789993\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.459140\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.414547\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.570765\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.063486\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.713807\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.198665\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.023950\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.019110\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.016971\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.015323\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.014114\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.012992\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.012082\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.011218\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.010497\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.009809\n",
            "\n",
            "Test set: Average loss: 0.0909, Accuracy: 679/700 (97.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.413557\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.413557\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.413557\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.863282\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.627757\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.562874\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.800587\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.402056\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.888195\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.547979\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.218613\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.059210\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.040643\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.036134\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.032879\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.029899\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.027529\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.025313\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.023483\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.021751\n",
            "\n",
            "Test set: Average loss: 0.3165, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Acc over 10 instances: 92.23 +- 5.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing using LambdaLR, Adagrad Optimizer, LR=0.0001 and BatchSize=128. Requires 40 epochs to converge.  "
      ],
      "metadata": {
        "id": "FCgWnAy9s999"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = models.alexnet(pretrained=True)\n",
        "  model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adagrad(model.classifier.parameters(), \n",
        "                              lr=0.0001)\n",
        "  \n",
        "  #LAMBDALR \n",
        "  lambda1 = lambda epoch: epoch // 2\n",
        "  scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        " \n",
        "  for epoch in range(40):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    scheduler.step()\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl4vg1OGoslm",
        "outputId": "07e22ace-e27d-41cd-e117-e635ca7a70b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.384259\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.384259\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.384259\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.054533\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.818138\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.475598\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.247474\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.009094\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.848499\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.697813\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.593834\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.500558\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.433781\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.374120\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.329476\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.289241\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.258001\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.229602\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.206923\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.186162\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.169213\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.153608\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.140639\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.128637\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.118516\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.109106\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.101071\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.093571\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.087098\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.081035\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.075754\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.070790\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.066431\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.062323\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.058688\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.055253\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.052195\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.049297\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.046701\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.044237\n",
            "\n",
            "Test set: Average loss: 0.1304, Accuracy: 687/700 (98.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.066511\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.066511\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.066511\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.747584\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.532415\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.243488\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.073254\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.912236\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.812591\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.720397\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.654108\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.590139\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.539972\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.491133\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.451325\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.412714\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.380572\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.349562\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.323373\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.298214\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.276726\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.256148\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.238410\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.221460\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.206733\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.192681\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.180387\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.168665\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.158346\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.148508\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.139797\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.131492\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.124098\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.117044\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.110733\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.104708\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.099291\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.094116\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.089442\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.084971\n",
            "\n",
            "Test set: Average loss: 0.3702, Accuracy: 609/700 (87.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.131305\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.131305\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.131305\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.836497\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.630582\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.341656\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.158786\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.974162\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.851388\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.733490\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.647984\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.566839\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.505187\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.447358\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.402158\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.360021\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.326305\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.294894\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.269228\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.245261\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.225318\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.206637\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.190852\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.176022\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.163333\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.151381\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.141046\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.131291\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.122782\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.114735\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.107662\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.100963\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.095034\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.089409\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.084402\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.079643\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.075384\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.071330\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.067683\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.064206\n",
            "\n",
            "Test set: Average loss: 0.3265, Accuracy: 612/700 (87.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.063094\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.063094\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.063093\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.750137\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.530977\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.224250\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.031895\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.840138\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.715161\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.598007\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.515663\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.439844\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.384023\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.333053\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.294237\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.258848\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.231141\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.205825\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.185537\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.166929\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.151720\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.137709\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.126066\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.115293\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.106213\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.097777\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.090578\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.083865\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.078075\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.072655\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.067937\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.063507\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.059617\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.055953\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.052713\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.049652\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.046926\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.044344\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.042031\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.039834\n",
            "\n",
            "Test set: Average loss: 0.1465, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.597439\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.597438\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.597439\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.246783\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.983898\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.596326\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.344070\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.097424\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.944383\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.806386\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.711917\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.625645\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.562002\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.503305\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.457845\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.415524\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.381523\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.349588\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.323199\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.298237\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.277163\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.257127\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.239933\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.223533\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.209284\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.195666\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.183721\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.172293\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.162192\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.152524\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.143924\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.135690\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.128327\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.121275\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.114939\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.108867\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.103389\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.098136\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.093378\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.088814\n",
            "\n",
            "Test set: Average loss: 0.3584, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.544058\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.544058\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.544058\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.222002\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.980884\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.624356\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.389144\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.154206\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.004282\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.865923\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.768931\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.678667\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.610890\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.547530\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.497926\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.451447\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.413977\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.378765\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.349707\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.322295\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.299231\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.277385\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.258709\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.240957\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.225581\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.210925\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.198095\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.185840\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.175020\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.164668\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.155463\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.146647\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.138761\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.131203\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.124407\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.117889\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.112003\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.106355\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.101233\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.096315\n",
            "\n",
            "Test set: Average loss: 0.3641, Accuracy: 612/700 (87.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.264558\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.264558\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.264559\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.916571\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.665190\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.306566\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.080318\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.859540\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.721357\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.596403\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.511317\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.434487\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.378691\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.328108\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.289734\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.254784\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.227409\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.202368\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.182272\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.163817\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.148720\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.134803\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.123236\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.112534\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.103517\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.095145\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.088007\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.081355\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.075625\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.070267\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.065608\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.061238\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.057406\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.053802\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.050617\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.047613\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.044942\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.042414\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.040153\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.038009\n",
            "\n",
            "Test set: Average loss: 0.1210, Accuracy: 688/700 (98.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.046162\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.046162\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.046162\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.745217\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.529527\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.221849\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.025445\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.829254\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.702071\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.583726\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.501238\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.425798\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.370614\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.320488\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.282499\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.248000\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.221084\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.196559\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.176955\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.159011\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.144373\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.130911\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.119742\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.109422\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.100736\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.092675\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.085806\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.079405\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.073891\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.068735\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.064249\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.060040\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.056347\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.052871\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.049798\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.046896\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.044314\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.041868\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.039679\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.037600\n",
            "\n",
            "Test set: Average loss: 0.1276, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 1.961949\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.961949\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 1.961949\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.669099\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.467867\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.185595\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.004461\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.819060\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.695625\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.579264\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.497624\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.422884\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.368249\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.318674\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.281132\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.247045\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.220447\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.196200\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.176806\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.159042\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.144540\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.131191\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.120107\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.109857\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.101222\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.093202\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.086361\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.079983\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.074482\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.069334\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.064852\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.060643\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.056947\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.053465\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.050385\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.047475\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.044883\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.042427\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.040226\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.038136\n",
            "\n",
            "Test set: Average loss: 0.1433, Accuracy: 679/700 (97.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.169483\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.169482\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.169482\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 1.899823\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 1.702975\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.415368\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.224643\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.028440\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.897607\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.773081\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.683816\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.599692\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.535965\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.476128\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.429193\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.385234\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.349869\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.316753\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.289563\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.264070\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.242784\n",
            "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.222792\n",
            "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.205865\n",
            "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.189937\n",
            "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.176292\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.163429\n",
            "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.152299\n",
            "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.141789\n",
            "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.132618\n",
            "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.123943\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.116317\n",
            "Train Epoch: 31 [0/100 (0%)]\tLoss: 0.109091\n",
            "Train Epoch: 32 [0/100 (0%)]\tLoss: 0.102697\n",
            "Train Epoch: 33 [0/100 (0%)]\tLoss: 0.096630\n",
            "Train Epoch: 34 [0/100 (0%)]\tLoss: 0.091228\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.086094\n",
            "Train Epoch: 36 [0/100 (0%)]\tLoss: 0.081499\n",
            "Train Epoch: 37 [0/100 (0%)]\tLoss: 0.077125\n",
            "Train Epoch: 38 [0/100 (0%)]\tLoss: 0.073190\n",
            "Train Epoch: 39 [0/100 (0%)]\tLoss: 0.069438\n",
            "\n",
            "Test set: Average loss: 0.3618, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Acc over 10 instances: 92.21 +- 5.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with ExponentialLR Scheduler, Adagrad Optimizer, LR=0.001, BatchSize=128. "
      ],
      "metadata": {
        "id": "WEPPz_-Avf38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(10):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = models.alexnet(pretrained=True)\n",
        "  model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adagrad(model.classifier.parameters(), \n",
        "                              lr=0.001)\n",
        "  \n",
        "  #ReduceOnPlateau\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        " \n",
        "  for epoch in range(20):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    scheduler.step()\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d4b787888c7407aac519631fb645648",
            "c4adafedbd94454689456279df1a32dd",
            "f594cdce94c446f584536ee931333e0a",
            "1d79ea869737458d8599ae0272047240",
            "3668b1769e914ee3863a6efd1462b6f3",
            "d949ca5acbef4d5c983efe894f6fa25d",
            "3a3fad75973740e597a0937084c7d536",
            "09de081fa874485fa5c8cd63764e8af3",
            "be6fc61a60664968b08488b37fb7c1cc",
            "e0942689a8d44ff0a5086a4190df2199",
            "f0489527c2c14d6e8217352026187fda",
            "d155d6fcabd140ebb354f85e12875adf",
            "0efd886b30fd4fbfa0adb731c6ed42a2",
            "ec116cebf29a4c27b3c62fdfa190fb6a",
            "ac031a335e8c4e58872cc2603e25a4a4",
            "6d4542a03ac94ee5a6e609e84f892f19",
            "413a7a480c49409586edfa544e048456",
            "b162a5c866ad4e02980e488ce660d060",
            "faa5969da4bf44d38e8dedc9bbc27dc9",
            "529161819c3646a79fd943dce58e487a",
            "1ae7d5e1b1be469385e489c084209b82",
            "357557edf47345cbbdd26efc8324cff9"
          ]
        },
        "id": "iCJpCoKGtkBt",
        "outputId": "c5234f62-6fb7-42f8-c89b-e4473f1d21d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d4b787888c7407aac519631fb645648"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d155d6fcabd140ebb354f85e12875adf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.298694\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.559826\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.264309\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.192827\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.155204\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.131487\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.115361\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.103627\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.094640\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.087524\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.081748\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.076965\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.072942\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.069512\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.066556\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.063983\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.061727\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.059733\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.057961\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.056377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1441, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.113487\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.735505\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.499642\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.394906\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.349939\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.362025\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.382025\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.317024\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.240285\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.203505\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.189282\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.180263\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.172686\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.166068\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.160242\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.155080\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.150480\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.146361\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.142656\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.139309\n",
            "\n",
            "Test set: Average loss: 0.3849, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.280741\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.750228\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.499854\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.551918\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.459529\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.298368\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.227107\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.204546\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.190796\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.179526\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.169980\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.161800\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.154721\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.148543\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.143112\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.138306\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.134029\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.130203\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.126765\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.123663\n",
            "\n",
            "Test set: Average loss: 0.3573, Accuracy: 600/700 (85.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.185941\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.535931\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.407365\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.335619\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.200741\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.132654\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.118901\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.109005\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.101074\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.094577\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.089161\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.084581\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.080660\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.077269\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.074310\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.071708\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.069406\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.067356\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.065521\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.063871\n",
            "\n",
            "Test set: Average loss: 0.1578, Accuracy: 687/700 (98.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.396822\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.709804\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.433455\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.347919\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.303095\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.268848\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.237148\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.211472\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.194121\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.181438\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.171141\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.162443\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.154988\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.148534\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.142896\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.137933\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.133537\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.129620\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.126111\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.122955\n",
            "\n",
            "Test set: Average loss: 0.3801, Accuracy: 594/700 (84.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.888752\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.672284\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.475223\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.425144\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.387396\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.304623\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.261650\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.237713\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.222339\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.209810\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.199192\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.190079\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.182178\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.175270\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.169184\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.163788\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.158976\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.154664\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.150782\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.147274\n",
            "\n",
            "Test set: Average loss: 0.3813, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.629925\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.679413\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.470026\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.394869\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.233749\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.145774\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.129403\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.118168\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.109194\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.101865\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.095774\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.090635\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.086247\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.082460\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.079163\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.076270\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.073714\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.071442\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.069411\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.067588\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.827011\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.722739\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.579201\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.321588\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.219541\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.174081\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.155377\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.141134\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.129843\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.120681\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.113106\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.106746\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.101337\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.096686\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.092650\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.089117\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.086005\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.083245\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.080783\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.078578\n",
            "\n",
            "Test set: Average loss: 0.1833, Accuracy: 672/700 (96.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.760321\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.697739\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.367625\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.308207\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.237826\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.180798\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.148030\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.133334\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.122451\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.113693\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.106489\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.100465\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.095356\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.090973\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.087175\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.083856\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.080935\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.078347\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.076040\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.073974\n",
            "\n",
            "Test set: Average loss: 0.1792, Accuracy: 672/700 (96.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.418558\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.728605\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.567735\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.518187\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.395797\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.280908\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.243928\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.223420\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.207693\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.194708\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.183803\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.174524\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.166540\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.159606\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.153535\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.148181\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.143430\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.139192\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.135393\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.131972\n",
            "\n",
            "Test set: Average loss: 0.3823, Accuracy: 592/700 (84.57%)\n",
            "\n",
            "Acc over 10 instances: 91.09 +- 5.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Submissions"
      ],
      "metadata": {
        "id": "BzWaYd4hID6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "lH53nIO8R0K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "aw3lV_V2IHv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # MODEL WITH MODIFIED KERNEL SIZE + BATCHNORM\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=5, stride=2), \n",
        "                      nn.BatchNorm2d(16),\n",
        "                      nn.ReLU()]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=5), \n",
        "                      nn.BatchNorm2d(32),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
        "                        nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*4*4, 32)\n",
        "        \n",
        "        self.layerout=nn.Linear(32,10)\n",
        "\n",
        "\n",
        "\n",
        "        # add one hidden layer with non linearity in the Fully Connected layers\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*4*4)\n",
        "        rel=torch.nn.ReLU()\n",
        "        x = self.fc(x)\n",
        "        x=rel(x)\n",
        "        x=self.layerout(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Net2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "\n",
        "        # MODEL WITH BASELINE KERNEL SIZE + BATCHNORM\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) , \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2), \n",
        "                      nn.BatchNorm2d(16),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n",
        "                      nn.BatchNorm2d(32),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*5*5, 32)\n",
        "        \n",
        "        self.layerout=nn.Linear(32,10)\n",
        "\n",
        "\n",
        "        # add one hidden layer with non linearity in the Fully Connected layers\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*5*5)\n",
        "        rel=torch.nn.ReLU()\n",
        "        x = self.fc(x)\n",
        "        x=rel(x)\n",
        "        x=self.layerout(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "nrs36zvFxIWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for seed in range(50):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.008, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 50 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "\n",
        "\n",
        "print(\"Runtime --- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfLBYXroSeFW",
        "outputId": "b375334f-9c27-4999-fd7d-7fc6eb999b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.310408\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.110315\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.242310\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.574097\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.439711\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.278930\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.145299\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.062085\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.022601\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.008470\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003873\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002280\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001445\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001063\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000836\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000708\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000624\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000570\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000527\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000494\n",
            "\n",
            "Test set: Average loss: 0.5925, Accuracy: 610/700 (87.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.189639\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.065951\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.726987\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.865591\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.519577\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.400716\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.249211\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.138211\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.055778\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.020998\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008709\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004220\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002571\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001722\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001343\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001090\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000947\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000857\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000786\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000734\n",
            "\n",
            "Test set: Average loss: 1.1779, Accuracy: 449/700 (64.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.192198\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.022736\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.391690\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.679504\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.627262\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.588728\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.494234\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.389219\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.263931\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.133366\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.050487\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.017358\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006605\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003162\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001853\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001244\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000923\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000761\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000655\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000589\n",
            "\n",
            "Test set: Average loss: 1.1368, Accuracy: 490/700 (70.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.353165\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.235808\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.993322\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.302130\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.594809\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.434777\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.265389\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.116738\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.057883\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.025448\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.009033\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004032\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002294\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001581\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001059\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000798\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000636\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000545\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000492\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000454\n",
            "\n",
            "Test set: Average loss: 0.5866, Accuracy: 613/700 (87.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.402880\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.273570\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.885192\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.803914\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.627945\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.565235\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.483521\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.968910\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.255928\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.100427\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.041241\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.013750\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.005098\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002589\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001415\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001138\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000896\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000710\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000614\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000561\n",
            "\n",
            "Test set: Average loss: 0.9443, Accuracy: 495/700 (70.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.168667\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.964563\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.188541\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.672495\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.592974\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.435610\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.317960\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.208732\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.099661\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.040304\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016245\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006864\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003534\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002175\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001508\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001156\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000970\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000835\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000752\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000692\n",
            "\n",
            "Test set: Average loss: 1.1957, Accuracy: 489/700 (69.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.286011\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.145385\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.541657\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.646398\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.578464\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.472518\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.290644\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.129715\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.050985\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.017614\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.006586\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002971\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001601\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001098\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000826\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000669\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000564\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000501\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000455\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000423\n",
            "\n",
            "Test set: Average loss: 0.6138, Accuracy: 589/700 (84.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.262052\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.135157\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.898354\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.306218\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.750987\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.376603\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.183016\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.111461\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.042163\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.019238\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008654\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004145\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002820\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001924\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001387\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001090\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000913\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000793\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000706\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000640\n",
            "\n",
            "Test set: Average loss: 0.4579, Accuracy: 613/700 (87.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.415911\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.295513\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.910289\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.829515\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.574078\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.453407\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.280315\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.139342\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.053561\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.021839\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007816\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002914\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001573\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001040\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000761\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000600\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000506\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000442\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000400\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000370\n",
            "\n",
            "Test set: Average loss: 0.8140, Accuracy: 588/700 (84.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.370086\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.223214\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.690219\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.748733\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.686232\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.621912\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.509114\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.363568\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.238053\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.143688\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.062680\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.026965\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.012408\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.006350\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.003651\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.002443\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001872\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001495\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001280\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001140\n",
            "\n",
            "Test set: Average loss: 0.9001, Accuracy: 522/700 (74.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.166488\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.916294\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.096235\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.780488\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.665724\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.640515\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.532669\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.380434\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.250773\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.127669\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.053564\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.019281\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.008424\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.004345\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002531\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001765\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001331\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001091\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000949\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000851\n",
            "\n",
            "Test set: Average loss: 1.2603, Accuracy: 492/700 (70.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.279441\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.093011\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.518797\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.580001\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.373307\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.236011\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.136676\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.060343\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.019951\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.007760\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003969\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002157\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001358\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000961\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000737\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000600\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000512\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000451\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000411\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000380\n",
            "\n",
            "Test set: Average loss: 0.4684, Accuracy: 627/700 (89.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.250406\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.093802\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.662124\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.739798\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.600811\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.520120\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.389857\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.231375\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.111807\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.046669\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.017483\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007357\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003744\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002254\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001563\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001209\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001001\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000871\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000781\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000716\n",
            "\n",
            "Test set: Average loss: 0.6392, Accuracy: 583/700 (83.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.365767\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.113659\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.414896\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.596850\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.493388\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.355376\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.250433\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.133415\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.048096\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.021496\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007490\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003630\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002060\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001311\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001003\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000758\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000650\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000573\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000523\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000487\n",
            "\n",
            "Test set: Average loss: 0.7445, Accuracy: 577/700 (82.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.369595\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.142225\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.379234\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.651570\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.615866\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.574148\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.422765\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.313315\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.215077\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.127961\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.057530\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.023977\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.009604\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.004506\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002536\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001618\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001164\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000930\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000780\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000679\n",
            "\n",
            "Test set: Average loss: 0.9773, Accuracy: 554/700 (79.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.253223\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.080363\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.398310\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.571290\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.404821\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.254862\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.167467\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.111026\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.053833\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.025663\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.011516\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.005894\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003187\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002032\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001436\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001088\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000879\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000756\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000668\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000603\n",
            "\n",
            "Test set: Average loss: 0.6634, Accuracy: 603/700 (86.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.509124\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.228614\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.433214\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.639633\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.556611\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.452426\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.326820\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.188657\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.105470\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.044111\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016807\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007686\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003901\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002278\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001454\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001101\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000883\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000762\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000675\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000615\n",
            "\n",
            "Test set: Average loss: 0.7390, Accuracy: 577/700 (82.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.237208\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.000462\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.178603\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.587049\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.458479\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.279019\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.123468\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.055316\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.021184\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.008181\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003730\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002100\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001409\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001058\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000846\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000726\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000637\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000581\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000539\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000506\n",
            "\n",
            "Test set: Average loss: 0.9463, Accuracy: 561/700 (80.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.420916\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.203551\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.334984\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.644329\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.604804\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.519748\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.618201\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.488752\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.265601\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.089191\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.044639\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.016924\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.008762\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.004480\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002591\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001927\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001417\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001177\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001011\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000898\n",
            "\n",
            "Test set: Average loss: 1.1303, Accuracy: 472/700 (67.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.250661\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.088936\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.467829\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.668855\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.601244\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.499088\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.360360\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.246226\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.140500\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.063802\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.025707\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.010221\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.004890\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002850\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001853\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001371\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001102\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000936\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000826\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000749\n",
            "\n",
            "Test set: Average loss: 1.2173, Accuracy: 490/700 (70.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.392593\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.244250\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.004098\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.433192\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.836913\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.444509\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.285774\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.153092\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.073877\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.039815\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.015913\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006447\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003437\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002072\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001563\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001106\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000903\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000739\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000665\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000596\n",
            "\n",
            "Test set: Average loss: 0.5986, Accuracy: 619/700 (88.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.366497\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.162829\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.411840\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.640422\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.575762\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.494591\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.401136\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.229810\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.102608\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.045209\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.014527\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007232\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003379\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001723\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001291\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000933\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000769\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000641\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000584\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000519\n",
            "\n",
            "Test set: Average loss: 1.0101, Accuracy: 517/700 (73.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.418749\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.265313\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.026027\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.340276\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.590216\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.485620\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.362743\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.218259\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.098621\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.035400\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.012099\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004976\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002532\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001565\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001116\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000874\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000737\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000651\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000592\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000550\n",
            "\n",
            "Test set: Average loss: 0.9100, Accuracy: 558/700 (79.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.239983\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.897983\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.779249\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.635895\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.559114\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.488523\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.250918\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.123957\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.043446\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.013084\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005012\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002057\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001351\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000866\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000603\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000501\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000445\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000400\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000365\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000340\n",
            "\n",
            "Test set: Average loss: 1.6364, Accuracy: 469/700 (67.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.314604\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.063412\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.095971\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.637226\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.524026\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.370685\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.242458\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.126548\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.056373\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.019427\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007980\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003976\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002421\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001561\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001216\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000988\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000849\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000770\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000708\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000662\n",
            "\n",
            "Test set: Average loss: 1.0092, Accuracy: 509/700 (72.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.224344\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.044921\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.333630\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.646636\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.585851\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.499225\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.373464\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.230070\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.111943\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.043951\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016012\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006474\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003175\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001914\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001311\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001006\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000833\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000724\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000652\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000603\n",
            "\n",
            "Test set: Average loss: 0.6663, Accuracy: 575/700 (82.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.278395\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.997588\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.972966\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.620884\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.535937\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.438536\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.304316\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.157638\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.060123\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.020665\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007255\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003269\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001891\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001281\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000958\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000778\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000673\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000604\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000555\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000520\n",
            "\n",
            "Test set: Average loss: 1.5985, Accuracy: 482/700 (68.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.292590\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.112415\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.590668\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.721736\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.637895\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.577127\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.522647\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.401017\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.251159\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.137076\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.058456\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.021700\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.008660\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.004151\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002435\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001646\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001265\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001029\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000892\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000797\n",
            "\n",
            "Test set: Average loss: 0.9965, Accuracy: 500/700 (71.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.227902\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.973184\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.120338\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.640094\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.587268\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.491662\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.377633\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.266206\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.160528\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.077338\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.031414\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.013518\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006210\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003350\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002141\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001534\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001204\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001009\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000883\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000795\n",
            "\n",
            "Test set: Average loss: 1.8050, Accuracy: 464/700 (66.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.260074\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.138549\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.839178\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.949525\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.499659\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.366965\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.207026\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.109392\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.045447\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.015936\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.007208\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003483\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002019\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001254\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000979\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000788\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000681\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000604\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000554\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000513\n",
            "\n",
            "Test set: Average loss: 0.9186, Accuracy: 565/700 (80.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.359101\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.165535\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.404311\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.610561\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.475452\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.291186\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.131925\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.044533\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.012641\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.004205\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.001907\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001078\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.000739\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000531\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000448\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000381\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000340\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000313\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000295\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000280\n",
            "\n",
            "Test set: Average loss: 0.5983, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.247107\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.114027\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.767506\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.814971\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.406277\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.260996\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.145852\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.067318\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.030264\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.010794\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.004484\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002330\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001393\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000938\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000717\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000591\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000510\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000455\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000415\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000387\n",
            "\n",
            "Test set: Average loss: 0.5803, Accuracy: 600/700 (85.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.360904\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.207294\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.644191\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.687307\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.611088\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.520291\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.393715\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.232924\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.107088\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.039799\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.013201\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.005078\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002396\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001445\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001003\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000777\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000647\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000569\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000514\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000475\n",
            "\n",
            "Test set: Average loss: 1.1289, Accuracy: 527/700 (75.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.283314\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.114765\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.671472\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.724899\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.597149\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.504389\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.374623\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.236673\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.114217\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.044267\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.015788\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006600\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003447\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002114\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001504\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001180\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000992\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000875\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000793\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000736\n",
            "\n",
            "Test set: Average loss: 0.7644, Accuracy: 520/700 (74.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.355435\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.147879\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.607696\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.697353\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.466865\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.324373\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.209467\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.113645\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.040994\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.015494\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005907\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.003298\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001801\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001152\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000871\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000689\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000605\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000537\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000490\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000455\n",
            "\n",
            "Test set: Average loss: 0.4530, Accuracy: 627/700 (89.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.184087\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.990362\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.318386\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.645053\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.560526\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.466279\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.342838\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.213245\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.103421\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.043302\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016635\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007295\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003882\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002484\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001736\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001355\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001122\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000974\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000878\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000807\n",
            "\n",
            "Test set: Average loss: 1.3478, Accuracy: 470/700 (67.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.305201\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.094352\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.190822\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.526390\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.239379\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.165680\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.088254\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.038632\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.014175\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.006053\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003254\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001807\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001092\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000821\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000653\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000532\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000460\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000421\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000390\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000363\n",
            "\n",
            "Test set: Average loss: 0.9045, Accuracy: 603/700 (86.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.437016\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.303498\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.908278\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.867493\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.610577\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.611723\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.453521\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.355806\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.198513\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.100464\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.037271\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.014604\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.006735\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.003778\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002443\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001705\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001317\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001100\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000957\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000863\n",
            "\n",
            "Test set: Average loss: 1.1510, Accuracy: 486/700 (69.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.475843\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.191895\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.484632\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.583572\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.445464\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.254642\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.207579\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.139088\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.090992\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.056229\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.033301\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.020817\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.012794\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.008074\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.005307\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.003601\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.002694\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.002136\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001756\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.001506\n",
            "\n",
            "Test set: Average loss: 0.8304, Accuracy: 572/700 (81.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.446368\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.159209\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.146218\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.691498\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.611084\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.488943\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.376299\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.288436\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.178383\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.086454\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.037231\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.015665\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.007716\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.004295\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.002716\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001935\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.001516\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.001276\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.001110\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000998\n",
            "\n",
            "Test set: Average loss: 1.3109, Accuracy: 511/700 (73.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.366389\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.237159\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.818810\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.714365\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.388774\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.237338\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.179411\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.102202\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.046714\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.019147\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.008634\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.004131\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002408\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001534\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001095\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000860\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000716\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000626\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000567\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000522\n",
            "\n",
            "Test set: Average loss: 0.8941, Accuracy: 575/700 (82.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.287052\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.143304\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.804863\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.798127\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.353777\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.160667\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.089526\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.043522\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.018146\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.006777\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.003071\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.001845\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001250\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.000853\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000677\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000567\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000486\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000436\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000399\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000370\n",
            "\n",
            "Test set: Average loss: 0.7051, Accuracy: 616/700 (88.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.263555\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.125573\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.660231\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.796795\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.576575\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.453490\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.322361\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.193349\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.109807\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.051518\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.022339\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.009842\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.004718\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002560\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001560\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001143\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000900\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000751\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000653\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000581\n",
            "\n",
            "Test set: Average loss: 0.9115, Accuracy: 564/700 (80.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.284770\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.118052\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.424479\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.629133\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.534037\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.437586\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.313848\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.190759\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.095570\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.037572\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.014854\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006593\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003519\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002104\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001487\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001154\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000960\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000841\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000756\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000693\n",
            "\n",
            "Test set: Average loss: 1.2116, Accuracy: 492/700 (70.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.380777\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.096836\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.083786\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.670322\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.572370\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.430431\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.326222\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.206770\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.106915\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.039217\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.015584\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.006245\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003117\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001792\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001176\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000904\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000742\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000633\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000563\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000514\n",
            "\n",
            "Test set: Average loss: 0.7914, Accuracy: 560/700 (80.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.240762\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.053915\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.447930\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.598828\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.399337\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.272279\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.202868\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.125742\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.070820\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.036538\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016409\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007433\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003613\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.002105\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001463\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.001110\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000893\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000769\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000681\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000616\n",
            "\n",
            "Test set: Average loss: 0.8259, Accuracy: 567/700 (81.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.276064\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.161904\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.861726\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.994785\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.597585\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.426589\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.297445\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.171614\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.099315\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.044044\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.016757\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.007078\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.003306\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001916\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001261\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000928\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000728\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000612\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000537\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000486\n",
            "\n",
            "Test set: Average loss: 0.6158, Accuracy: 603/700 (86.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.277601\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.116583\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.569722\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.625739\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.400445\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.243743\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.138885\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.062486\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.020652\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.009090\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.004159\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002187\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001434\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001072\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000850\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000711\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000625\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000567\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000525\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000492\n",
            "\n",
            "Test set: Average loss: 0.8520, Accuracy: 581/700 (83.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.485771\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.335413\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.033352\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 1.073425\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.496768\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.340510\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.231841\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.134812\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.068720\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.030865\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.012126\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.005358\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.002780\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001728\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.001184\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000888\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000739\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000638\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000568\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000520\n",
            "\n",
            "Test set: Average loss: 0.5676, Accuracy: 623/700 (89.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.413201\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.171166\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.357201\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.590354\n",
            "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.477134\n",
            "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.333308\n",
            "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.199799\n",
            "Train Epoch: 35 [0/100 (0%)]\tLoss: 0.093842\n",
            "Train Epoch: 40 [0/100 (0%)]\tLoss: 0.037005\n",
            "Train Epoch: 45 [0/100 (0%)]\tLoss: 0.012828\n",
            "Train Epoch: 50 [0/100 (0%)]\tLoss: 0.005300\n",
            "Train Epoch: 55 [0/100 (0%)]\tLoss: 0.002702\n",
            "Train Epoch: 60 [0/100 (0%)]\tLoss: 0.001701\n",
            "Train Epoch: 65 [0/100 (0%)]\tLoss: 0.001220\n",
            "Train Epoch: 70 [0/100 (0%)]\tLoss: 0.000967\n",
            "Train Epoch: 75 [0/100 (0%)]\tLoss: 0.000805\n",
            "Train Epoch: 80 [0/100 (0%)]\tLoss: 0.000701\n",
            "Train Epoch: 85 [0/100 (0%)]\tLoss: 0.000636\n",
            "Train Epoch: 90 [0/100 (0%)]\tLoss: 0.000585\n",
            "Train Epoch: 95 [0/100 (0%)]\tLoss: 0.000548\n",
            "\n",
            "Test set: Average loss: 0.6159, Accuracy: 570/700 (81.43%)\n",
            "\n",
            "Acc over 50 instances: 78.65 +- 7.47\n",
            "Runtime --- 493.29764008522034 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "HeILCPo-IJ9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "# Extract a subset of 100 (class balanced) total samples, 50 samples per class\n",
        "\n",
        "accs = []\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for seed in range(50):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:50]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[50:400]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "  model = models.alexnet(pretrained=True)\n",
        "  model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adagrad(model.classifier.parameters(), \n",
        "                              lr=0.001)\n",
        "  \n",
        "  #LAMBDALR \n",
        "  lambda1 = lambda epoch: epoch // 2\n",
        "  scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        " \n",
        "  for epoch in range(20):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    scheduler.step()\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 10 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "\n",
        "print(\"Runtime --- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "swjGpUWqIK1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0783745ccc814f9a820e3c04842aaf8a",
            "107b97c12d7648babeedb610c46550ae",
            "7261830546ee4f2790012390fde004fc",
            "93922710882d4eb8809e6bb8d2668c1f",
            "ca2f95872ac248de8ecbf0492c4f02f1",
            "e9313f8814484695a4ae4ecf38040e55",
            "97244596aee8431bb1dd733605102135",
            "f5f657914e6744889f0e45bade1ac76d",
            "b193773340d54f3bb541fd4b5777c626",
            "f732619a846b4eacaf86785c6de08a94",
            "446b1f7d0d1e4c49acdb263fac90ffdd"
          ]
        },
        "outputId": "730148b5-18b1-400a-d550-2664c560cfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 100 Num Samples For Val 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0783745ccc814f9a820e3c04842aaf8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 1.946438\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 1.946439\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 1.946438\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.564543\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.355683\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.378494\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.463494\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.756692\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.464682\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.112091\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.019915\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.017432\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.015817\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.014352\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.013202\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.012111\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.011218\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.010368\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.009657\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.008980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0642, Accuracy: 691/700 (98.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.826610\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.826610\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.826610\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.862327\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.694386\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.768520\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.655511\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.787428\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.009272\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.143741\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.088159\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 1.568203\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.672141\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.674407\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.169708\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.035521\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.025325\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.022651\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.021036\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.019591\n",
            "\n",
            "Test set: Average loss: 0.3141, Accuracy: 613/700 (87.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.829466\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.829466\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.829466\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.884690\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.608148\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.375126\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.340670\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.775570\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.624837\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.008904\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.455321\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.173940\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.034775\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.029993\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.026607\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.023673\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.021468\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.019507\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.017963\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.016558\n",
            "\n",
            "Test set: Average loss: 0.2851, Accuracy: 611/700 (87.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.487953\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.487953\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.487953\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.737250\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.466947\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.345417\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.420843\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.848094\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.992953\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.524517\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.064671\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.023331\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.017526\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.014614\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.013101\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.011934\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.011069\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.010269\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.009604\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.008969\n",
            "\n",
            "Test set: Average loss: 0.0800, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.752556\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.752556\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.752556\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.780909\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.558291\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.361973\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.278302\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.206355\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.166563\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.144450\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.180686\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.677305\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 3.546861\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 2.364749\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 1.269687\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.369223\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.030220\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.023259\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.019152\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.016112\n",
            "\n",
            "Test set: Average loss: 0.3992, Accuracy: 609/700 (87.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.670615\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.670616\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.670616\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.807931\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.605969\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.455639\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.542214\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.304505\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.512150\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.898881\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.503782\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.234187\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.065212\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.052371\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.046605\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.041551\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.037652\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.034115\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.031284\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.028677\n",
            "\n",
            "Test set: Average loss: 0.3324, Accuracy: 598/700 (85.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.375647\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.375647\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.375648\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.770371\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.432125\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.229427\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.168683\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.152338\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.141936\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.217976\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.199784\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.296691\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.087848\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.007319\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.006587\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.005966\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.005504\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.005095\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.004770\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.004472\n",
            "\n",
            "Test set: Average loss: 0.0544, Accuracy: 694/700 (99.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.536169\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.536169\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.536169\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.747395\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.460802\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.371687\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.418904\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.424842\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.347958\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.083638\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.025566\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.022801\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.020675\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.018652\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.017044\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.015518\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.014274\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.013093\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.012110\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.011176\n",
            "\n",
            "Test set: Average loss: 0.0869, Accuracy: 686/700 (98.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.358886\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.358887\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.358886\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.699304\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.497107\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.723794\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.756194\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.783968\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.336898\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.053027\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.029680\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.025527\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.023029\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.020752\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.018979\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.017315\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.015968\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014694\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013636\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012630\n",
            "\n",
            "Test set: Average loss: 0.0892, Accuracy: 682/700 (97.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.427843\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.427842\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.427843\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.900612\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.667419\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.701127\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.966290\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.303045\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.812923\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.464872\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.202681\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.063784\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.044397\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.039200\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.035458\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.032097\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.029461\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.027016\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.025008\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.023115\n",
            "\n",
            "Test set: Average loss: 0.3145, Accuracy: 608/700 (86.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.152171\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.152172\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.152172\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.761028\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.525917\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.344162\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.298362\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.527650\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.242577\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.781370\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.751849\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.296677\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.036875\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.031886\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.028287\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.025139\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.022758\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.020631\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.018953\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.017425\n",
            "\n",
            "Test set: Average loss: 0.2948, Accuracy: 622/700 (88.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.232217\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.232217\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.232217\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.663656\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.387391\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.247278\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.219905\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.217127\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.193165\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.073801\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.027419\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.022286\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.019799\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.017586\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.015863\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.014250\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.012954\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.011740\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.010745\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.009812\n",
            "\n",
            "Test set: Average loss: 0.0577, Accuracy: 693/700 (99.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.420649\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.420650\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.420650\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.801950\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.478670\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.368160\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.558454\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.392449\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.165046\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.408442\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.065709\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.020245\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.016113\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.013862\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.012604\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.011593\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.010826\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.010109\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.009509\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.008930\n",
            "\n",
            "Test set: Average loss: 0.0771, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.491287\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.491287\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.491287\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.751939\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.572371\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.608090\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.869015\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.521957\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.801344\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.690095\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.188051\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.087899\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.048913\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.041934\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.037378\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.033432\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.030407\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.027651\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.025425\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.023356\n",
            "\n",
            "Test set: Average loss: 0.1596, Accuracy: 660/700 (94.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.225724\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.225723\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.225723\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.805219\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.502213\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.284298\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.218375\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.231347\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.291464\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.639113\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.651257\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.600027\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.042825\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.016372\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.012139\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.010093\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.008994\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.008159\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.007551\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.007013\n",
            "\n",
            "Test set: Average loss: 0.1329, Accuracy: 669/700 (95.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.361003\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.361003\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.361003\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.795257\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.506133\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.323770\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.322823\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.644196\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.887882\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.848185\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.224835\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.030953\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.021357\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.017433\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.015528\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.014086\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.013031\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.012069\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.011279\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.010530\n",
            "\n",
            "Test set: Average loss: 0.1342, Accuracy: 671/700 (95.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 3.216595\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 3.216595\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 3.216595\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.962253\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.610737\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.420335\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.467137\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.947766\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.906162\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.608879\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.094387\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.027876\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.024193\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.021450\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.019474\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.017675\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.016248\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014920\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013830\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012805\n",
            "\n",
            "Test set: Average loss: 0.0811, Accuracy: 684/700 (97.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.369757\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.369757\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.369757\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.687633\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.432135\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.405676\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.511276\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.837988\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.680218\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.204406\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.027690\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.023652\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.021181\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.018965\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.017259\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.015675\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.014407\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.013223\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.012249\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.011333\n",
            "\n",
            "Test set: Average loss: 0.1062, Accuracy: 678/700 (96.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.520338\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.520338\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.520338\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.844182\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.652523\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.447496\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.365894\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.502942\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.372959\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.351096\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.092032\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.722328\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.251752\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.080906\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.046648\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.036985\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.031648\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.027756\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.025089\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.022858\n",
            "\n",
            "Test set: Average loss: 0.4049, Accuracy: 578/700 (82.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.271811\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.271811\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.271811\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.746609\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.587672\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.407966\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.326728\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.384914\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.187636\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.767889\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.582583\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.792008\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.508185\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.131234\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.041635\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.032039\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.028523\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.025696\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.023608\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.021775\n",
            "\n",
            "Test set: Average loss: 0.3859, Accuracy: 580/700 (82.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.749668\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.749668\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.749668\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.868456\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.542696\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.286256\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.200942\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.139492\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.107033\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.082193\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.064476\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.050882\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.041658\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.034659\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.029787\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.025615\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.022488\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.019727\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.017580\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.015658\n",
            "\n",
            "Test set: Average loss: 0.1192, Accuracy: 669/700 (95.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.958440\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.958441\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.958441\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.928377\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.674576\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.789985\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.992320\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.999989\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.766409\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.338098\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.102164\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.047741\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.040780\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.036276\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.032833\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.029657\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.027127\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.024767\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.022831\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.021011\n",
            "\n",
            "Test set: Average loss: 0.1950, Accuracy: 646/700 (92.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.492645\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.492645\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.492645\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.789099\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.542526\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.421669\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.597297\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.531466\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.033246\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.721164\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.092386\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.023169\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.020428\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.018443\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.017047\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.015761\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.014713\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.013708\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.012860\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012043\n",
            "\n",
            "Test set: Average loss: 0.1512, Accuracy: 663/700 (94.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.467507\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.467507\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.467507\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.864461\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.657770\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.464898\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.480006\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.200841\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.392050\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.439114\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.546422\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.299737\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.065678\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.044860\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.040626\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.036787\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.033756\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.030948\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.028656\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.026509\n",
            "\n",
            "Test set: Average loss: 0.2350, Accuracy: 638/700 (91.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.295574\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.295574\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.295574\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.832943\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.651569\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.581115\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.961920\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.900179\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.325999\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.807498\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.649324\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.261585\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.119481\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.051427\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.043419\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.038919\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.035487\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.032373\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.029872\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.027554\n",
            "\n",
            "Test set: Average loss: 0.3450, Accuracy: 599/700 (85.57%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.243800\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.243800\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.243800\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.921358\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.594903\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.642157\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.849919\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.769373\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.235130\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.124372\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.046127\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.040302\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.036114\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.032238\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.029221\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.026397\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.024121\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.021980\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.020214\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.018550\n",
            "\n",
            "Test set: Average loss: 0.1625, Accuracy: 658/700 (94.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.384280\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.384280\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.384280\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.858102\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.656623\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.921231\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.342806\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.004280\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.637900\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.720521\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.336028\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.225565\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.098197\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.058256\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.047235\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.041518\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.037535\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.034056\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.031316\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.028804\n",
            "\n",
            "Test set: Average loss: 0.2442, Accuracy: 636/700 (90.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.544538\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.544538\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.544538\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.800304\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.628551\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.617981\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.958706\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.679975\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.150191\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.736052\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.448629\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.189128\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.066686\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.050197\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.043203\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.037905\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.034186\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.030985\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.028497\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.026234\n",
            "\n",
            "Test set: Average loss: 0.2692, Accuracy: 629/700 (89.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.481200\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.481200\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.481200\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.842750\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.657632\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.711217\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.017609\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.645384\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.925642\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.850744\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.649409\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.356652\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.134661\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.052955\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.040397\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.036638\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.033739\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.031033\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.028809\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.026715\n",
            "\n",
            "Test set: Average loss: 0.3303, Accuracy: 607/700 (86.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.534570\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.534570\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.534569\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.886180\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.580495\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.326505\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.249808\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.321543\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.615583\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.317301\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.189983\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.040654\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.023297\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.019929\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.018145\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.016592\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.015355\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014187\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013218\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012297\n",
            "\n",
            "Test set: Average loss: 0.1618, Accuracy: 661/700 (94.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.578916\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.578916\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.578916\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.776306\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.475140\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.265182\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.206217\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.241151\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.337797\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.955410\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.987434\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.884457\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.030468\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.012325\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.009222\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.007612\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.006736\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.006087\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.005630\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.005236\n",
            "\n",
            "Test set: Average loss: 0.0821, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.622137\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.622137\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.622137\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.813702\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.468091\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.236828\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.163310\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.111454\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.085090\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.065075\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.051268\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.040744\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.033699\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.028186\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.024299\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.020939\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.018412\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.016176\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.014433\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012871\n",
            "\n",
            "Test set: Average loss: 0.1061, Accuracy: 680/700 (97.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.411944\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.411944\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.411943\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.881294\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.620287\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.451315\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.750233\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.139978\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.217778\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.940498\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.541346\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.116041\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.020841\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.019190\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.017867\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.016590\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.015529\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014504\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013637\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012801\n",
            "\n",
            "Test set: Average loss: 0.1603, Accuracy: 664/700 (94.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.070198\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.070198\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.070198\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.776793\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.599934\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.020670\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.211737\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.920891\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.484199\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.374429\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.199821\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.139180\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.062043\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.045382\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.038729\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.034947\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.031982\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.029218\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.026946\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.024807\n",
            "\n",
            "Test set: Average loss: 0.2718, Accuracy: 617/700 (88.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.024231\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.024231\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.024230\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.668658\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.457744\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.401009\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.429666\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.590194\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.456317\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.170632\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.040893\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.030438\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.026832\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.023946\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.021765\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.019736\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.018103\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.016568\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.015299\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.014100\n",
            "\n",
            "Test set: Average loss: 0.0878, Accuracy: 685/700 (97.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.360535\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.360534\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.360534\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.857416\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.652067\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.524223\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.729530\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.986209\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.589748\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.024603\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.540438\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.540879\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.161490\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.046675\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.036128\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.031919\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.029012\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.026471\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.024479\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.022657\n",
            "\n",
            "Test set: Average loss: 0.2591, Accuracy: 618/700 (88.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.246589\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.246589\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.246589\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.739959\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.454648\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.255372\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.186626\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.140827\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.112469\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.090996\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.068315\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.049778\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.038748\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.032247\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.027974\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.024284\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.021468\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.018946\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.016962\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.015169\n",
            "\n",
            "Test set: Average loss: 0.1148, Accuracy: 673/700 (96.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.270492\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.270492\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.270492\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.815310\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.578550\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.613489\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.154187\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.328753\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.511873\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.509488\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.271431\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.171319\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.054500\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.037553\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.033225\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.029905\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.027352\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.025022\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.023138\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.021380\n",
            "\n",
            "Test set: Average loss: 0.2513, Accuracy: 635/700 (90.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.132848\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.132848\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.132848\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.671798\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.425897\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.305231\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.344399\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.663493\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.800642\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.493413\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.054989\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.021127\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.017427\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.015117\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.013615\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.012300\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.011277\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.010336\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.009572\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.008860\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 661/700 (94.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.425690\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.425690\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.425690\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.846210\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.652194\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.444595\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.386021\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.832965\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 2.133432\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.981834\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.043055\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.955925\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.309501\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.128584\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.048361\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.039657\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.034326\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.030140\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.027145\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.024599\n",
            "\n",
            "Test set: Average loss: 0.3063, Accuracy: 609/700 (87.00%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.424807\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.424806\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.424807\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.837813\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.538412\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.300251\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.217681\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.156034\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.124122\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.102479\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.085069\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.074018\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.057240\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.045293\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.035136\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.029617\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.026023\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.023025\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.020668\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.018529\n",
            "\n",
            "Test set: Average loss: 0.1462, Accuracy: 666/700 (95.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.786573\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.786573\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.786574\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.925756\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.576984\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.601385\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.504600\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.327757\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.114299\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.040664\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.035212\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.030612\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.027191\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.024014\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.021548\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.019258\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.017432\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.015734\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.014349\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.013060\n",
            "\n",
            "Test set: Average loss: 0.0786, Accuracy: 681/700 (97.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.493236\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.493236\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.493236\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.917192\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.590367\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.381042\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.388708\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.748645\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.809559\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.678724\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.197663\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.025688\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.022546\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.020040\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.018217\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.016571\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.015276\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.014076\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.013093\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.012168\n",
            "\n",
            "Test set: Average loss: 0.1509, Accuracy: 667/700 (95.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.085816\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.085816\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.085816\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.765221\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.520927\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.320646\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.270916\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.507591\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.326895\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.979478\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.104844\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.044252\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.037985\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.033499\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.030186\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.027201\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.024870\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.022728\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.020992\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.019374\n",
            "\n",
            "Test set: Average loss: 0.2681, Accuracy: 629/700 (89.86%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.453292\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.453292\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.453292\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.855724\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.666669\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 1.047595\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 1.201022\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.253233\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.697169\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.437899\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.133008\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.050295\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.040621\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.036192\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.032843\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.029751\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.027287\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.024987\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.023097\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.021319\n",
            "\n",
            "Test set: Average loss: 0.1708, Accuracy: 660/700 (94.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.585149\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.585149\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.585149\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.782147\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.530167\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.381356\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.389255\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.682955\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.733466\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.628503\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.151790\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.034128\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.027769\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.024627\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.022446\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.020436\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.018822\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.017305\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.016052\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.014868\n",
            "\n",
            "Test set: Average loss: 0.1945, Accuracy: 647/700 (92.43%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.581367\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.581367\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.581367\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.943553\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.619374\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.370063\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.349736\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.687538\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.022892\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.710318\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.287659\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.038652\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.022778\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.018739\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.016905\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.015447\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.014326\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.013278\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.012410\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.011585\n",
            "\n",
            "Test set: Average loss: 0.1200, Accuracy: 674/700 (96.29%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.625194\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.625194\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.625194\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.872134\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.580567\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.582347\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.697891\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.933663\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.508225\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.197436\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.044269\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.037334\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.033347\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.029721\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.026906\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.024282\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.022181\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.020217\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.018607\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.017096\n",
            "\n",
            "Test set: Average loss: 0.1484, Accuracy: 663/700 (94.71%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.887968\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.887968\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.887968\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.932812\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.619571\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.579030\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.622315\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.714401\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.451282\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.153178\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.043260\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.036867\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.032692\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.028923\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.026058\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.023427\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.021342\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.019406\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.017824\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.016344\n",
            "\n",
            "Test set: Average loss: 0.0788, Accuracy: 687/700 (98.14%)\n",
            "\n",
            "Num Samples For Training 100 Num Samples For Val 700\n",
            "Train Epoch: 0 [0/100 (0%)]\tLoss: 2.389125\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.389125\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.389124\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.734966\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.520882\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.481581\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.590774\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.309198\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.512109\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.311927\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.051000\n",
            "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.032683\n",
            "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.028958\n",
            "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.025796\n",
            "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.023395\n",
            "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.021182\n",
            "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.019420\n",
            "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.017778\n",
            "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.016432\n",
            "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.015167\n",
            "\n",
            "Test set: Average loss: 0.1563, Accuracy: 665/700 (95.00%)\n",
            "\n",
            "Acc over 10 instances: 93.05 +- 4.58\n",
            "Runtime --- 347.8862111568451 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_OkF1zFdegr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d4b787888c7407aac519631fb645648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4adafedbd94454689456279df1a32dd",
              "IPY_MODEL_f594cdce94c446f584536ee931333e0a",
              "IPY_MODEL_1d79ea869737458d8599ae0272047240"
            ],
            "layout": "IPY_MODEL_3668b1769e914ee3863a6efd1462b6f3"
          }
        },
        "c4adafedbd94454689456279df1a32dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d949ca5acbef4d5c983efe894f6fa25d",
            "placeholder": "",
            "style": "IPY_MODEL_3a3fad75973740e597a0937084c7d536",
            "value": "100%"
          }
        },
        "f594cdce94c446f584536ee931333e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09de081fa874485fa5c8cd63764e8af3",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be6fc61a60664968b08488b37fb7c1cc",
            "value": 170498071
          }
        },
        "1d79ea869737458d8599ae0272047240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0942689a8d44ff0a5086a4190df2199",
            "placeholder": "",
            "style": "IPY_MODEL_f0489527c2c14d6e8217352026187fda",
            "value": " 170498071/170498071 [00:13&lt;00:00, 14405906.65it/s]"
          }
        },
        "3668b1769e914ee3863a6efd1462b6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d949ca5acbef4d5c983efe894f6fa25d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3fad75973740e597a0937084c7d536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09de081fa874485fa5c8cd63764e8af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be6fc61a60664968b08488b37fb7c1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0942689a8d44ff0a5086a4190df2199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0489527c2c14d6e8217352026187fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d155d6fcabd140ebb354f85e12875adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0efd886b30fd4fbfa0adb731c6ed42a2",
              "IPY_MODEL_ec116cebf29a4c27b3c62fdfa190fb6a",
              "IPY_MODEL_ac031a335e8c4e58872cc2603e25a4a4"
            ],
            "layout": "IPY_MODEL_6d4542a03ac94ee5a6e609e84f892f19"
          }
        },
        "0efd886b30fd4fbfa0adb731c6ed42a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_413a7a480c49409586edfa544e048456",
            "placeholder": "",
            "style": "IPY_MODEL_b162a5c866ad4e02980e488ce660d060",
            "value": "100%"
          }
        },
        "ec116cebf29a4c27b3c62fdfa190fb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa5969da4bf44d38e8dedc9bbc27dc9",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_529161819c3646a79fd943dce58e487a",
            "value": 244408911
          }
        },
        "ac031a335e8c4e58872cc2603e25a4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ae7d5e1b1be469385e489c084209b82",
            "placeholder": "",
            "style": "IPY_MODEL_357557edf47345cbbdd26efc8324cff9",
            "value": " 233M/233M [00:02&lt;00:00, 92.1MB/s]"
          }
        },
        "6d4542a03ac94ee5a6e609e84f892f19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413a7a480c49409586edfa544e048456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b162a5c866ad4e02980e488ce660d060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa5969da4bf44d38e8dedc9bbc27dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529161819c3646a79fd943dce58e487a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ae7d5e1b1be469385e489c084209b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357557edf47345cbbdd26efc8324cff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0783745ccc814f9a820e3c04842aaf8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_107b97c12d7648babeedb610c46550ae",
              "IPY_MODEL_7261830546ee4f2790012390fde004fc",
              "IPY_MODEL_93922710882d4eb8809e6bb8d2668c1f"
            ],
            "layout": "IPY_MODEL_ca2f95872ac248de8ecbf0492c4f02f1"
          }
        },
        "107b97c12d7648babeedb610c46550ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9313f8814484695a4ae4ecf38040e55",
            "placeholder": "",
            "style": "IPY_MODEL_97244596aee8431bb1dd733605102135",
            "value": "100%"
          }
        },
        "7261830546ee4f2790012390fde004fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f657914e6744889f0e45bade1ac76d",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b193773340d54f3bb541fd4b5777c626",
            "value": 244408911
          }
        },
        "93922710882d4eb8809e6bb8d2668c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f732619a846b4eacaf86785c6de08a94",
            "placeholder": "",
            "style": "IPY_MODEL_446b1f7d0d1e4c49acdb263fac90ffdd",
            "value": " 233M/233M [00:03&lt;00:00, 83.0MB/s]"
          }
        },
        "ca2f95872ac248de8ecbf0492c4f02f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9313f8814484695a4ae4ecf38040e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97244596aee8431bb1dd733605102135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5f657914e6744889f0e45bade1ac76d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b193773340d54f3bb541fd4b5777c626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f732619a846b4eacaf86785c6de08a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446b1f7d0d1e4c49acdb263fac90ffdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}